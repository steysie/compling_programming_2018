{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LabWork#1\n",
    "## *Anastasia Nikiforova*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests # Загрузка новостей с сайта.\n",
    "from bs4 import BeautifulSoup # Превращалка html в текст.\n",
    "import re # Регулярные выражения.\n",
    "import time # Время"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала захотелось все же поиграться с тем, что можно делать с requests. Ностальгия!\n",
    "\n",
    "Готовый класс - в самом низу этого ноутбука.\n",
    "\n",
    "Отсюда и до [:-1] - процесс работы. Может даже будет весело <s>не гарантирую</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"https://tproger.ru/news/pypl-october-2018/\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Тут вот я попалась на очень странную штуку!\n",
    "Когда код весь почти был написан, внезапно возникла ошибка: SSLError: [Errno 2] No such file or directory\n",
    "Оказалось, что это своего рода limitation библиотеки requests.\n",
    "\n",
    "В итоге перестала работать даже функция requests.get(\"http://tproger.ru\")\n",
    "\n",
    "Проблема, как оказалось, еще не решена, хотя существует уже больше трех лет.\n",
    "Нашла обсуждения в issues на github, поскольку перезапуск кернела решил проблему. \n",
    "\n",
    "Вот же странности!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cookies: <RequestsCookieJar[<Cookie __cfduid=d1010be6dbee243c3ed39e0b06220398a1539543863 for .tproger.ru/>]>\n",
      "time to download: 0:00:00.298509\n",
      "page encoding UTF-8\n",
      "Server response:  200\n",
      "Is everything ok?  True\n",
      "Page's URL:  https://tproger.ru/news/pypl-october-2018/\n",
      "Wall time: 320 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "resp = requests.get(\"https://tproger.ru/news/pypl-october-2018/\")\n",
    "print(\"cookies:\", resp.cookies)\n",
    "print(\"time to download:\", resp.elapsed)\n",
    "print(\"page encoding\", resp.encoding)\n",
    "print(\"Server response: \", resp.status_code)\n",
    "print(\"Is everything ok? \", resp.ok)\n",
    "print(\"Page's URL: \", resp.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сейчас попробуем чего-нибудь вытащить со страницы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python лидирует в рейтинге PYPL за октябрь 2018 года\n"
     ]
    }
   ],
   "source": [
    "findheaders = re.compile('<h1 class=\"entry-title\" itemprop=\"headline\">(.+)</h1>', re.S) # done!\n",
    "art_body = re.compile('<p>(.+)</p>', re.S)\n",
    "\n",
    "# Получает текст страницы.\n",
    "art=requests.get(\"https://tproger.ru/news/pypl-october-2018/\")\n",
    "# Находим заголовок.\n",
    "title = findheaders.findall(art.text)[0]\n",
    "print(title) #works fine!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все новости - внутри  простых <\\p>\n",
    "удалить внутри картинки и гиперссылки"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "По поводу кода ниже, а именно (bs.find_all(\"p\")[:-7]), хотелось бы сделать появнение.\n",
    "\n",
    "Дело в том, что, как оказалось, на сайте tproger.ru статьи оформлены очень удобно для того, чтобы их оттуда вытащить: в обычных <p> тэгах. \n",
    "Причем при проверке различных элементов в исходном коде оказалось, что только текст статьи оформлен в таких тэгах.\n",
    "Нооо вот при выкачивании выяснилось, что кроме статьи в <p> находится одно сообщение, которое сразу при загрузке страница не отображается, только при нажатии на кнопку для подписки на новости.\n",
    "[Такое со всеми статьями - проверила точечно на новых, старых и посередине]\n",
    "\n",
    "Ниже я привела пример такого текста в \"разделе\" trash.\n",
    "\n",
    "По идее, можно еще избавиться от строчки Source и автора, но все же они могут представлять некоторый интерес."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----\n",
      " Python лидирует в рейтинге PYPL за октябрь 2018 года \n",
      "-----\n",
      " Опубликованы новые данные ежемесячного рейтинга языков программирования PYPL за октябрь 2018 года. Данные разместил Пьер Карбоннелл (Pierre Carbonnelle). Python занял первое место, а язык Java хоть и находится на втором, но немного потерял позиции. Напомним, что рейтинг PYPL формируется на основе того, как часто искали руководства по тому или иному ЯП. Топ-5 по-прежнему составляют Python, Java, JavaScript, C# и PHP, хотя две последние позиции поменялась местами. Perl и Delphi продолжают падать, а вот Kotlin и Go, наоборот, растут. PHP также начал серьёзно падать в рамках рейтинга.  Для сравнения, в 2017 году ситуация выглядела иначе. Тогда лидером был язык Java, а Python располагался на втором месте. На третьем месте шёл PHP, который на текущий момент потерял пару очков, за ним C# от Microsoft, а на пятом месте располагался JavaScript. На графике приведены данные популярности языков программирования из первой десятки, начиная с 2005 года. Судя по ним Java, JavaScript и MATLAB особо не меняются. PHP падает, а вот Python и R растут, хотя последний и начал немного терять баллы.  Что касается Swift, который вышел в 2014 году, то он пока не догнал Objective-C по популярности, хотя компания Apple всячески продвигает его. В частности, последняя версия Telegram для iOS написана на нём, однако переход повлёк за собой ряд проблем для пользователей iOS 12. Миграция данных приводит к аварийному завершению программы или её зависанию. По словам Карбоннелла, рейтинг даёт общую картину, исходя из количества поисковых запросов в Google, в которых есть название языка и слово tutorial. А вот слово programming может искажать результаты, поскольку в случае Python речь может идти как о ЯП, так и о змеях. А в случае PHP или JavaScript такой неоднозначности нет. В этом состоит отличие от индекса TIOBE, который фиксирует количество страниц в Сети, упоминающих тот или иной ЯП. При этом PYPL показывает рост рейтинга на фоне появления первой или новой версии языка, особенно если там много изменений и нет полной обратной совместимости как в Python 2 и 3 соответственно. Также Карбоннелл использовал инструмент отслеживания популярности IDE с аналогичным принципом. Для этого используется слово download. В топ-5 по состоянию на октябрь 2018 года входят Visual Studio, Eclipse, Android Studio, NetBeans, IntelliJ, то есть картина та же, что и в прошлом месяце.  В остальном ситуация не слишком изменилась в сравнении с прошлым месяцем.  Какими будут языки программирования через 20 лет — сложный вопрос. Ведь для этого нужно понимать не только текущие тренды развития IT, но и прогнозировать их на длительное время. Разные эксперты по-разному оценивают перспективы. Одни говорят о качественном скачке, другие уверены, что «монстры» вроде C++, Java и прочих никуда не денутся. via Top IDE index\n",
      "\n",
      "Source: PYPL PopularitY of Programming Language Андрей Галадей\n",
      "\n",
      "-----\n",
      "Here comes the trash (na-na-na-na):\n",
      " Кто спроектировал первую программируемую вычислительную машину? Загрузка... Только самые важные IT-новости, один раз в сутки Вы успешно подтвердили свою подписку на «Аргументы и функции». Ждите первое письмо, оно придёт уже совсем скоро! «Аргументы и функции» — обзор самого важного из мира разработки. Даже не нужно переходить по ссылкам, просто прочитать одно письмо. Присылаем каждый день, по вечерам. Выглядит это примерно так: \n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "bs = BeautifulSoup(resp.text, \"html5lib\") \n",
    "\n",
    "title = bs.h1.text \n",
    "\n",
    "text = BeautifulSoup(\" \".join([p.text for p in bs.find_all(\"p\")[:-7]]), \"html5lib\").get_text()\n",
    "\n",
    "trash = BeautifulSoup(\" \".join([p.text for p in bs.find_all(\"p\")[-7:]]), \"html5lib\").get_text()\n",
    "\n",
    "print(\"\\n-----\\n\", title, \"\\n-----\\n\", text)\n",
    "print(\"\\n-----\\nHere comes the trash (na-na-na-na):\\n\", trash, \"\\n-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ух! Сработало, как магия!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добыча ссылок   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно добыть ссылки со страницы.\n",
    "\n",
    "Здесь хотелось бы выразить благодарность разработчикам сайта, поскольку несмотря на то, что у них нет удобной структуры адреса по дням как в Ленте, каталог новостей на Tproger оформлен в виде https://tproger.ru/category/news/page/3/, при этом на страницах новости разделены по дням! \n",
    "\n",
    "Ву-ху!\n",
    "\n",
    "Сразу видно, что имеешь дело с программистами :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://tproger.ru/news/nvidia-rapids/',\n",
       " 'https://tproger.ru/news/yandex-media-services-memorandum/',\n",
       " 'https://tproger.ru/news/lyrebird-voice-api/',\n",
       " 'https://tproger.ru/news/native-load-balancing-gke/',\n",
       " 'https://tproger.ru/news/facebook-leak/',\n",
       " 'https://tproger.ru/news/apigee-new-capabilities/',\n",
       " 'https://tproger.ru/news/neural-net-snakes/',\n",
       " 'https://tproger.ru/news/activeqa-jeopardy/',\n",
       " 'https://tproger.ru/news/zdnet-fuchsia-code/',\n",
       " 'https://tproger.ru/news/unity-visual-search/',\n",
       " 'https://tproger.ru/news/google-pick-out-voice/',\n",
       " 'https://tproger.ru/news/google-new-services-cloud-identity/',\n",
       " 'https://tproger.ru/news/neural-networks-illusions/',\n",
       " 'https://tproger.ru/news/cloud-storage-dual-region-option/',\n",
       " 'https://tproger.ru/news/poc-code-edge/',\n",
       " 'https://tproger.ru/news/google-oboe/',\n",
       " 'https://tproger.ru/news/essential-ai-smartphone/',\n",
       " 'https://tproger.ru/news/airfase-vulnerability/',\n",
       " 'https://tproger.ru/news/token-binding-protocol/',\n",
       " 'https://tproger.ru/news/common-sence-ai/',\n",
       " 'https://tproger.ru/news/mozilla-things-gateway/',\n",
       " 'https://tproger.ru/news/dod-security/',\n",
       " 'https://tproger.ru/news/amazon-jobs/',\n",
       " 'https://tproger.ru/news/russia-signed-coe-convention/',\n",
       " 'https://tproger.ru/news/google-censorship-leak/',\n",
       " 'https://tproger.ru/news/ai-movement-training/',\n",
       " 'https://tproger.ru/news/headhunter-salary-oct-2018/',\n",
       " 'https://tproger.ru/news/internet-archive-commodore-64/',\n",
       " 'https://tproger.ru/news/google-developed-activeqa/',\n",
       " 'https://tproger.ru/news/jigsaw-outline/',\n",
       " 'https://tproger.ru/news/google-established-bigquery-in-uk/',\n",
       " 'https://tproger.ru/news/microsoft-oin-patents/',\n",
       " 'https://tproger.ru/news/alice-yandex-voice/',\n",
       " 'https://tproger.ru/news/duplex-spam-calls/',\n",
       " 'https://tproger.ru/news/google-apps-api/',\n",
       " 'https://tproger.ru/news/facebook-akkio/',\n",
       " 'https://tproger.ru/news/ml-dot-net-0-6-release/',\n",
       " 'https://tproger.ru/news/chemical-program-language-srn-plus-plus/',\n",
       " 'https://tproger.ru/news/google-drops-pentagon/',\n",
       " 'https://tproger.ru/news/intel-announced-9th-gen-processors/',\n",
       " 'https://tproger.ru/news/microsoft-announced-snip-insights/',\n",
       " 'https://tproger.ru/news/google-plus-leak-data/',\n",
       " 'https://tproger.ru/news/infer-net-source-code-revealed/']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2s = BeautifulSoup(requests.get(\"https://tproger.ru/category/news/\").text, \"html5lib\").find_all(\"h2\")\n",
    "#print(h2s)\n",
    "links = [l(\"a\")[0][\"href\"] for l in h2s]\n",
    "links   # так красивее, чес принтом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну все, с одной страница все ссылки получили, значит и со всех собрать - не проблема.\n",
    "\n",
    "А значит, и корпус статей собрать - как пить дать. (тьфу-тьфу)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NB! Как раз на этом месте выдалась ошибка SSLError: [Errno 2] No such file or directory (писала в самом начале про неё).\n",
    "накаркала..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Загрузка статьи по URL.\n",
    "def getOneArticle(url):\n",
    "    \"\"\" getOneArticle gets the body of an article from Tproger.ru\"\"\"\n",
    "    resp = requests.get(url)\n",
    "    bs = BeautifulSoup(resp.text, \"html5lib\") \n",
    "    aTitle = bs.h1.text\n",
    "    anArticle = BeautifulSoup(\" \".join([p.text for p in bs.find_all(\"p\")[:-7]]), \"html5lib\").get_text()\n",
    "    return \"\\n----------\\n\" + aTitle + \"\\n-----\\n\" + anArticle\n",
    "\n",
    "# Загрузка всех статей с одной страницы.\n",
    "def getPage(url):\n",
    "    \"\"\" Gets all URLs with news from one page. \"\"\"\n",
    "    res = \"\"\n",
    "    try:\n",
    "        # Грузим страницу со списком всех статей.\n",
    "        day = requests.get(url) \n",
    "        # Получаем фрагменты с нужными нам адресами статей.\n",
    "        h2s = BeautifulSoup(day.text, \"html5lib\").find_all(\"h2\")\n",
    "        # Получаем все адреса на статьи за день.\n",
    "        links = [l.find_all(\"a\")[0][\"href\"] for l in h2s]\n",
    "        # Загружаем статьи.\n",
    "        for l in links[:10]:\n",
    "            res += getOneArticle(l)\n",
    "            #res += \"\\n\\n*****\\n\"\n",
    "            time.sleep(0.3) # Мы ведь этичные хакеры.  - Ага \n",
    "    except:\n",
    "        pass\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "NVIDIA анонсировала платформу GPU-ускорения для обработки больших массивов данных и машинного обучения\n",
      "-----\n",
      "После двух лет разработки NVIDIA анонсировала открытую платформу GPU-ускорения RAPIDS для обработки больших массивов данных и машинного обучения на основе архитектуры CUDA. Использование RAPIDS с алгоритмом машинного обучения XGBoost даёт 50-кратное увеличение производительности по сравнению с системами на базе CPU. Платформа базируется на открытых проектах, таких как Apache Arrow, pandas и scikit-learn. NVIDIA планирует добавить в неё новые библиотеки и возможности машинного обучения, для этого компания сотрудничает с Anaconda, BlazingDB, Databricks, Quansight и другими игроками рынка открытого ПО. NVIDIA интегрировала RAPIDS во фреймворк для анализа и обработки данных Apache Spark, чтобы ускорить его распространение. Открытые библиотеки RAPIDS уже доступны на сайте платформы, в ближайшее время компания собирается добавить контейнеризованные версии в репозитории NVIDIA GPU Cloud. В августе 2018 года NVIDIA представила новую серию графических ускорителей GeForce RTX с технологией трассировки лучей. В компании заявили, что видеокарты в 6 раз мощнее предыдущего поколения видеоускорителей NVIDIA. Source: блог NVIDIA Наташа Маркова\n",
      "----------\n",
      "«Яндекс» и правообладатели не смогли договориться о новой политике против пиратства\n",
      "-----\n",
      "Компании «Яндекс», Mail.Ru Group, «Газпром-Медиа» и Ассоциация продюсеров кино и телевидения не пришли к общему выводу в отношении антипиратского меморандума. «Яндекс» считает, что контент должен подвергаться блокировке только по решению суда, правообладатели же утверждают обратное. «Яндекс» настаивает на том, что правообладатель, подающий заявление на нарушение авторских прав и блокировку контента, должен прилагать к нему копию заключения Мосгорсуда о предварительном обеспечении защиты авторских прав. Также компания выдвинула перечень условий меморандума, в который входит, например, невозможность подачи иска в суд на «Яндекс» со стороны подписавшего. Медиасервисы отказались от таких условий. Меморандум, предложенный ими, содержал пункт о создании антипиратского реестра. Интернет-компания в течение шести часов должна будет ограничить отображение в поисковой выдаче ссылки на контент, попавший в этот реестр. Ответственной за этот ресурс предлагается назначить ассоциацию «Интернет-видео», либо новый орган, сформированный правообладателями. Такие условия получили поддержку со стороны Роскомнадзора. Интернет-компания уже была вовлечена в спор, связанный с нарушением авторских прав. В августе 2018 года Роскомнадзор обязал «Яндекс» удалить ссылки на пиратский контент, размещённый на сервисе Яндекс.Видео, пригрозив блокировкой. Source: ComputerWorld\n",
      " Никита Нельсон\n",
      "----------\n",
      "Имитатор голоса Lyrebird открыли для сторонних разработчиков\n",
      "-----\n",
      "Сервис Lyrebird из Канады запустил бесплатную бета-версию API для синтеза голоса. Этот программный интерфейс позволит сторонним разработчикам создавать цифровые копии голосов пользователей. Lyrebird позволяет создавать нужные копии голоса на основе одной минуты речи. Предполагается, что это даст возможность создавать индивидуальные «голоса» для приложений, аватаров и так далее.  Для синтеза используется нейросеть, которую разработали в университете Монреаля. В прошлом году с его помощью уже имитировали голоса Барака Обамы, Дональда Трампа и Хиллари Клинтон. Публичное тестирование было начато в сентябре 2017 года. Инструкция по применению API находится здесь. Для использования нужно зарегистрироваться в системе и дать согласие на обработку личных данных. Разработчики пока не уточняют этого. Однако, в перспективе подобные технологии действительно могут стать источником проблем. Современные смартфоны, «умные» колонки и прочие устройства вполне могут «слушать» пользователя, фиксировать его разговоры и так далее. А также из них можно получить образы голоса и подделать их. Lyrebird — не единственный подобный сервис. Ранее в Ирландии сообщили о разработке синтезатора речи, который не требует подключения к Сети. Он может работать на смартфоне в качестве локального приложения. А весной 2018 года Google открыла для сторонних разработчиков доступ к собственной технологии синтеза речи. via Rusbase\n",
      " Андрей Галадей\n",
      "----------\n",
      "Google добавила в GKE нативное распределение нагрузки между контейнерами\n",
      "-----\n",
      "Google рассказала о доработках инструмента распределения нагрузки между контейнерами в Google Kubernetes Engine (GKE). С помощью обновлённого балансировщика пользователь самостоятельно создаёт группы конечных точек трафика в сети. Система распределяет нагрузку между контейнерами для большей эффективности. В GKE добавлена возможность создания объекта с внешним доступом, предоставляемым выделенным балансировщиком. Это позволяет настраивать маршрутизацию до конечных объектов по указанному пути или имени хост-сервера. Новая система распределения нагрузки имеет следующие особенности: Cloud Native Computing Foundation обновили систему оркестровки контейнеров GKE в конце сентября 2018 года. По словам разработчиков, в версии 1.12 стали стабильно работать две функции: Kubelet TLS Bootstrap для подписи сертификатов безопасности при TLS-соединениях и поддержка виртуальных машин Azure. Source: блог Google Cloud Артем Гаврилов\n",
      "----------\n",
      "Facebook: украдены персональные данные около 30 миллионов пользователей\n",
      "-----\n",
      "Facebook рассказала, что хакеры получили доступ к персональной информации около 30 миллионов пользователей, воспользовавшись багом в коде соцсети. Преступники похитили токены доступа Facebook и использовали их для захвата учётных данных пользователей. 14 сентября 2018 года Facebook заметила необычный всплеск пользовательской активности и начала расследование. 25 сентября разработчики обнаружили атаку и в течение двух дней остановили её, закрыли уязвимость и защитили учётные записи, сбросив токены доступа. Компания обратилась в ФБР, которое теперь занимается расследованием. Изначально корпорация полагала, что похищены данные 50 миллионов человек, сейчас Facebook говорит о 30 миллионах пострадавших. Установлено, что злоумышленники использовали технику перехода от одной учётной записи к другой, похищая токены доступа друзей. Таким образом, преступники похитили 400 000 профилей и получили доступ к информации, которую видели сами пользователи при просмотре собственных страниц: посты, списки друзей, группы, имена последних собеседников в Messenger. Содержание сообщений было доступно, если пользователь получал их как администратор группы. Похитители использовали токены доступа этих 400 000 человек для кражи данных примерно 30 миллионов пользователей: На специальной странице справочного центра пользователи могут узнать, был ли скомпрометирован их профиль. Компания пообещала в ближайшее время отправить персонализированные сообщения 30 миллионам пострадавших с объяснением произошедшего, а также рекомендациями по защите аккаунта. Source: новостной центр Facebook Наташа Маркова\n",
      "----------\n",
      "Google представила новые инструменты для платформы Apigee\n",
      "-----\n",
      "Разработчики из Google рассказали о выходе из бета-теста новых средств для Apigee — платформы для управления и анализа API. Новые инструменты помогают отслеживать возникновение проблем, использовать облачные службы в качестве расширений и размещать приложения в изолированной среде. Бета-тест нововведений начался в июле, и теперь команда Google Cloud представила новые функции платформы: Google продолжает развивать облачные технологии и в середине октября 2018 года объявила о запуске трёх новых сервисов в Cloud Identity. Разработчики сообщили, что инновации призваны упростить аутентификацию пользователей, повысив при этом уровень безопасности. Source: блог Google Cloud Артем Гаврилов\n",
      "----------\n",
      "«Хорошенький малютка питон» и «змея-блондинка»: нейросеть придумала новые названия видов змей\n",
      "-----\n",
      "Исследовательница Джанелль Шейн (Janelle Shane) собрала датасет из около 1000 английских распространённых слов и научила нейросеть создавать с его помощью названия видов змей. Некоторые названия получились совсем невероятными: Некоторые варианты выглядят более похожими на реальные названия: Ещё несколько названий, которые придумала нейросеть: Также Шейн добавила в нейросеть 4500 названий костюмов на Хэллоуин. Вот что получилось: Автор статьи пообещала выслать остальные названия тем, кто оставит свою электронную почту. Джанелль Шейн известна своими экспериментами с соцсетями. Она тренирует алгоритмы генерировать комплименты, оригинальные названия рыб, насекомых и мороженого. Source: блог AIWeirdness Наташа Маркова\n",
      "----------\n",
      "В Google AI обучили нейросеть играть в «Jeopardy!»\n",
      "-----\n",
      "Разработчики из Google AI протестировали способности нейросети на вопросах из игры «Jeopardy!». С помощью агента ActiveQA нейросеть распознавала в утверждении вопрос и давала ответ в вопросительной форме. «Катализатором» для исследований в этой области стала победа компьютера Watson от IBM над двумя чемпионами мира на шоу «Jeopardy!» в 2011 году. Агент Active Question Answering перефразирует заданный вопрос во множество вариантов. Затем он отправляет запросы в базу данных с ответами, получает несколько и выбирает среди них правильный и наиболее полный. В отличие от обычного QA, который без изменения отправляет заданный вопрос в среду с ответами, ActiveQA способен обрабатывать лексически сложные вопросы. Успех нейросети основывался на обучении с подкреплением. ИИ стал точнее переформулировать задаваемые вопросы, получая положительную реакцию на правильные ответы. Например, машине задаётся вопрос: «Ганди был под глубоким влиянием графа, написавшего „Войну и мир“». Нейронная сеть должна сформулировать вопрос, содержащий правильный ответ. Например: «Кто такой Лев Толстой?» Google AI представила пакет ActiveQA на основе TensorFlow в октябре 2018 года. Пакет размещён в репозитории GitHub и доступен для всех пользователей TensorFlow от Google. Source: ZDNet\n",
      " Никита Нельсон\n",
      "----------\n",
      "ZDNet: Всё, что смогли выяснить специалисты о Fuchsia OS, исходя из её кода\n",
      "-----\n",
      "Агентство национальной безопасности США провело проверку операционной системы Fuchsia — разработки компании Google. Свои выводы представители ведомства обнародовали на недавнем саммите по безопасности Linux в Ванкувере, США. Впервые об этой операционной системе стало известно в 2016 году, когда её код нашли на GitHub. Причём официальных заявлений не было. Первоначально предполагалось, что новая ОС предназначена для встраиваемых систем, но затем заговорили о её универсальном применении. Некоторые СМИ даже предположили, что Fuchsia заменит Android и Chrome OS. Кроме того, говорилось о кроссплатформенности и архитектурных преимуществах новой системы над существующими. От вышеупомянутых систем «Фуксия» отличалась использованием микроядра «Zircon», производного от «Little Kernel», а не ядра Linux.  Последнее означает, что драйверы устройств, файловая система и всё остальное не встроены в ядро, а выполняются в пространстве пользователя. В Linux всё работает в пределах ядра. Специалисты АНБ Джеймс Картер (James Carter) и Стивен Смалли (Stephen Smalley) рассказали некоторые подробности, которые им удалось получить из кода Fuchsia. Эта система является модульной, что косвенно подтверждает её универсальность. Иначе говоря, операционная система может работать как на встраиваемой платформе, так и на ПК или смартфоне. В зависимости от задач можно добавлять необходимые модули. Сама же ОС базируется на нескольких типах объектов, доступных ядру через системные вызовы. При этом с точки зрения разработчика Fuchsia не отличается от Unix/Linux. Она также поддерживает POSIX. Для разработки доступны Google Flutter SDK и Swift — язык программирования от Apple. В плане безопасности, по словам Смалли и Картера, ситуация пока выглядит не слишком хорошо. В частности, можно получить доступ к любому дескриптору и его дочерним процессам. По их словам, предстоит ещё много работы по улучшению безопасности. Смалли и Картер призвали сообщество включиться в работу над Fuchsia OS. Это означает, что новая ОС от Google ещё крайне далека от релиза или даже относительно стабильной бета-версии, особенно в сравнении с Linux. Также отмечается, что новая ОС имеет архитектуру, которая серьёзно отличается от других ОС. То есть программы на Fuchsia будут выполняться не так как в большинстве операционных систем. Хотя, возможно, с точки зрения пользователя разницы и не будет. При этом, по некоторым данным, в Google Home Hub — «умном» динамике с 7-дюймовым экраном, были обнаружены следы Fuchsia. Разумеется, это не означает, что смарт-устройство обязательно получит именно эту ОС, но, вероятно, её попытаются протестировать таким образом. Веб-демо Fuchsia уже доступно. Также опубликована часть документации. В начале года компания Google опубликовала инструкцию по установке Fuchsia OS на пользовательские устройства. Правда, это оказалось не слишком просто. Для работы нужны два ПК или ноутбука, соединённых по сети. Source: ZDNet Андрей Галадей\n",
      "----------\n",
      "Unity выпустила Visual Search, инструмент для управления 3D-ассетами с помощью ИИ\n",
      "-----\n",
      "Разработчики Unity3D рассказали в своём блоге о новом плагине для игрового движка — Visual Search. Данный проект, созданный в сотрудничестве со специалистами по машинному обучению из Resonai, предлагает визуальный поиск и фильтрацию ассетов из Unity Asset Store прямо в окне редактора. Авторы проекта подробно рассказали о возможностях нового инструмента в видеоролике:  Инструмент позволяет искать ассеты не только визуально, но и с использованием текстового описания или ключевых слов. Разработчик может фильтровать критерии с помощью блокировки отдельных элементов в системе ProBuilder или по необходимым категориям: стоимость, количество полигонов или материалов.  Также для поиска можно использовать макеты. Для этого необходимо создать 3D-прототип будущего объекта, а затем загрузить его компоненты в плагин. Система найдёт визуально похожие ресурсы в базе магазина.  Плагин Visual Search доступен бесплатно в магазине ассетов Unity. В середине сентября 2018 года команда разработчиков Unity опубликовала первую публичную версию Unity Hub — центра управления разработкой игры. Инструмент также выступает как объединённая утилита для всей экосистемы игрового движка. Source: блог Unity3D Тимур Кондратьев\n"
     ]
    }
   ],
   "source": [
    "r = getPage(\"https://tproger.ru/category/news/\")\n",
    "print (r)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Фууууух, получилось!\n",
    "Вообще, для удобства разделения статей, наверное, лучше, чтобы был какой-то идентификатор/разделитель статей. Мало ли, вдруг понадобится (чисто гипотетически).\n",
    "\n",
    "Сейчас выглядит все так:\n",
    "\n",
    "-----\n",
    "Unity выпустила Visual Search, инструмент для управления 3D-ассетами с помощью ИИ\n",
    "----- \n",
    "[бла-бла-бла, текст статьи]\n",
    "\n",
    "xxx: Переделаю-ка я верхние черточки в звездочки!\n",
    "xxx: А нет, чёт не оч.\n",
    "xxx: Ну, тогда просто звездочек в конец каждой статьи можно добавить.\n",
    "xxx: эээ.. ну сойдет. Лучше, чем ничего.\n",
    "xxx: Нет, все-таки мне не нравится. А что если черточек сверху сделать вдвое больше, чем черточек после заголовка?..\n",
    "xxx: Идеально!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь осталось только пройти по всем (ну или например по первым 20 страницам на этот раз) страницам новостей и собрать из них тексты.\n",
    "\n",
    "И в ООП-формате все оформить (мы же воспитанные программисты)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формат: ООП \n",
    "\n",
    "*(Хотя многие со мной поспорят о том, что Python и ООП как-то связаны)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getNewsPaper:\n",
    "         \n",
    "    # Конструктор - вызывается при создании объекта и инициализирует его.\n",
    "    def __init__(self):\n",
    "        self.articles=[]     # Загруженные статьи.\n",
    "        self.titles=[]       # Заголовки статей.\n",
    "        self.dictionaries=[] # Словари для каждой из статей.\n",
    "        # Создаем и загружаем морфологический словарь.\n",
    "        self.morph=pymorphy2.MorphAnalyzer()\n",
    "        \n",
    "    def getOneArticle(self, url):\n",
    "        \"\"\" getOneArticle gets the body of an article from Tproger.ru \"\"\"\n",
    "        one_title = []\n",
    "        one_article = []\n",
    "        resp = requests.get(url)\n",
    "        bs = BeautifulSoup(resp.text, \"html5lib\") \n",
    "        \n",
    "        self.titles.append(bs.h1.text)\n",
    "        one_title.append(bs.h1.text)\n",
    "        \n",
    "        self.articles.append(BeautifulSoup(\" \".join([p.text for p in bs.find_all(\"p\")[:-7]]), \"html5lib\").get_text().replace(\"\\xa0\", \" \"))\n",
    "        one_article.append(BeautifulSoup(\" \".join([p.text for p in bs.find_all(\"p\")[:-7]]), \"html5lib\").get_text().replace(\"\\xa0\", \" \"))\n",
    "        \n",
    "        return one_title, one_article\n",
    "\n",
    "    # Загрузка всех статей с одной страницы.\n",
    "    def getPage(self, url):\n",
    "        \"\"\" Gets all URLs with news from one page. \"\"\"\n",
    "        return_page = []\n",
    "        try:\n",
    "            # Грузим страницу со списком всех статей.\n",
    "            day = requests.get(url) \n",
    "            # Получаем фрагменты с нужными нам адресами статей.\n",
    "            h2s = BeautifulSoup(day.text, \"html5lib\").find_all(\"h2\")\n",
    "            # Получаем все адреса на статьи за день.\n",
    "            links = [l.find_all(\"a\")[0][\"href\"] for l in h2s]\n",
    "            # Загружаем статьи.\n",
    "            for l in links[:10]:\n",
    "                self.articles.append(self.getOneArticle(l))\n",
    "                return_page.append(self.getOneArticle(l))\n",
    "                time.sleep(0.3)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return return_page\n",
    "\n",
    "    # Загрузка всех статей за несколько дней.\n",
    "    def getPeriod(self, last):\n",
    "        #         self.articles.append(self.getPage(\"https://tproger.ru/category/news/\"))\n",
    "        #         return_period.append(self.getPage(\"https://tproger.ru/category/news/\"))\n",
    "        \n",
    "        #         start = 2  \n",
    "        #         # Можно было бы и с первой начать, но /news/page/1 переадресует на /news/, лучше обойтись без переадресаций\n",
    "        period = []\n",
    "        for i in range(last):\n",
    "            # last+1, т.к. последний элемент нам тоже нужен\n",
    "            self.articles.append(self.getPage('https://tproger.ru/category/news/' + str(i)))\n",
    "            period.append(self.getPage('https://tproger.ru/category/news/' + str(i)))\n",
    "            \n",
    "        return period\n",
    "            \n",
    "    # Заагрузка топовых новостей за последнюю неделю / за все время        \n",
    "    def getTop(self, **options):\n",
    "        '''period in ['week', 'month', 'ever', '100']'''\n",
    "        return_top = []\n",
    "        \n",
    "        if options.get(\"period\") == \"week\":\n",
    "            self.articles.append(self.getPage('https://tproger.ru/top/week/'))\n",
    "            return_top.append(self.getPage('https://tproger.ru/top/week/'))\n",
    "            \n",
    "        if options.get(\"period\") == \"month\":    \n",
    "            self.articles.append(self.getPage('https://tproger.ru/top/month/'))\n",
    "            return_top.append(self.getPage('https://tproger.ru/top/month/'))\n",
    "\n",
    "        if options.get(\"period\") in [\"ever\", \"100\"]:\n",
    "            # Вообще, на сайте предлагают загрузить топовые новости за последние 100 лет. Юмористы\n",
    "            # но раз предлагают, включим \"100\"\n",
    "            self.articles.append(self.getPage('https://tproger.ru/top/ever/'))\n",
    "            return_top.append(self.getPage('https://tproger.ru/top/ever/'))\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return return_top\n",
    "\n",
    "    # Потроение вектора для статьи.\n",
    "    posConv = {'ADJF':'_ADJ','NOUN':'_NOUN','VERB':'_VERB'}\n",
    "    def getArticleDictionary(self, text, needPos = None):\n",
    "        words = [a[0] for a in re.findall(\"([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)\", text)]\n",
    "        reswords = []\n",
    "    \n",
    "        for w in words:\n",
    "            wordform = self.morph.parse(w)[0]\n",
    "            try:\n",
    "                if wordform.tag.POS in ['ADJF', 'NOUN', 'VERB']:\n",
    "                    if needPos != None:\n",
    "                        reswords.append(wordform.normal_form + self.posConv[wordform.tag.POS])\n",
    "                    else:\n",
    "                        reswords.append(wordform.normal_form)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        stat = Counter(reswords)\n",
    "        stat = {a: stat[a] for a in stat.keys() if stat[a] > 1}\n",
    "        return stat\n",
    "\n",
    "    # Посчитаем вектора для всех статей.\n",
    "    def calcArticleDictionaries(self, needPos = None):\n",
    "        self.dictionaries = []\n",
    "        for a in self.articles:\n",
    "            self.dictionaries.append(self.getArticleDictionary(a, needPos))\n",
    "            \n",
    "    # Сохраняем статьи в файл.\n",
    "    def saveArticles(self, filename):\n",
    "        \"\"\" Saves all articles to a file with a filename. \"\"\"\n",
    "        newsfile = open(filename, \"w\")\n",
    "        for art in self.articles:\n",
    "            newsfile.write('\\n=====\\n' + art)\n",
    "        newsfile.close()\n",
    "\n",
    "    # Читаем статьи из файла.\n",
    "    def loadArticles(self, filename):\n",
    "        \"\"\" Loads and replaces all articles from a file with a filename. \"\"\"\n",
    "        newsfile = open(filename, encoding=\"utf-8\")\n",
    "        text = newsfile.read()\n",
    "        self.articles = text.split('\\n=====\\n')[1:]\n",
    "        for a in self.articles:\n",
    "            self.titles.append(a.split('\\n-----\\n')[0])\n",
    "        newsfile.close()\n",
    "\n",
    "    # Для удобства - поиск статьи по ее заголовку.\n",
    "    def findNewsByTitle(self, title):\n",
    "        if title in self.titles:\n",
    "            return self.titles.index(title)\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['NVIDIA анонсировала платформу GPU-ускорения для обработки больших массивов данных и машинного обучения'], ['После двух лет разработки NVIDIA анонсировала открытую платформу GPU-ускорения RAPIDS для обработки больших массивов данных и машинного обучения на основе архитектуры CUDA. Использование RAPIDS с алгоритмом машинного обучения XGBoost даёт 50-кратное увеличение производительности по сравнению с системами на базе CPU. Платформа базируется на открытых проектах, таких как Apache Arrow, pandas и scikit-learn. NVIDIA планирует добавить в неё новые библиотеки и возможности машинного обучения, для этого компания сотрудничает с Anaconda, BlazingDB, Databricks, Quansight и другими игроками рынка открытого ПО. NVIDIA интегрировала RAPIDS во фреймворк для анализа и обработки данных Apache Spark, чтобы ускорить его распространение. Открытые библиотеки RAPIDS уже доступны на сайте платформы, в ближайшее время компания собирается добавить контейнеризованные версии в репозитории NVIDIA GPU Cloud. В августе 2018 года NVIDIA представила новую серию графических ускорителей GeForce RTX с технологией трассировки лучей. В компании заявили, что видеокарты в 6 раз мощнее предыдущего поколения видеоускорителей NVIDIA. Source: блог NVIDIA Наташа Маркова']), (['«Яндекс» и правообладатели не смогли договориться о новой политике против пиратства'], ['Компании «Яндекс», Mail.Ru Group, «Газпром-Медиа» и Ассоциация продюсеров кино и телевидения не пришли к общему выводу в отношении антипиратского меморандума. «Яндекс» считает, что контент должен подвергаться блокировке только по решению суда, правообладатели же утверждают обратное. «Яндекс» настаивает на том, что правообладатель, подающий заявление на нарушение авторских прав и блокировку контента, должен прилагать к нему копию заключения Мосгорсуда о предварительном обеспечении защиты авторских прав. Также компания выдвинула перечень условий меморандума, в который входит, например, невозможность подачи иска в суд на «Яндекс» со стороны подписавшего. Медиасервисы отказались от таких условий. Меморандум, предложенный ими, содержал пункт о создании антипиратского реестра. Интернет-компания в течение шести часов должна будет ограничить отображение в поисковой выдаче ссылки на контент, попавший в этот реестр. Ответственной за этот ресурс предлагается назначить ассоциацию «Интернет-видео», либо новый орган, сформированный правообладателями. Такие условия получили поддержку со стороны Роскомнадзора. Интернет-компания уже была вовлечена в спор, связанный с нарушением авторских прав. В августе 2018 года Роскомнадзор обязал «Яндекс» удалить ссылки на пиратский контент, размещённый на сервисе Яндекс.Видео, пригрозив блокировкой. Source: ComputerWorld\\n Никита Нельсон']), (['Имитатор голоса Lyrebird открыли для сторонних разработчиков'], ['Сервис Lyrebird из Канады запустил бесплатную бета-версию API для синтеза голоса. Этот программный интерфейс позволит сторонним разработчикам создавать цифровые копии голосов пользователей. Lyrebird позволяет создавать нужные копии голоса на основе одной минуты речи. Предполагается, что это даст возможность создавать индивидуальные «голоса» для приложений, аватаров и так далее.  Для синтеза используется нейросеть, которую разработали в университете Монреаля. В прошлом году с его помощью уже имитировали голоса Барака Обамы, Дональда Трампа и Хиллари Клинтон. Публичное тестирование было начато в сентябре 2017 года. Инструкция по применению API находится здесь. Для использования нужно зарегистрироваться в системе и дать согласие на обработку личных данных. Разработчики пока не уточняют этого. Однако, в перспективе подобные технологии действительно могут стать источником проблем. Современные смартфоны, «умные» колонки и прочие устройства вполне могут «слушать» пользователя, фиксировать его разговоры и так далее. А также из них можно получить образы голоса и подделать их. Lyrebird — не единственный подобный сервис. Ранее в Ирландии сообщили о разработке синтезатора речи, который не требует подключения к Сети. Он может работать на смартфоне в качестве локального приложения. А весной 2018 года Google открыла для сторонних разработчиков доступ к собственной технологии синтеза речи. via Rusbase\\n Андрей Галадей']), (['Google добавила в GKE нативное распределение нагрузки между контейнерами'], ['Google рассказала о доработках инструмента распределения нагрузки между контейнерами в Google Kubernetes Engine (GKE). С помощью обновлённого балансировщика пользователь самостоятельно создаёт группы конечных точек трафика в сети. Система распределяет нагрузку между контейнерами для большей эффективности. В GKE добавлена возможность создания объекта с внешним доступом, предоставляемым выделенным балансировщиком. Это позволяет настраивать маршрутизацию до конечных объектов по указанному пути или имени хост-сервера. Новая система распределения нагрузки имеет следующие особенности: Cloud Native Computing Foundation обновили систему оркестровки контейнеров GKE в конце сентября 2018 года. По словам разработчиков, в версии 1.12 стали стабильно работать две функции: Kubelet TLS Bootstrap для подписи сертификатов безопасности при TLS-соединениях и поддержка виртуальных машин Azure. Source: блог Google Cloud Артем Гаврилов']), (['Facebook: украдены персональные данные около 30 миллионов пользователей'], ['Facebook рассказала, что хакеры получили доступ к персональной информации около 30 миллионов пользователей, воспользовавшись багом в коде соцсети. Преступники похитили токены доступа Facebook и использовали их для захвата учётных данных пользователей. 14 сентября 2018 года Facebook заметила необычный всплеск пользовательской активности и начала расследование. 25 сентября разработчики обнаружили атаку и в течение двух дней остановили её, закрыли уязвимость и защитили учётные записи, сбросив токены доступа. Компания обратилась в ФБР, которое теперь занимается расследованием. Изначально корпорация полагала, что похищены данные 50 миллионов человек, сейчас Facebook говорит о 30 миллионах пострадавших. Установлено, что злоумышленники использовали технику перехода от одной учётной записи к другой, похищая токены доступа друзей. Таким образом, преступники похитили 400 000 профилей и получили доступ к информации, которую видели сами пользователи при просмотре собственных страниц: посты, списки друзей, группы, имена последних собеседников в Messenger. Содержание сообщений было доступно, если пользователь получал их как администратор группы. Похитители использовали токены доступа этих 400 000 человек для кражи данных примерно 30 миллионов пользователей: На специальной странице справочного центра пользователи могут узнать, был ли скомпрометирован их профиль. Компания пообещала в ближайшее время отправить персонализированные сообщения 30 миллионам пострадавших с объяснением произошедшего, а также рекомендациями по защите аккаунта. Source: новостной центр Facebook Наташа Маркова']), (['Google представила новые инструменты для платформы Apigee'], ['Разработчики из Google рассказали о выходе из бета-теста новых средств для Apigee — платформы для управления и анализа API. Новые инструменты помогают отслеживать возникновение проблем, использовать облачные службы в качестве расширений и размещать приложения в изолированной среде. Бета-тест нововведений начался в июле, и теперь команда Google Cloud представила новые функции платформы: Google продолжает развивать облачные технологии и в середине октября 2018 года объявила о запуске трёх новых сервисов в Cloud Identity. Разработчики сообщили, что инновации призваны упростить аутентификацию пользователей, повысив при этом уровень безопасности. Source: блог Google Cloud Артем Гаврилов']), (['«Хорошенький малютка питон» и «змея-блондинка»: нейросеть придумала новые названия видов змей'], ['Исследовательница Джанелль Шейн (Janelle Shane) собрала датасет из около 1000 английских распространённых слов и научила нейросеть создавать с его помощью названия видов змей. Некоторые названия получились совсем невероятными: Некоторые варианты выглядят более похожими на реальные названия: Ещё несколько названий, которые придумала нейросеть: Также Шейн добавила в нейросеть 4500 названий костюмов на Хэллоуин. Вот что получилось: Автор статьи пообещала выслать остальные названия тем, кто оставит свою электронную почту. Джанелль Шейн известна своими экспериментами с соцсетями. Она тренирует алгоритмы генерировать комплименты, оригинальные названия рыб, насекомых и мороженого. Source: блог AIWeirdness Наташа Маркова']), (['В Google AI обучили нейросеть играть в «Jeopardy!»'], ['Разработчики из Google AI протестировали способности нейросети на вопросах из игры «Jeopardy!». С помощью агента ActiveQA нейросеть распознавала в утверждении вопрос и давала ответ в вопросительной форме. «Катализатором» для исследований в этой области стала победа компьютера Watson от IBM над двумя чемпионами мира на шоу «Jeopardy!» в 2011 году. Агент Active Question Answering перефразирует заданный вопрос во множество вариантов. Затем он отправляет запросы в базу данных с ответами, получает несколько и выбирает среди них правильный и наиболее полный. В отличие от обычного QA, который без изменения отправляет заданный вопрос в среду с ответами, ActiveQA способен обрабатывать лексически сложные вопросы. Успех нейросети основывался на обучении с подкреплением. ИИ стал точнее переформулировать задаваемые вопросы, получая положительную реакцию на правильные ответы. Например, машине задаётся вопрос: «Ганди был под глубоким влиянием графа, написавшего „Войну и мир“». Нейронная сеть должна сформулировать вопрос, содержащий правильный ответ. Например: «Кто такой Лев Толстой?» Google AI представила пакет ActiveQA на основе TensorFlow в октябре 2018 года. Пакет размещён в репозитории GitHub и доступен для всех пользователей TensorFlow от Google. Source: ZDNet\\n Никита Нельсон']), (['ZDNet: Всё, что смогли выяснить специалисты о Fuchsia OS, исходя из её кода'], ['Агентство национальной безопасности США провело проверку операционной системы Fuchsia — разработки компании Google. Свои выводы представители ведомства обнародовали на недавнем саммите по безопасности Linux в Ванкувере, США. Впервые об этой операционной системе стало известно в 2016 году, когда её код нашли на GitHub. Причём официальных заявлений не было. Первоначально предполагалось, что новая ОС предназначена для встраиваемых систем, но затем заговорили о её универсальном применении. Некоторые СМИ даже предположили, что Fuchsia заменит Android и Chrome OS. Кроме того, говорилось о кроссплатформенности и архитектурных преимуществах новой системы над существующими. От вышеупомянутых систем «Фуксия» отличалась использованием микроядра «Zircon», производного от «Little Kernel», а не ядра Linux.  Последнее означает, что драйверы устройств, файловая система и всё остальное не встроены в ядро, а выполняются в пространстве пользователя. В Linux всё работает в пределах ядра. Специалисты АНБ Джеймс Картер (James Carter) и Стивен Смалли (Stephen Smalley) рассказали некоторые подробности, которые им удалось получить из кода Fuchsia. Эта система является модульной, что косвенно подтверждает её универсальность. Иначе говоря, операционная система может работать как на встраиваемой платформе, так и на ПК или смартфоне. В зависимости от задач можно добавлять необходимые модули. Сама же ОС базируется на нескольких типах объектов, доступных ядру через системные вызовы. При этом с точки зрения разработчика Fuchsia не отличается от Unix/Linux. Она также поддерживает POSIX. Для разработки доступны Google Flutter SDK и Swift — язык программирования от Apple. В плане безопасности, по словам Смалли и Картера, ситуация пока выглядит не слишком хорошо. В частности, можно получить доступ к любому дескриптору и его дочерним процессам. По их словам, предстоит ещё много работы по улучшению безопасности. Смалли и Картер призвали сообщество включиться в работу над Fuchsia OS. Это означает, что новая ОС от Google ещё крайне далека от релиза или даже относительно стабильной бета-версии, особенно в сравнении с Linux. Также отмечается, что новая ОС имеет архитектуру, которая серьёзно отличается от других ОС. То есть программы на Fuchsia будут выполняться не так как в большинстве операционных систем. Хотя, возможно, с точки зрения пользователя разницы и не будет. При этом, по некоторым данным, в Google Home Hub — «умном» динамике с 7-дюймовым экраном, были обнаружены следы Fuchsia. Разумеется, это не означает, что смарт-устройство обязательно получит именно эту ОС, но, вероятно, её попытаются протестировать таким образом. Веб-демо Fuchsia уже доступно. Также опубликована часть документации. В начале года компания Google опубликовала инструкцию по установке Fuchsia OS на пользовательские устройства. Правда, это оказалось не слишком просто. Для работы нужны два ПК или ноутбука, соединённых по сети. Source: ZDNet Андрей Галадей']), (['Unity выпустила Visual Search, инструмент для управления 3D-ассетами с помощью ИИ'], ['Разработчики Unity3D рассказали в своём блоге о новом плагине для игрового движка — Visual Search. Данный проект, созданный в сотрудничестве со специалистами по машинному обучению из Resonai, предлагает визуальный поиск и фильтрацию ассетов из Unity Asset Store прямо в окне редактора. Авторы проекта подробно рассказали о возможностях нового инструмента в видеоролике:  Инструмент позволяет искать ассеты не только визуально, но и с использованием текстового описания или ключевых слов. Разработчик может фильтровать критерии с помощью блокировки отдельных элементов в системе ProBuilder или по необходимым категориям: стоимость, количество полигонов или материалов.  Также для поиска можно использовать макеты. Для этого необходимо создать 3D-прототип будущего объекта, а затем загрузить его компоненты в плагин. Система найдёт визуально похожие ресурсы в базе магазина.  Плагин Visual Search доступен бесплатно в магазине ассетов Unity. В середине сентября 2018 года команда разработчиков Unity опубликовала первую публичную версию Unity Hub — центра управления разработкой игры. Инструмент также выступает как объединённая утилита для всей экосистемы игрового движка. Source: блог Unity3D Тимур Кондратьев'])]\n"
     ]
    }
   ],
   "source": [
    "newspaper_getter = getNewsPaper()\n",
    "a = newspaper_getter.getPage(\"https://tproger.ru/category/news/\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Работаем в терминале Linux как профи: подборка полезных команд'], ['Видели про-юзеров Linux, которые эффективно работают в терминале? Хотите так же? Не вопрос: в этой статье мы собрали некоторые из полезных команд, которые помогут вам сэкономить время и повысить продуктивность. Когда вы начинаете что-то вводить в терминале, вы можете нажать Tab и вам будут предложены возможные варианты продолжения, которые начинаются с введённой вами строки. Например, если вы хотите скопировать файл с именем file1.txt, вы можете ввести только cp f, нажать Tab и увидеть возможные варианты. Также Tab можно использовать для автодополнения команд. Представьте ситуацию, когда вы спустились глубоко по иерархии папок, затем перешли в папку, которая находится в совершенно другом месте, а потом поняли, что вам нужно вернуться обратно. В таком случае вам всего лишь нужно ввести следующую команду: cd - Она вернёт вас в последнюю рабочую директорию и вам не придётся вручную вводить длинный путь. Примечание Последняя рабочая директория хранится в переменной окружения OLDPWD; вы можете использовать эту переменную для своих целей (попробуйте echo $OLDPWD) или даже подложить команде cd - другой путь (OLDPWD=/usr/bin cd -). Это слишком очевидно. Чтобы вернуться в домашнюю директорию из любого места, вы можете использовать следующую команду: cd ~ А вообще, можно ограничиться командой cd и получить тот же результат. В большинстве современных дистрибутивов Linux эта команда должна сработать. Вы, наверное, догадываетесь, какая команда нужна для отображения содержимого каталога. Всем известно, что для этого можно использовать ls -l. Однако не все знают, что можно обойтись командой ll. Конечно, всё зависит от дистрибутива, но в большинстве случаев вы сможете воспользоваться этой командой. Примечание На самом деле, ll является не отдельной командой, а псевдонимом для ls -l. Допустим, вам нужно запустить несколько команд одну за другой. Что вы будете делать? Подождёте завершения первой команды, а затем запустите следующую? Вместо этого вы можете использовать разделитель ;. Таким образом можно запустить несколько команд на одной строке. Вам не нужно ждать, пока какая-то из команд завершит свою работу, чтобы запустить следующую. command_1; command_2; command_3 Примечание При запуске команд таким образом, они выполняются не параллельно, а последовательно. Если вам нужен именно первый вариант, то используйте конструкцию (command_1 &); (command_2 &). Как запускать несколько команд за раз вы уже знаете. А как убедиться, что команды не завершились с ошибкой? Допустим, вы хотите собрать код и запустить его в случае успешной сборки. В этом случае вы можете использовать разделитель &&, который запускает следующую команду только при условии, что предыдущая успешно завершилась. command_1 && command_2 Как пример использования && можно привести команду sudo apt update && sudo apt upgrade для обновления системы через терминал на системах, основанных на Debian. Есть несколько способов «убить» программу. Команда killall сделает это по имени, а kill требуется номер процесса. Например, killall chrome убьёт все процессы chrome. Также можно послать любому процессу сигнал прерывания (как Ctrl+C) с помощью kill -INT <номер процесса>. Чтобы поставить работающую команду на паузу нажмите комбинацию Ctrl+Z, а чтобы продолжить — %. Представим ситуацию, когда вы воспользовались какой-то командой пару часов назад и снова хотите её использовать, но не можете вспомнить название. Здесь поможет обратный поиск. С его помощью можно по заданному условию найти команду в истории. Просто нажмите комбинацию Ctrl+R и введите часть команды. Затем вам будут показаны команды из истории, которые удовлетворяют заданному условию. Ctrl+R <условие> По умолчанию показывается только один результат. Чтобы увидеть больше результатов, нужно повторно нажать Ctrl+R. Чтобы выйти из поиска, нажмите Ctrl+C. Учтите, что в некоторых оболочках Bash можно использовать Page Up и Page Down с условием поиска для автодополнения команды. Многие привыкли использовать комбинацию Ctrl+S для сохранения. Однако после её использования в терминале, он часто зависает. Чтобы вернуть его в нормальное состояние, нажмите комбинацию Ctrl+Q. Допустим, вы вводите длинную команду и вдруг понимаете, что вам нужно что-то изменить в её начале. Чтобы попасть в начало или конец строки вы можете несколько раз нажать клавишу стрелки влево/вправо или Home/End. А можете нажать Ctrl+A или Ctrl+E. В ситуациях, когда вам нужно анализировать логи при запущенном приложении, можно использовать команду tail с флагом -f. tail -f <путь к лог-файлу> Также можно использовать регулярные выражения в grep, чтобы выводить только нужные строки: tail -f <путь к лог-файлу> | grep <регулярное выражение> Кроме того, вы можете использовать флаг -F, чтобы tail продолжал работу даже в случае удаления лог-файла. Таким образом, когда лог-файл снова будет создан, tail продолжит логирование. Если вы хотите просматривать системный лог в реальном времени, воспользуйтесь аналогичной опцией -f команды journalctl: journalctl -f Серверные логи обычно сжимаются gzip’ом для сохранения дискового пространства. Это становится проблемой для разработчика или сисадмина, который анализирует эти логи. Возможно, вам придётся скопировать архив в другое место, а затем извлечь его, так как не всегда есть права на извлечение логов. К счастью, в таких ситуациях всегда спасут z-команды. Они являются альтернативами обычных команд, которые используются для работы с логами вроде less, cat, grep. Поэтому вы можете воспользоваться zless, zcat, zgrep и т.д., даже не извлекая логи. Команда cat не всегда лучший выбор для отображения содержимого файла, особенно если он большой — cat выведет сразу весь файл. Вы можете использовать Vi, Vim или другой терминальный текстовый редактор, но если вам просто нужно прочитать файл, то команда less подойдёт гораздо лучше. less <путь к файлу> В less можно искать нужную подстроку, листать по страницам, отображать номера строк и не только. А ещё less может читать не только текстовые документы, но ещё и архивы и другие типы файлов. Использование аргумента предыдущей команды может пригодиться во многих ситуациях. Например, вы создали директорию и вам нужно сразу в неё перейти. Ещё лучше использовать alt+. . Множественное нажатие точки позволяет выбрать аргумент одной из нескольких предыдущих команд. С помощью !! можно вызвать даже всю предыдущую команду. Это особенно полезно в тех случаях, когда оказывается, что для запуска команды нужны рут-привилегии. Быстрое sudo !! позволяет сэкономить немного времени. Вероятно, вы уже знаете, зачем нужна команда alias. Её можно приспособить для исправления опечаток. Представим, что вместо grep вы часто пишете gerp. Если вы установите псевдоним следующим образом, то вам не придётся больше перепечатывать команду: alias gerp=grep К слову, для исправления опечаток не обязательно использовать псевдонимы — утилита The Fuck сама исправит предыдущую команду. Чтобы выключить компьютер из терминала, введите poweroff, а для перезагрузки — reboot. Здесь не всё однозначно, так как между дистрибутивами Linux и терминалами есть определённая разница. Но в общем случае вставить текст можно одним из следующих способов: Возможно, это слишком очевидно. Если у вас запущена команда, работу которой вы хотите завершить, просто нажмите Ctrl+C и команде будет отправлен сигнал прерывания (SIGINT). А если вы хотите быстро покинуть терминал, нажмите комбинацию Ctrl+D, которая для баша и других интерактивных программ означает окончание ввода. Команда yes может пригодиться, если какой-то скрипт/команда требует взаимодействия с пользователем, которое заключается только в нажатии Y каждый раз. yes | <команда или скрипт> Если вам нужно только очистить содержимое файла, а не удалить его, вы можете сделать это следующим образом: > имя_файла В терминале Linux можно искать разными способами. Если вам нужно узнать, есть ли файл(ы) с определённым текстом, можете воспользоваться этой командой: grep -Pri <текст_для_поиска> <путь_к_директории> Почти все команды/инструменты командной строки содержат справку с указаниями по работе. Чтобы получить справку, воспользуйтесь этой командой: <команда> --help Кроме того, порой можно получить более подробную справку с помощью команды man <команда>. Если вы хотите взглянуть на все команды, которые вы когда-либо запускали, введите history. Если вам нужен не полный список, а только несколько последних, воспользуйтесь командой fc -l. При получении команд одним из вышеуказанных способов рядом с каждой командой находится её номер в истории. Чтобы быстро запустить команду из этого списка просто введите !<номер команды>. Если вы хотите выполнить команду так, чтобы она не сохранилась в истории, просто введите пробел перед командой. Чтобы поднять сервер и сделать доступной текущую директорию по адресу http://localhost:8000/ введите python3 -m SimpleHTTPServer. Если зажать Ctrl, а затем нажать по очереди X и E, то откроется текстовый редактор, в котором можно будет спокойно записать длинную команду, а после выхода из него — выполнить её. Если вы вывели в терминал сырые бинарные данные или ещё что-то, что выводить не стоило, то убрать увиденную абракадабру позволит команда reset. Чтобы получить информацию о текущих смонтированных файловых системах с удобным оформлением по столбцам, введите команду mount | column -t. Также вы можете воспользоваться командой findmnt, которая отображает информацию в виде красивого дерева и сама форматирует столбцы, а также может найти нужную файловую систему: Есть инструмент pstree, который умеет рисовать красивые деревья процессов. Например: Чтобы заблокировать экран, используйте команду $ loginctl lock-session. Для разблокировки экрана введите команду $ loginctl unlock-session. Конечно, вряд ли вы сможете использовать терминал при заблокированном экране, однако вы можете пойти обходными путями. Например, можно создать задачу для разблокировки через какое-то время. Чтобы запустить шелл от имени суперпользователя, можно воспользоваться командой sudo -s.  Во многих источниках можно встретить вариант sudo su, который тоже работает, но медленнее, так как запускает лишний процесс. Команда xdg-open позволяет открыть любой файл в соответствующей программе. Например, xdg-open file.txt откроет файл в текстовом редакторе по умолчанию. Чтобы посмотреть статус системных сервисов, введите команду systemctl status или systemctl status <имя сервиса>, если вас интересует конкретный сервис. Если вам нужно найти файл, но вы не знаете, где конкретно он находится, можно воспользоваться командой find. Например: Чтобы использовать вывод одной команды в качестве аргумента другой, используйте конструкцию команда-2 $(команда-1). Например: Команда cal может нарисовать календарь на текущий месяц (и даже выделить текущее число) или на другой промежуток: Чтобы выполнить какую-то команду в нужное вам время, воспользуйтесь at: echo команда-для-выполнения | at время_выполнения Учтите, что эта команда может отсутствовать на вашей системе, и вам придётся установить её самостоятельно. Чтобы получить свой внешний IP-адрес введите curl ifconfig.me или curl ipinfo.io/ip. Возможно, сначала вам придётся установить curl. Введите команду curl wttr.in/<нужный город> и получите красивую таблицу с прогнозом погоды: Чтобы получить быстрый доступ к таблице ASCII просто введите man ascii. Небольшие выражения можно вычислять прямо в терминале. Это можно сделать либо с помощью конструкции echo <выражение> | bc, либо echo $((<выражение>)). Например: Если вы хотите выполнить команду в другой директории, но при это не хотите покидать текущую, то просто оберните команду скобками. Например, (cd /tmp && ls). Здесь скобки запускают подшелл (subshell), внутри которого мы и выполняем cd. Чтобы узнать, какие динамические библиотеки нужны программе или библиотеке и как они будут разрешены при запуске, используйте команду ldd: Чтобы определить тип содержимого, находящегося в файле, используйте команду file: Чтобы запускать команду каждые несколько секунд (по умолчанию две) и смотреть на её вывод, воспользуйтесь командой watch <команда>. Чтобы узнать, сколько времени уходит на запуск команды, используйте time <команда>. Чтобы узнать, какие системные вызовы совершает программа, введите strace <команда>. Если вы набираете какую-то команду и вам по какой-то причине нужно прерваться и, например, ввести другую команду, вы можете «запомнить» то, что вы ввели комбинацией Ctrl+U, а затем вставить комбинацией Ctrl+Y. Чтобы выкачать сайт используйте команду wget --random-wait -r -p -e robots=off -U mozilla <адрес сайта>. С помощью команды ssh -N -L 2001:localhost:80 somemachine можно создать туннель от 80 порта на удалённой машине до 2001 на локальной. А какие полезные команды знаете вы? Делитесь в комментариях. На основе «20 Linux Command Tips and Tricks That Will Save You A Lot of Time» Никита Прияцелюк, последний центурион'])\n"
     ]
    }
   ],
   "source": [
    "test = getNewsPaper()\n",
    "a = test.getOneArticle(\"https://tproger.ru/articles/useful-linux-commands/\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(['Низкоуровневая модель памяти'], ['В прошлой статье была рассмотрена высокоуровневая часть модели памяти. В этой статье подробно описано, что на самом деле происходит с памятью в компьютере на примере Intel x86_64. Схематично и очень упрощённо модель памяти выглядит так:  Статья детально описывает эту модель, а также принципы работы виртуальной памяти в защищённом режиме. Регистр процессора — блок памяти, расположенный прямо на кристалле процессора и образующий сверхбыструю оперативную память (СОЗУ). В x86_64 размеры регистра, за исключением векторных, варьируются от 8 до 64 бит. Программа может быстро работать только с памятью, находящейся в регистрах процессора. Даже для простого инкремента ячейки памяти из RAM (Random Access Memory) необходимо выполнить 3 операции: сначала загрузить эту ячейку из RAM или кэша в регистр, затем инкрементировать и после выгрузить ячейку из регистра обратно в RAM или кэш. Количество и специализация регистров зависит от архитектуры процессора, но типы регистров можно разделить на две группы: регистры общего назначения, необходимые для хранения и использования переменных, принадлежащих программам, и специализированные регистры, хранящие различную метаинформацию, которая может даже менять поведение процессора. Если с регистрами общего назначения всё понятно, то вот некоторые специализированные регистры в x86_64 и их назначение: Также существуют векторные регистры общего назначения (ZMM0–ZMM31), необходимые для математических операций над несколькими числами (вектором) одновременно. Многогигабайтную память в процессор не засунуть, поэтому в процессоре есть относительно небольшое количество регистров и кэш, а остальная память хранится в RAM. Но RAM имеет очень большую задержку (около 230 тактов процессора i7-4770), поэтому часть этой памяти кэшируется в самом процессоре. Кэширование многоуровневое, чем меньше память и непосредственно ближе к регистрам, тем проще её адресовать и тем она быстрее. L1-кэш находится в каждом ядре процессора, кэширует как данные, так и инструкции (которые в свою очередь тоже являются данными; один и тот же набор двоичного кода может быть интерпретирован и как данные, и как инструкции). Задержка минимальна — 4 такта на i7-4770 и размер 64 КБ (32 КБ на данные и 32 КБ на инструкции) на каждое виртуальное ядро. L2-кэш также в каждом ядре, но кэширует только данные — 12 тактов и 1 МБ на ядро. L3-кэш общий для всего процессора, кэширует только данные — 36 тактов и 8 МБ. Задержка в тактах примерная и не отражает реального положения дел, цифры приведены на основе бенчмарков только для того, чтобы показать относительную разницу кэшей и RAM. Кэш L1 представляет собой таблицу из сетов, каждый из которых формируется из записей, состоящих из битов состояния, тега и кэш-линии. Вот урезанная версия такой записи:  Индекс — это номер сета в таблице, а также с 6-го по 11-й биты указателя на память в RAM. Поэтому от расположения в RAM (номер адреса) зависит то, в каком сете будет кэшироваться этот блок памяти размером в кэш-линию. С 0-го по 5-й биты — это смещение внутри кэш-линии. Сет состоит из нескольких записей, в случае с Haswell их 8. Тег — это остальная часть указателя с 12-го бита для нахождения нужной записи в сете. Биты состояния указывают на то, модифицировалась ли кэш-линия, а также актуальность кэш-линии. У каждого ядра свой L1- и L2-кэш, поэтому их нужно синхронизировать. Для этого и существуют биты состояния. Более подробно на Хабре. Кэш-линия — это несколько физически подряд расположенных байтов из RAM, в нашем случае 64 байта. В i7-4770 128 сетов в L1-кэше на одно виртуальное ядро — 64 на данные и 64 на инструкции. Поэтому для индексирования этих кэш-сетов необходимо 6 бит (2^6 = 64).\\nВсего выходит 1024 записи (128 сетов по 8 записей), в каждой из которых по 64 байта в кэш-линии. В сумме 64 КБ L1-кэша на ядро. Здесь есть небольшая проблема: если произвести небольшие вычисления, можно понять, что каждые 64 КБ в памяти будут кэшироваться в одном и том же сете. Поэтому использование только каждых 64 КБ или бо́льшую степень двойки, полностью нивелирует профит от использования L1-кэша. Это доказывается многими бенчмарками, но в реальной работе никто так делать не будет. Кэши прочих уровней имеют эту же проблему, только для индекса используются больше бит. При попытке двух ядер одновременно инкрементировать один и тот же байт RAM памяти, допустим, равный 0, в зависимости от «фазы луны» мы можем получить любое число от 0 до 2. Например, оба процессора прочитали из RAM 0, затем оба инкрементировали свои регистры до 1, затем оба записали 1 обратно в RAM. Это одна из самых основных проблем многопоточных программ и разных программ с общей памятью. Решается это добавлением префиксного байта LOCK в начало ассемблерной инструкции (например xaddl), что делает её атомарной. Данная операция посылает сигнал в другие ядра, запрещая использование данных, пока не будет завершена ассемблерная инструкция. Таким образом, пока одно ядро занято инкрементом, другое будет ждать. Во времена DOS-систем процессоры работали в «реальном режиме», где используется сегментная адресация памяти, при которой любому процессу доступна вся память компьютера, и он может выполнить любую допустимую инструкцию процессора. Когда процесс работает в среде ОС монопольно, это очень выгодно с точки зрения производительности, но появляются проблемы при нескольких работающих одновременно процессах. Для того, чтобы один процесс не смог случайно или специально несогласованно использовать память другого процесса или исполнить привилегированную инструкцию, которая может повлиять на работу ОС и других процессов, и был придуман «Защищённый режим». Для этого во все операции необходимо добавить проверку прав доступа. Права доступа на инструкции определяются через кольца защиты, а памяти — через виртуальную адресацию. За исключением некоторых тонкостей, ненужных в этой статье, для переключения в защищённый режим x86_64 (есть ещё классический защищённый режим x86) необходимо установить биты PE (Разрешение защиты, бит 0) и PG (Страничный режим, бит 31), включающий MMU, в регистре CR0, а также указатель на таблицу PML4 в регистре CR3 (c 13 по 63 бит). Именно в таблице PML4 и хранится информация об адресах, правах доступа и прочем. В процессорах x86 используется система колец защиты. Каждая инструкция имеет свои требования к правам процесса. Текущий уровень прав выполняющегося процесса указан в таблицах дескрипторов, указатели на которые лежат в соответствующих регистрах. В x86 существует 4 кольца защиты, от полных прав доступа (0) до пользовательских прав (3). Но для переносимости на другие платформы некоторые ОС используют только два кольца. Даже Windows NT, которая раньше была очень даже кроссплатформенной, сейчас имеет редакцию только для x86 и ARM-систем. Виртуальная адресация — метод управления памятью компьютера, позволяющий выполнять программы, требующие больше оперативной памяти, чем имеется в компьютере, а также изолировать память процессов друг от друга. Виртуальный указатель не соответствует физическому указателю на ячейку в RAM. Это сделано для того, чтобы виртуальная память могла быть больше размера реальной RAM памяти, например для файла подкачки (физическая память + подкачка явно больше, чем просто физическая память) или поблочного отображения файлов и устройств в память. А также для того, чтобы при разыменовании указателя проходил процесс проверки прав доступа к этой памяти при трансляции адреса из виртуального в физический, чтобы случайно или специально не залезть в чужую память. В защищённом режиме сам процессор и L1-кэш работают с виртуальным адресом, а L2, L3 и RAM — с физическим. Трансляцией адресов из виртуального в физический, а также проверкой прав доступа занимается MMU (Memory Management Unit) — блок управления памятью, который использует PML4. Таблица выглядит следующим образом:  Виртуальный указатель делится на две части: с 12-го по 47-й бит — сдвиги внутри таблиц в PML4, с 0-го по 11-й бит — сдвиг внутри физического адреса для поддержки арифметики указателей. Указатель на начало PML4-таблицы лежит в CR3 с 12-го по 51-й биты. С 39-го по 47-й бит виртуального указателя — номер страницы из PML4-таблицы, в которой лежит указатель на начало PDPT-таблицы. С 30-го по 38-й бит находится номер страницы из PDPT таблицы, в которой лежит указатель на начало PD. Аналогично и с PT, но в ней уже лежит физический адрес, к которому добавляется сдвиг виртуального указателя — с 0-го по 11-й биты. В итоге получается физический адрес, по которому MMU и обращается к физической памяти. Внимательный читатель мог заметить две сложности: TLB-кэш внутренне схож с LX-кэшами, только служит для трансляции виртуальных адресов и представляет собой таблицу из сетов, внутри которых записи: «виртуальный адрес ↔ физический адрес». PML4 адресована только по физическим адресам, поэтому сама PML4 может кэшироваться только в L2-L3 кэшах. Сдвиг внутри страницы — 12 бит, поэтому минимальный размер такой страницы — 4 КБ. Процессу очень редко нужны сразу 4 КБ памяти (а меньше выделить у ОС нельзя). Поэтому процессы используют аллокаторы, о которых было сказано в прошлой статье. Для увеличения скорости трансляции виртуальных адресов, можно убрать некоторые промежуточные таблицы, но в таком случае освободившиеся биты будут использованы для сдвига внутри страницы, что увеличивает размер таких страниц. В x86_64 размеры страниц бывают 4 КБ (12 бит), 2 МБ (12 + 9 бит вместо PT), 1 ГБ (12 + 9 + 9 бит вместо PD). Использование большого количества 4 КБ страниц может привести к сильному увеличению размера PML4, которая перестанет помещаться в L2-L3 кэши, что сильно увеличит время разыменования. TLB представляет собой схожую с LX-кэшами структуру. В процессоре Intel Haswell существует два TLB разного уровня. Индексация и тегирование производятся аналогично, но кэш-линия заменена на адрес физической таблицы и её атрибуты и права доступа из PML4, но не всегда полностью (об этом в разделе про Meltdown). TBL тоже многоуровневый и тоже отдельно кэширует указатели на данные и указатели на инструкции. TLB первого уровня в Haswell делится на ITLB (instructions) и на DTLB (data). Минимальная задержка — 1 такт, размер сета — 4 записи, только для 4 КБ сегментов, либо только для 2 МБ сегментов (зависит от ОС и её настроек). Размер — 32 записи для 2 МБ сегментов и 64 — для 4 КБ. TLB второго уровня (STLB) — только для данных, задержка — 22 такта, размер сета — 8 записей, размер — 1024 записи для 2 МБ и 4 КБ сегментов одновременно. При смене контекста TLB приходится сбрасывать из-за возможного совпадения виртуальных адресов. По отношению к виртуальной адресации кэши данных и инструкций могут быть поделены на 3 используемых в реальных процессорах типа. Physically indexed, physically tagged (PIPT) — физически индексируемые и физически тегируемые (тег и индекс берутся из физического адреса). Такие кэши просты и избегают проблем с наложением (aliasing), но они медленны, так как перед обращением в кэш требуется запрос физического адреса в TLB. Этот запрос может вызвать промах в TLB и дополнительное обращение в PML4.\\nL2 и L3 расположены после MMU, поэтому они могут использовать только этот тип. L1 остаётся либо каждый раз лезть в MMU, что слишком долго, либо использовать другой тип, что L1 и делает. Virtually indexed, virtually tagged (VIVT) — виртуально индексируемые и виртуально тегируемые. И для тегирования, и для индекса используется виртуальный адрес. Благодаря этому проверки наличия данных в кэше проходят быстрее, не требуя обращения к MMU. Однако возникает проблема наложения, когда несколько виртуальных адресов соответствуют одному и тому же физическому. В этом случае данные будут закэшированы дважды, что сильно усложняет поддержку когерентности. Другой проблемой являются омонимы, ситуации, когда один и тот же виртуальный адрес (например, в разных процессах) отображается различные физические адреса. Становится невозможно различить такие отображения исключительно по виртуальному индексу. Возможные решения: сброс кэша при переключении между процессами, требование непересечения адресных пространств процессов, тегирование виртуальных адресов идентификатором адресного пространства (address space ID, ASID) или использование физических тегов. Также возникает проблема при изменении отображения виртуальных адресов в физические: требуется сброс кэш-линий, для которых изменилось отображение. Virtually indexed, physically tagged (VIPT) — виртуально индексируемые и физически тегируемые. Для индекса используется виртуальный адрес, а для тега — физический. Преимуществом над первым типом является меньшая задержка, поскольку можно искать кэш-линию одновременно с трансляцией адресов в TLB, однако сравнение тега задерживается до получения физического адреса. Преимуществом над вторым типом является обнаружение омонимов (homonyms), так как тег содержит физический адрес. Для данного типа требуется больше бит для тега, поскольку индексные биты используют иной тип адресации. Но даже у VIPT-кэша есть проблема: при изменении кэш-записи в кэше одного ядра или в RAM придётся искать эту же кэш-запись в кэшах других ядер для инвалидации. Но искать придётся по физическому адресу (два одинаковых виртуальных адреса у разных процессов могут указывать на разную физическую память), поэтому использование индексации по виртуальному адресу заставляет пробегать весь кэш и сравнивать каждый тег. В процессорах Intel это решается индексацией и по виртуальному, и по физическому адресу. Из-за того, что биты с 0-го по 11-й — это смещение внутри страницы в таблице PML4, они совпадают у физического и виртуального адреса. Поэтому используемые для индекса с 6-го по 11-й биты указателя совпадают и у физического, и у виртуального адреса. Благодаря этому L1-кэш имеет тип как PIPT, так и VIPT, но его максимальный размер в процессорах Intel равен минимальному размеру страницы из таблицы PML4, умноженному на количество записей в сете кэша: 64 КБ * 8 = 512 КБ — L1-кэш на все ядра и 64 КБ — L1 на одно ядро. Но увеличение размера записей в сете увеличивает задержку. Поэтому в современных Intel процессорах L1-кэш особо не растёт, ибо это увеличит задержку. Процесс трансляции В защищённом режиме разыменование идёт следующим образом: 1. Идём параллельно в L1 и в TLB. Далее весь процесс делится на две части: поиск нужной кэш-линии (п. 2) и трансляция виртуального адреса (п. 3). 2.1. Берём индекс из виртуального указателя и ищем нужный сет. Далее ждём, пока закончится процесс трансляции виртуального адреса (п. 3). 2.2. После получения физического адреса сравниваем теги в записях внутри сета. Если нашлось совпадение, это нужная нам кэш-линия. Если нет, идём дальше в L2. 2.3 Производим аналогичные действия, но уже с кэшами L2 и L3. В них используются только физические адреса, благо такой мы уже получили, ведь процесс трансляции (п. 3) закончился до начала этапа (п. 2.2). 2.4 Если в кэшах не нашлось, идём в медленный RAM. 3.1 Идём в TLB и сравниваем индексы и теги по аналогии с кэшами. Если нашли совпадение, возвращаемся в этап 2.1. 3.2 Если нет (такая ситуация называется Page walk), берём физический адрес из регистра CR3 и повторяем этапы 2.3–2.4, но для поиска и чтения PML4. Если нашли совпадения в таблице, проверяем права доступа и возвращаемся на этап 2.1 с полученным физическим адресом. 3.3 Если совпадения в таблице нет, либо были нарушены права доступа (такая ситуация называется Page fault), например, попытка произвести запись, когда доступно только чтение, то генерируем прерывание. Далее вызывается обработчик прерываний ОС, который решает, что делать с этой ситуацией. Вот тут мы и подходим плавно к высокоуровневой памяти. Перед запуском процесса ОС выделяет ему сегменты необходимого размера в PML4 (выбирая среди 4 КБ, 2 МБ, 1 ГБ) для стека и статической памяти. У каждого процесса своя таблица PML4. При смене контекста ОС выставит указатель в CR3 на нужную таблицу. При необходимости процесс может запросить у ОС дополнительный сегмент, например, через системный вызов mmap. ОС при возможности добавит запись в PML4, а также в свои таблицы и вернёт указатель на начало этой памяти процессу, иначе — вернёт нулевой указатель. Получается, что на низком уровне автоматическая, статическая, динамическая память — это всё сегменты. PML4 изначально была создана для виртуальной памяти, но ОС использует её и для других задач. Файл подкачки Часть памяти может находиться в файле подкачки. При нехватке памяти ОС может часть этой памяти перенести на диск. Вместе с этим ОС удаляет запись из PML4, чтобы при попытке обратиться к этой памяти был сгенерирован Page Fault. Это было сделано для того, чтобы при попытке обратиться к памяти, которая была отгружена на диск, вызвать ОС, которая загрузит память из диска обратно в ОС, вернёт запись в PML4 и передаст управление обратно процессу, который на сей раз благополучно повторит процесс разыменования. Файл или устройство, отображённое в память mmap можно использовать и для отображения файла (в *nix системах устройства — тоже файлы) в память. Для этого создаётся буфер в физической памяти, в которую записывается часть файла, и создаётся запись в PML4. Если процесс выйдет за границу виртуальной памяти в PML4, то ОС загрузит новый кусок файла в буфер и отредактирует PML4, сдвинув виртуальный указатель, но оставив физический. В итоге для процесса внешне будет казаться, что весь файл загружен в память, когда на самом деле только часть этого файла в буфере. COW память и ленивая аллокация При вызове mmap в некоторых ОС никакой памяти на самом деле не выделяется, только добавляется запись в структурах самой ОС. При первом обращении процессом к ещё не аллоцированной памяти будет Page fault, во время которого ОС произведёт полную аллокацию. Это удобно в случае, когда процесс аллоцировал память, но никогда её не использовал. В таком случае пройдёт только первый, ленивый и относительно быстрый этап аллокации. С COW дела обстоят схоже. Copy On Write память — это память, общая для чтения у нескольких объектов в системе (под объектами имеется в виду что угодно, начиная от объектов в ООП и заканчивая процессами с разделяемой памятью). Если же один из объектов начинает писать в память, то перед этим такая память делится на две части: одна остаётся у всех остальных объектов, а другая становится памятью только для объекта, который хочет её изменить. В итоге мы получаем экономию памяти, а в случае необходимости записи в эту память, мы её копируем. У всех процессов с COW-памятью в PML4 ставится флаг «только чтение» для того, чтобы при записи вызвать ОС через Page Fault для разделения памяти. Реальные же права доступа на запись ОС хранит в своих таблицах. Это были несколько примеров того, как ОС по-хитрому использует PML4 для своих нужд. Во многих современных процессорах обнаружили уязвимость в этой системе виртуальных адресов и кэшей, которую назвали Meltdown. Перед тем как продолжить, рассмотрим немного подробнее таблицу PML4. В ней и в каждой подтаблице есть не только адрес следующей таблицы, но и биты с метаинформацией этой таблицы. Рассмотрим некоторые из них.  Наконец, перейдём к самому принципу работы уязвимости Metldown. Ранее уже было написано о спекулятивном исполнении команд в статье об архитектуре процессора. Цитата из статьи: «Спекулятивное исполнение команд — это выполнение команды до того, как станет известно, понадобится эта команда или нет». Суть атаки в том, что современные ОС всё своё адресное пространство отображают в адресном пространстве каждого пользователя (процесса), другими словами добавляют в PML4-таблицу процесса указатели на подтаблицы ОС, но с флагом U/S, установленным в 0. Это сделано для того, чтобы при переключении контекста процесса уменьшить количество операций перезаписи CR3 и улучшить кэширование данных ОС с чистыми VIPT- или VIVP-кэшами (PML4 первого процесса → PML4 второго процесса вместо PML4 первого процесса → PML4 ОС → PML4 второго процесса), но эту защиту можно обойти в некоторых процессорах Intel и ARM (у AMD обнаружили только уязвимость Spectre, у которой другой принцип работы). Некоторые ОС используют одну PML4-таблицу на всю систему, а при переключении контекста меняют только подтаблицы. Отсюда в той же Windows максимальный размер ОЗУ 256 ГБ — максимальный размер таблицы PDP. При спекулятивном исполнении в уязвимых процессорах часть команд может выполниться до того, как будет проверено, стоит ли бит 0 в U/S флаге, причём благодаря спекуляции код атаки может быть выполнен из недостижимого кода, например, в ветке else при всегда истинном условии в if, перед тем, как процессору станет известно, что этот код недостижим. После того, как процессор поймёт, что доступа к этой памяти нет, он обнулит результат команд доступа к памяти, сбросив соответствующие регистры, но TLB- и L1-кэш он сбрасывать не будет. А в них уже не хранится флаг U/S, поэтому процесс может получить полный доступ к успевшей закэшироваться закрытой памяти ядра ОС, ведь процессор, который нашёл нужную запись в L1 и TLB, не будет проверять её в PML4. Решается это проблема либо аппаратно: добавлением флагов из PML4 в TLB, либо программно: удаление таблиц памяти ОС из PML4-процессов, что увеличит время смены контекста и системных вызовов. Это далеко не всё, что можно рассказать про низкоуровневую память и уж тем более про всю архитектуру ПК пусть даже одного x86_64. Это лишь некоторая часть базы, сильно привязанная к конкретной архитектуре, в других архитектурах используются схожие, но не одинаковые подходы. Для более подробного изучения этой темы вы можете прочитать литературу, например, трёхтомник Э. С. Таненбаума или документы от Intel про x86_64 — благо они есть в свободном доступе — или другую микроархитектуру, а также посмотреть доклад с конференции C++ Russia 2018 для уточнения некоторых мелочей про кэши и виртуальную адресацию, не описанных в этой статье. Иван Борисов Никита Прияцелюк, последний центурион']), (['Руководство по Windows PowerShell для начинающих'], ['PowerShell — это объектно-ориентированный программный движок и скриптовый язык с интерфейсом командной строки, предоставляющий IT-профессионалам более широкие возможности для конфигурирования операционных систем семейства MS Windows. Проще говоря, это своего рода универсальный инструмент администрирования. В данной статье будут рассмотрены базовые приёмы написания скриптов на PowerShell, позволяющие простым путём автоматизировать управление Windows-окружением. PowerShell предлагает как чисто консольный интерфейс, так и полноценную среду разработки PowerShell ISE (Integrated Scripting Environment, встроенное скриптовое окружение) для скриптов. Для запуска интерфейса командной строки введите powershell в меню «Выполнить» (WinKey + R). PowerShell ISE запускается с помощью команды «PowerShell ISE» в том же меню. ISE более предпочтительно, так как предоставляет более широкие возможности разработчику благодаря подсветке синтаксиса, функции автозаполнения кода и другим особенностям, присущим многим «большим» IDE.  Скрипты сохраняются в виде файлов с расширением .ps1. Несмотря на то, что PowerShell уже давно является нативной частью ОС Windows, вы не сможете запустить его скрипты простым двойным щелчком. Для этого надо кликнуть правой кнопкой по скрипту и выбрать «Запустить в PowerShell». Также существуют системные политики, ограничивающие выполнение скриптов. Можно проверить текущие параметры политики, введя команду Get-ExecutionPolicy. Результатом будет одно из следующих значений: Для начала работы необходимо изменить настройку политики запуска на RemoteSigned, используя команду Set-ExecutionPolicy: Командлеты — это команды с предопределённой функцией, подобные условным операторам в языках программирования. У них есть несколько ключевых особенностей: Каждый командлет содержит в себе глагол и существительное, разделяемые дефисом. Например: При необходимости список всех доступных командлетов можно вывести с помощью Get-Help-Category:  Также можно создавать и свои собственные командлеты. У каждого командлета есть несколько параметров, определяющих его работу. PowerShell ISE автоматически предлагает все доступные параметры с отображением их типа. Например, Get-Service-NameW* выводит список служб, у которых имя начинается с W. Если вы забыли, какие параметры у введённого командлета, воспользуйтесь Get-Member. Например, Get-Process | Get-Member:  Если вы не нашли того, что нужно, или не уверены в том, как правильно задаются параметры, можно даже запросить примеры с помощью параметра -Examples:  Некоторые командлеты также могут вызываться с помощью алиасов, например вместо Get-Help можно просто написать Help. При написании больших скриптов или коллективной разработке можно пользоваться комментариями. Каждый комментарий начинается с символа #, а блок комментариев ограничивается комбинациями символов <# и #> в начале и в конце соответственно. PowerShell позволяет осуществлять обмен данными между командлетами с помощью конвейера. Например: Можно использовать несколько конвейеров. Например, следующий скрипт выводит список имён всех служб за исключением остановленных: Get-Service | WHERE {$_.status -eq “Running”} | SELECT displayname Итак, благодаря этому руководству у новичков появилось представление о том, что из себя представляет PowerShell. Также мы рассмотрели варианты изменения политики выполнения скриптов, что такое командлет, как они обмениваются данными с помощью конвейера и как получить свойства нужного объекта. Помните, что в случае затруднений можно воспользоваться командлетом Get-Help. Перевод статьи «Windows PowerShell Scripting Tutorial for Beginners» Corewood']), (['Эволюция криптографии: от математики до физики'], ['Нужно ли быть математическим гением, чтобы понять криптографию? Возможно, низкоуровневое понимание нужно разве что криптографу, чья работа состоит в изобретении сложного для взлома алгоритма. Шифрование каждый день используется для защиты банковских карт, удалённого подключения к рабочему месту по сети или интеллектуальной собственности от цифрового пиратства. Цель этой статьи — объяснить изумительную науку криптографию настолько просто, чтобы все могли понять, как она используется для шифрования данных. Слова «криптология» и «криптография» в современной литературе часто используются как взаимозаменяемые термины, но между ними есть семантическая разница. У слов различные значения, которые проще всего объяснить так: Большая часть статьи посвящена «криптографии», как это принято сегодня, и хотелось бы, чтобы вы знали различия и значения обоих слов. Само по себе изучение криптологии как науки существует уже много лет. Первый известный случай использования криптографии был обнаружен в надписи, высеченной в 1900 году до нашей эры, в главной гробнице дворянина Хнумхотепа II, в Египте. Писец (да) то тут, то там использовал некоторые странные иероглифы вместо обычных. Целью было скорее не спрятать сообщение, а изменить его форму, чтобы оно выглядело более величавым.  Во время расцвета Римской Империи (100-й год до нашей эры) Юлий Цезарь, как известно, использовал форму шифрования для передачи секретных сообщений своим армейским генералам во время войны. Этот шифр подстановки (шифр Цезаря), вероятно, самый упоминаемый в литературе исторический шифр (шифр — алгоритм, используемый для шифровки или дешифровки сообщений). В шифрах подстановки каждая буква исходного сообщения (исходное сообщение — текст, который будет зашифрован) заменяется на другую букву, находящуюся правее или левее исходной по алфавиту, для получения шифротекста (шифротекст — зашифрованное сообщение). В шифре Цезаря используется сдвиг вправо на 3 позиции, так что «A» становится «D», «B» — «E» и т. д. Алфавит в данном случае будет замкнут и после «X» следует «A». Во время Второй мировой войны морские пехотинцы США завербовали и обучили людей из племени индейцев навахо, свободно владеющих языком навахо. Это был привлекательный способ использования кодирования, мало людей не из племени знали этот язык, а также не было никаких опубликованных книг на языке навахо. Речевой язык навахо был не очень сложен по криптографическим стандартам и, вероятно, был бы быстро взломан при взаимном сотрудничестве носителя языка и обученных криптографов. У японцев была возможность сделать это, когда в 1942 году на Филлипинах они пленили Джо Кийомию во время Марша смерти в Батаане. После попадания в плен Кийомия, навахо-сержант в армии США, но не носитель кода, получил приказ расшифровать перехваченное радиосообщение. Однако, так как Кийомия не был обучен коду, используемому военными службами США, то слова ему показались ничего не значащими. Когда он доложил, что не может понять перехваченные сообщения, его начали пытать. Японская Имперская Армия и ВМС так и не взломали устный код. В начале 1970-х компания IBM осознала, что их пользователи требуют какой-нибудь вид шифрования, и сформировала «криптогруппу», возглавляемую Хорстом Фейстелем. Они спроектировали шифр под названием Lucifer. В 1973 году Национальное бюро стандартов США (сейчас NIST — Национальный институт стандартов и технологий США) объявило конкурс на блочный шифр, который должен был стать национальным стандартом. Видимо, они наконец-то осознали, что покупают множество коммерческого ПО без хорошей криптографической поддержки. В конце концов, Lucifer приняли и переименовали в DES (Data Encryption Standard). DES был взломан методом полного перебора в 1997 году. Главной уязвимостью DES была маленькая длина ключа. Когда вычислительная мощность компьютеров увеличилась, то метод полного перебора стал возможен для получения исходного сообщения. И если в 1980-х годах DES был единственным вариантом, то сейчас это изменилось. Сегодня у нас есть широкий выбор более сильных, быстрых и улучшенных алгоритмов. Сейчас проблематично выбрать среди них нужный. В 1997 году NIST объявил конкурс на новый блочный шифр и получил 50 заявок. В 2000 году был принят Rijndael и назван AES (Advanced Encryption Standard). Шифрование — это процесс изменения данных таким образом, чтобы они стали неузнаваемыми и бесполезными для несанкционированного лица. А дешифрование — процесс превращения данных в их первоначальный вид. Наиболее безопасные виды шифрования используют математические алгоритмы и переменную — «ключ». Выбранный ключ (зачастую любая случайная последовательность) вводится при шифровании и является неотъемлемой частью изменения данных. Тот же самый ключ необходим для дешифровки сообщения. Это основа защиты, если ключ известен только уполномоченному лицу (лицам), то данные не смогут раскрыть другие стороны. Только те, кому известен ключ, могут расшифровать сообщение. Такой способ называется симметричным шифрованием, и он наиболее популярен. Основные причины, по которым требуется криптография: Криптография — это искусство и наука сокрытия (через шифрование) чувствительной информации. Она включает в себя шифрование (когда шифр первоначально применяется к необработанному «открытому/исходному тексту» и дешифрование, когда шифр используется для возвращения сообщению читабельной формы). Чтобы лучше понять эти шифры, рассмотрим простенький пример. Квадрат Полибия — шифр простой замены. В данном примере будет использоваться двумерная матрица 6х6, содержащая заглавные буквы алфавита и цифры от 0 до 9. Матрица будет выглядеть следующим образом:  С матрицей 6х6 (36 буквенно-цифровых знаков) мы можем начать замену. Например, буква «А» имеет адрес 1х1 или x=1, y=1. Эту запись можно упростить до 11. Другой пример: адрес буквы «N» будет 2х3 или x=2, y=3 или 23. Давайте зашифруем простое сообщение: Сообщение: ENCRYPT ME 2 DAY Шифротекст: 51–23–31–63–15–43–24 13–51 55 41–11–15 Шифр может сделать достаточно длинным и сложным, используя прописные буквы и специальные символы. Также повторение символов и написание алфавита вразброс может дать непредсказуемый результат, устойчивый для метода полного перебора. Это аналогично полиморфизму, используемому сегодня в современных компьютерных системах шифрования.  Шифр Цезаря, названный в честь его создателя — Юлия Цезаря, считается самым первым шифром, изобрётенным человечеством. Цезарь использовал его для кодирования сообщений своим генералам, чтобы враги из Римской Империи не смогли прочитать приказы при перехвате. Шифр Цезаря имеет элементарную форму шифрования, и его очень легко взломать. По этой причине сейчас он не используется для шифрования каких-либо серьёзных данных. Шифр Цезаря просто сдвигает алфавит вправо или влево. Разные значения сдвига приводят к разным результатам шифровки. Число сдвига — это число букв, на которое происходит смещение в одну из сторон, для создания шифротекста. Пример использования шифра со сдвигом влево на 3: Сообщение: ENCRYPT ME Шифротекст: HQFUBSW PH Шифротекст выше может быть легко взломан методом полного перебора, который заключается в сдвиге в одну из сторон на одну позицию, пока не получится какое-то смысловое сообщение. Прим. пер. Существует более простой способ взлома шифра Цезаря — частотный анализ, о котором автор не упоминает. Он заключается в подсчёте частоты встреч каждого символа в любом обычном тексте и в шифротексте. Потом символы с похожими частотами заменяются. Например, если в шифротексте чаще всего встречается буква «T», то она заменяется на букву «Е» для английского алфавита. Этот способ действует только для текстов свыше 300 символов. С помощью такого принципа шифрования можно создать более сложные шифры, такие как шифры Виженера или Гронсфельда. В них применяется метод замены. В расшифровке можно легко запутаться, так как каждый символ сдвигается по своим правилам.  Прежде чем продолжить изучение, важно понять, как работают простые шифры, так как они являются базой, на которой строится всё шифрование. Стеганография — процесс написания скрытых сообщений, больше подходит под определение классической криптографии, так как современная криптография стала синонимом «компьютерной безопасности». Полиморфизм является относительно продвинутой практикой в криптографии и часто используется в техниках компьютерного шифрования. Полиморфное преобразование — это техника, которая самостоятельно модифицирует криптоалгоритм после каждого выполнения, таким образом на каждой итерации получаются различные результаты. Такое часто встречается в шифровальных алгоритмах, используемых злоумышленниками. Это означает, что если понадобится зашифровать одну и ту же информацию два раза, то алгоритм выдаст разные шифротексты. Например, представьте ключ к машине. Сейчас у нас всех есть маленькое электронное устройство, которое может по одному лишь нажатию кнопки дистанционно разблокировать машину. Вы наверняка не задумываетесь о том, что при каждом нажатии на кнопку вы отсылаете небольшое сообщение с помощью своего устройства, и это сообщение индивидуально для каждой машины. Если оно совпадёт, то машина откроется. Было бы проще назначить каждой машине свою частоту работы, но это сложно в реализации, ведь диапазона частот может не хватить. Вместо этого используется одна частота, но разные алгоритмы шифрования информации, посылаемой машине. Эти алгоритмы полиморфны. Такие алгоритмы сложнее взломать, так как они каждый раз меняются. Даже если хакер разузнает алгоритм (что уже сложно с полиморфными шифрами), то ему ещё потребуется найти пару из машины и ключа, что довольно сложно. Сегодня шифры используют алгоритмы либо с секретным, либо с публичным ключом. В шифрах с закрытым ключом используется единственный ключ, которым обмениваются стороны. Такой ключ или шифр также называют симметричным.  В 1949 году Клод Шеннон из Bell Laboratories опубликовал фундаментальную теорию, положившую начало симметричному шифрованию, а десятилетия эволюции принесли примеры высокого качества. Однако только в 1975 году мощный алгоритм с закрытым ключом DES стал доступен для общего пользования. Шифрование с помощью открытого ключа или асимметричное шифрование также возникло в середине 1970-х. Асимметричные шифры используют пару ключей — открытый, им делятся с другими людьми, и соответствующий ему закрытый, пользователь должен хранить его в секрете от других. Например, получатель может сгенерировать пару ключей и поделиться открытым ключом со всеми, кто хочет отослать ему секретное сообщение. Отправитель может зашифровать сообщение для получателя с помощью открытого ключа, а получатель расшифровать его с помощью закрытого.  Стойкость шифровального алгоритма зависит от трёх важных факторов: Пример из жизни: популярный файловый архиватор PKZIP обычно применял встроенный алгоритм шифрования, который использовал 64-битный ключ. В теории для проверки всех ключей понадобилось бы 2^64 попытки. Но на практике оказалось, что в шифровании есть недостаток, который позволил сократить количество попыток взлома шифротекста до 2^27.\\nЕдинственный способ найти такие недостатки — попытаться взломать алгоритм шифрования на практике, используя трюки, сработавшие на других алгоритмах. О качестве алгоритма можно судить после того, как алгоритм подвергли тщательному анализу и попытались взломать. Но даже это не спасает от возможности того, что позже кто-то другой найдёт недостатки в алгоритме шифрования. DES выдержал испытание временем, потому что качество шифрования доказывалось на протяжении многих лет опубликованных исследований. После четверти века исследований учёным удалось найти несколько спекулятивных атак, которые в конечном итоге не были столь эффективными, как метод полного перебора. Единственная реальная слабость DES-шифра — его критически маленькая длина ключа в 56 бит.  Triple DES (3DES) \\u200aмодификация DES, позволяющая увеличить длину ключа до 112 или 168 бит. Получившийся шифр намного медленнее других шифров, использующих такую же длину ключа, но вышел из употребления, когда мощные компьютерные атаки взломали алгоритм. AES \\u200a(Advanced Encryption Standard или Rijndael) поддерживает три длины ключа 128, 192 и 256 бит и использует 128-битный размер блоков. В настоящее время он считается достаточно стойким и используется по всему миру.  Так как DES был специально разработан для аппаратного обеспечения, то не было предусмотрено, чтобы он эффективно работал в ПО. NIST протестировал работу алгоритма AES в программной среде и разработал требования к хранению криптоматериала, чтобы гарантировать, что AES будет эффективно работать на C и Java, которые используются на рабочих станциях, а также в более ограниченных средах встроенный процессоров ARM и смарт-карт. Архитектура AES основана на принципе, известном как замена и перестановка, и быстро работает как в программном, так и на аппаратном уровнях. В отличие от своего предшественника — DES, AES не использует сеть Фейстеля (прим. пер. — один из методов построения блочных шифров). AES — это вариант Rijndael с фиксированной длиной блоков в 128 бит и критическим размером 128, 192 и 256 бит. Напротив, спецификации Rijndael задаются длиной блоков и ключей, которые могут быть кратными 32 в диапазоне от 128 до 256 бит. AES работает с матрицей порядка 4х4 столбцов с байтами, называемой состоянием, хотя некоторые версии Rijndael имеют больший размер блоков и дополнительные столбцы. Большинство расчётов AES выполняются в определённом конечном поле. Длина ключа, используемого для шифрования AES, указывает на количество повторений раундов преобразования, которые преобразуют входной сигнал, называемый исходным текстом, а конечный вывод — шифротекстом. Число циклов повторения выглядит следующим образом:  Каждый раунд состоит из нескольких этапов обработки, каждый из которых содержит четыре одинаковых по алгоритму, но разных по входным данным этапа, включая тот, который зависит от самого ключа шифрования. Набор обратных повторений применяется для преобразования шифротекста в исходный текст с использованием того же самого ключа шифрования. Смотрите также: 10 популярных кодов и шифров  На приведённой выше диаграмме нарисовано квантовое распределение ключей (протокол BB84), являющееся безопасным способом связи, который реализует криптографический протокол с участием компонентов квантовой механики. Он позволяет двум сторонам создавать общий закрытый ключ (симметричные ключи), известный только им, который может быть использован для шифровки и дешифровки сообщений. Квантовая механика — это совокупность научных законов, описывающих поведение фотонов, электронов и других частиц, составляющих вселенную. Индустрии ищут большую безопасность от хакеров, поэтому новое поколение криптографии уже будет основываться не на математике, а на физике. Учёные в области физики атомов и частиц уже вошли в мир криптографии. Эти учёные хотят использовать законы квантовой механики для отправки сообщений, которые невозможно взломать. Они основоположники новой науки — квантовой криптографии, которая достигла совершеннолетия только в последние несколько десятилетий. Квантовая криптография опирается на физику частиц. Частицы, составляющие нашу вселенную, по своей сути являются неопределёнными явлениями, способными одновременно существовать в более чем одном месте или состоянии. Они самостоятельно выбирают, как себя вести, только когда они сталкиваются с объектом или кто-то измеряет их свойства. Криптография — это захватывающая область информационной безопасности и является одной из самых сложных дисциплин. Как только мы полностью понимаем такие простые шифры, как Цезаря и Полибия, то при переходе к более сложным шифрам с повторяющимися итерациями шифрования становится легче понимать алгоритмы вроде DES и AES. Криптология — это самостоятельная наука, и мы исследовали её историю, фундаментальные основы шифров от наименее до самых сложных типов, используемых сегодня. Перевод статьи «Understanding Cryptography From Math to Physics» Варвара Николаева ']), (['Как на фронтенд-собеседовании превратить сложный вопрос в лёгкий'], ['Рассказывает Алекс Паттисон За последние пару месяцев я активно проводил собеседования фронтенд-разработчиков. Каждый, кто когда-либо бывал на подобных собеседованиях, знает, что вопросы могут быть совершенно разные, а вот уровень знаний необходимо демонстрировать неизменно высокий. Вас могут спросить о чём угодно, начиная со структур данных/алгоритмов и заканчивая нюансами CSS. При подготовке к очередному собеседованию возникает чувство, будто нас просят вызубрить всю информацию с MDN. Тем более, что большинство тренировочных материалов для собеседований сконцентрированы на CS, поэтому фронтенд-разработчику приходится особенно туго. В этой статье я собираюсь объяснить мой подход к подготовке к интервью. Суть в том, чтобы научиться сводить каждый вопрос по структурам данных/алгоритму к DOM. DOM — это дерево узлов. Оно очень органично соединяет друг с другом как определённые структуры данных: графы (родительский класс), связные списки (подкласс дерева и, в частности, унарное дерево), — так и сами деревья. Немного креатива, и любой вопрос можно подстроить под фронтенд-тематику. Все примеры будут рассматриваться для Vanilla JS. На собеседованиях довольно часто спрашивают о DOM-манипуляциях, а времени пробежаться по любимому фреймворку (и всему встроенному инструментарию) не остаётся. Перед соискателями ставится трудная задача, ведь специалисты, как правило, мало пишут на Vanilla JS. Они предпочитают React, Angular, Vue и т. д. Но попрактиковаться в работе с чистым JS заранее всё-таки нужно: так вы гарантированно облегчите свою жизнь на следующем собеседовании. А учитывая тот факт, что почти все проблемы с кроссбраузерностью остались в прошлом, лишний козырь в виде умения писать на чистом JS явно не помешает. Задача: Верните значение k-тому с конца элементу односвязного списка. Первый шаг при преобразовании вопроса о структурах данных/алгоритмах к формату фронтенда — поиск оптимального способа представления данных в DOM. Поскольку связный список — не что иное, как унарное дерево, можно представить задачу в виде иерархической структуры узлов, где у одного предка будет по одному потомку. Несмотря на удобство данного варианта, лучше рассмотреть задачу под другим углом — так будет легче добавить простые стили. Фронтенд-вопрос: Измените цвет фона k-того с конца среди сестринских элементов. Вы стараетесь смоделировать связный список, поэтому необходимо ограничение: в каждом узле может быть доступ только к nextElementSibling (и встроенным стилям). Теперь задачу можно решить по-разному. Для начала проанализируйте возможные варианты. Не садитесь сразу за написание кода, ведь первое пришедшее на ум решение не всегда оказывается самым правильным. Варианты действий: Давайте разберём каждый вариант по порядку: Как оказалось, можно немножко схитрить во втором варианте, взяв из него самое лучшее и избежав лишних действий в виде повторной итерации по длине списка. Мы просто добавим к нему сразу два указателя. Пока главный указатель движется к концу списка, а разница между указателями равна k\\u200a-1, следующий указатель остаётся в узле, который мы выделим цветом. На картинке всё смотрится более понятно:  Представим, что у нас есть HTML, который выглядит как-то так: Как именно использовать последний указатель для закрашивания k из последнего элемента красным? Посмотрим внимательнее на код: Сначала мы инициализируем наши указатели leader и follower (2–3 строки) — оба они указывают на начало списка. Задаём выполнение цикла до тех пор, пока первый указатель не достигнет следующего сестринского элемента. Это будет индикатором окончания списка. Строка 12 перенаправляет наш главный указатель в сторону родственного элемента. Теперь посмотрим на строки 6–10. Здесь указано, нужно ли нам также перенаправлять и follower. Нам нужно, чтобы при k = 1 оба указателя двигались вперёд. Логика тут немного хромает, но мы вынуждены действовать так в рамках стандартного использования языка. Если я говорю «сделать вторую с конца ссылку красной», то это значит поменять цвет ссылки под номером 8. При наших текущих условиях входное значение k = 2 как раз и будет «второй ссылкой с конца».\\nВыставим дополнительное условие в строке 15 — так мы случайно не покрасим первую ссылку, если значение k превысит количество узлов в списке. Наконец, вызываем наш метод: Проверяем рабочий пример — вот здесь:  Данный код выполняется с O(n) временной сложностью и O(1) пространственной сложностью. Даже несмотря на то, что нам придётся перебирать весь список для определения длины, этот вариант всё равно окажется лучшим из всех существующих. Если вас заинтересовал третий вариант, то, как я и обещал, даю подсказку: Нам нужно просто поменять цвет узла, а возвращаемое значение функции здесь не важно. countAhead — это количество узлов перед текущим значением (включительно). То есть при вызове верхнего уровня метод recTurnKthToLastRed() вернёт длину нашего списка. Примечание: После обсуждения статьи с друзьями я бы хотел подчеркнуть, что самое важное здесь — найти работающее решение. А мозговой штурм с обдумыванием вариантов как раз и нужен для того, чтобы самим для себя понять, что именно мы хотим получить. Не пытайтесь сразу найти лучшую реализацию чего-либо. Ведь всегда можно провести оптимизацию и сделать небольшой рефакторинг готового кода. Написанный код, который ещё не оптимизировали, — это ВСЕГДА лучше, чем оптимизированный код, который ещё не написали. Смотрите также: Хочу стать frontend разработчиком: базовые знания и план обучения Перевод статьи «Cracking the (Frontend) Coding Interview» Ольга Сайфудинова ']), (['Избавляемся от шляп, ищем расстояния в массиве и нули в факториале — подборка задач для программистов'], ['На острове проживает группа людей. Появившийся джинн собрал всех и надел магические шляпы (или шляпу) некоторым людям (т. е., минимум одну шляпу). Волшебная природа шляпы проявляется в том, что она видима другими людьми, но не видна самому носителю. Чтобы избавиться от шляпы, обладатели (и только они) должны нырнуть в реку ровно в полночь. Если всего человек — N, а шляп — С, сколько времени займет избавление от шляп? Люди не могут сообщать друг другу о наличии шляпы (любым способом). Примечание Джинн не сообщил, сколько шляп он надел. A bunch of men are on an island. A genie comes down and gathers everyone together and places a magical hat(s) on some people’s heads (i e , at least one person has a hat). The hat is magical: it can be seen by other people, but not by the wearer of the hat himself. To remove the hat, those (and only those who have a hat) must dunk themselves underwater at exactly midnight. If there are N people and C hats, how long does it take the men to remove the hats? The men cannot tell each other (in any way) that they have a hat. Note: Genie does not tell how many hats she has put. Рассмотрим сначала простые случаи: c = 1: в этом случае, человек со шляпой увидит, что ни у кого больше нет шляп и вычислит, что следовательно, он сам является носителем шляпы. с = 2: каждый обладатель шляпы увидит только одного человека со шляпой, все остальные будут видеть шляпы у обоих. Если бы шляпа была одна — ситуация разрешилась бы в первую ночь, но в этом случае никто не пойдет купаться, не зная, что у него есть шляпа. На второй день люди, видящие одну шляпу догадаются, что и сами являются обладателями шляп и во вторую ночь нырнут оба. с = 3: в этом случае носитель шляпы увидит двух человек со шляпой, все остальные будут видеть 3 шляпы. Если бы шляп было 1 или 2 — были бы люди, нырнувшие в первую или во вторую ночь. Поскольку этого не произошло, носители шляп догадаются, что количество шляп — 3, и в третью ночь все обладатели шляп пойдут купаться. Аналогичным образом можно увидеть, что для удаления всех C шляп потребуется C дней. Lets take some simple cases first c = 1: in this case person who has hat on his head can see that no one else has hat on their head, so he will understand that he is the one with hat. c = 2: in this case person with hat will see one other person with hat, rest all will see two hats. Now had there been only 1 hat, this case would have been solved on very first day, but in this case no one will go on first night, so the guy who see one hat will understand that there must be one hat on his head, so both of them will go underwater on second night. c = 3: in this case person with hat will see two hats and rest all will see three hats, now had there been 1 or 2 hats some guys would have gone on first or second night, thus on third day guys who see 2 hats will understand that they have hats on their head and they will all go underwater on third night. Similarly we can see that it will take c days to remove all hats. Дан массив с повторяющимися элементами. Задача — найти максимальное расстояние между двумя вхождениями элемента. Примеры: Вход: arr[] = {3, 2, 1, 2, 1, 4, 5, 8, 6, 7, 4, 2}\\nВыход: 10 // максимальное расстояние для 2 это 11-1 = 10\\n// максимальное расстояние для 1 это 4-2 = 2\\n// максимальное расстояние для 4 это 10-5 = 5 Given an array with repeated elements, the task is to find the maximum distance two occurrences of an element. Examples: Input : arr[] = {3, 2, 1, 2, 1, 4, 5, 8, 6, 7, 4, 2}\\nOutput: 10 // maximum distance for 2 is 11-1 = 10\\n// maximum distance for 1 is 4-2 = 2\\n// maximum distance for 4 is 10-5 = 5 Простое решение — перебирать элементы по одному, находить первое и последнее вхождения в массиве, и считать разницу позиций в качестве максимального расстояния. Временная сложность при этом подходе будет O(n^2). Эффективное решение — использовать хэш-таблицу. Идея заключается в том, чтобы пройти входной массив, сохраняя индекс первого вхождения элементов в хэш-таблице. Для каждого последующего вхождения находить разницу между индексом элемента и индексом первого вхождения. Если эта разница больше текущего результата — обновить результат. A simple solution for this problem is to one by one pick each element from array and find its first and last occurence in array and take difference of first and last occurence for maximum distance. Time complexity for this approach is O(n2). An efficient solution for this problem is to use hashing. The idea is to traverse input array and store index of first occurrence in a hash map. For every other occurrence, find the difference between index and the first index stored in hash map. If difference is more than result so far, then update the result. Сколько замыкающих нулей в 100! (факториал числа 100)? How many trailing zeroes are there in 100! (100 factorial) ? Для каждого множителя, кратного 10, будет добавлен один замыкающий ноль; также для каждого множителя, заканчивающегося на 5, будет добавлен один ноль (поскольку 5*2 = 10, а двоек в этой последовательности достаточно). Необходимо также не забывать о повторениях: 10, 20,…., 90 = 9 нулей 100 = 2 нуля 5, 15, 25……95 = 10 нулей и ещё дополнительная 5 в числах 25, 50 и 75 = 3 нуля итого 9+2+10+3 = 24 нуля. For every factor of 10, there will be one trailing zero similarly for every factor of 5 there will be one trailing zero (as 5*2 =10, and there are enough number of 2’s). But we have to take care of repetitions. 10, 20,…., 90 = 9 zeros 100 = 2 zeros 5, 15, 25……95 = 10 zeros and 1 extra 5 in each of 25, 50 and 75 = 3 zeros so total 9+2+10+3 = 24 zeros. Андрей Приб, Spice IT Recruitment']), (['Что делать, если в наследство достался некачественный код — отвечают эксперты'], ['Программирование — это не только написание кода, но и его исправление. Наш подписчик явно столкнулся с ситуацией, где ему потребовался совет опытного человека: «Что делать, если в наследство достался некачественный код?» За разъяснениями мы обратились к нашим экспертам, а полученные ответы предоставляем вашему вниманию. Если вам в голову пришел такой вопрос, то главное не бежать все переписывать. Обычно это стандартная реакция одного программиста на код другого программиста — в коде ничего непонятно, сложно разбираться. Кажется проще переписать все с нуля. Переписать вы всегда успеете, сначала попробуйте разобраться в требованиях и способах использования приложения. Обычно кодовая база растет эволюционно и накапливает бизнес-логику, которая может быть неочевидной для внешнего человека. Попытка переписать эту логику с нуля породит много багов и проблем. Если после недели чтения кода и документации все ещё ничего непонятно, то, кажется, код действительно «не очень». В этот момент нужно понять, как вы будете взаимодействовать с этим кодом. Вам его дали на поддержку? Или нужно его развивать? Если нужно поддерживать систему в работоспособном состоянии без серьезных доработок, то переписывать точно НЕ нужно. Напишите тесты на критические функции, чтобы быть уверенным в корректной работе. Если возникает вопрос о доработке системы, которую сложно расширять или архитектура которой устарела, то это вопрос, требующий обсуждения с заказчиком системы. Обязательно поговорите с заказчиком и донесите до него информацию о том, что расширение текущей системы становится дорогим (или долгим). Нужно подготовить обоснование тому, почему мы хотим причинить боль всем, переписывая систему, которая уже работает. Когда все заинтересованные лица поняли проблему и согласились ее исправлять, нужно подготовить план действий. Я участвовал в нескольких таких проектах — хуже всего при переработке системы уйти на полгода писать новый код. Такой подход проваливается в подавляющем большинстве случаев. Самый удачный вариант — переписывать частями. Выделять модули из существующей системы, покрывать тестами, а затем менять реализацию внутри модуля. Обязательно показывать промежуточные результаты всем заинтересованным и слушать обратную связь. Так вы победите! Для начала нужно убедиться, что код действительно плох: не исключено, что он сложен для понимания из-за специфики предметной области. Например, код DSP-контроллера чрезвычайно сложно читать без релевантного опыта. Для верности стоит посоветоваться с коллегами, более осведомленными в данном вопросе. Если код действительно плохого качества, важно понять, можно ли закрыть на него глаза. Толерантность к чужому коду — важный скилл. Если нельзя, стоит определить:\\n— Какова трудоемкость переписывания кода (оценка трудоемкости — это сложно, есть риск просчитаться на целые месяцы);\\n— Хорошо ли ты понимаешь код, который планируешь переписать;\\n— А сможешь ли ты написать более качественный код;\\n— Если проблемный код не покрыт тестами (а так, скорее всего, и будет), то есть риск, что новый код создаст регрессию в проекте. Готов ли ты взять на себя ответственность за результат? На заметку: переписанный код в лучшем случае будет оценен по достоинству вашими коллегами, но не начальством. Другими словами, премию и прочие плюшки за это не жди. Если все же остается желание переписать код, могу дать несколько советов:\\n— Максимально декомпозируй код — чем меньше кусок переписываемого кода, тем лучше.\\n— Используй методы рефакторинга, покрой код тестами, если их нет.\\n— Не трать много времени на переписывание, оно не должно мешать основной работе. В первую очередь нужно разделить понятия наследия и некачественного кода. Legacy code — устойчивое выражение, обозначающее, что код, попавший в руки, был написан достаточно давно. И тут нужно учесть, что, возможно то, что было написано 10 лет назад очень актуально для своего времени. Назвать такой код некачественным — вряд ли, устаревшим — вполне. Это может быть вполне поддерживаемая, масштабируемая система, просто сейчас никто таких не делает. С таким кодом можно продолжить работу в определенной парадигме. А некачественный код мог быть написан буквально вчера. И это уже совсем другой — более тяжелый случай. Если в случае Legacy code возможна дальнейшая поддержка и надстройка, то когда тебе достается откровенно плохая работа, есть смысл задуматься о рентабельности проекта в целом. В этом случае придется, скорее всего, переписывать с нуля. И тут очень важно понять, на каком этапе находится проект: если это старт, то переписать код достаточно легко, если уже заключительный этап — то проще доделать как есть, хотя риск, что в итоге система не будет работать велик. Важно объяснить это заказчику. Наверно, каждый разработчик на своем программистском поприще рано или поздно сталкивался с ситуацией, когда смотришь на чужой код и думаешь: «Это что, шутка?» Что же делать, когда открываешь код и на глаза, кроме слез, ничего не попадает? Как не впасть в панику и не положить заявление на стол? Понятие «некачественный код» у каждого свое. Более того, в каждом языке программирования свои парадигмы и представления об идеальном коде. Для себя я вывела несколько уровней «греха» под названием «некачественный код». 0-ой уровень:\\nпроблема с переменными. Это очень абстрактное понятие, но если обобщить, то сюда попадает интуитивно непонятное наименование переменной (ar1, ar2, item), лишние переменные, которые нигде не используются, подтягивание одних и тех же данных несколько раз и т.д. Решить данную проблему достаточно просто, хоть и не доставит особого удовольствия. Переименовать переменные в интуитивно понятный вид (array_for_organization, obuv_items, users_telephones), удалить лишние вызовы, присваивания и переменные. 1-ый уровень:\\nодин из самых безобидных, как по мне, отсутствие комментариев. Должна признаться, сама я этим иногда тоже грешу, особенно в ситуациях, когда «надо сделать вчера», тогда ты делаешь максимально быстро, но потом это аукается и приходится потратить очень много времени на разбор кода. Однако, если все достаточно четко расписано, структурировано и нет лишнего, этот грех является самым меньшим и с ним проще всего справиться. В этом случае ваша проблема заключается в том, что вам придется писать комментарии вместо создателя кода. Комментарии в данной ситуации помогут вам разобраться что к чему и почему и ускорят разработку в дальнейшем. 2-ой уровень:\\nэтот «грех» уже серьезнее, чем первые два и может доставить очень много неприятностей. Неоптимизированная выборка, зацикливание и прочее. Что-то, что может повесить систему или страницу (если говорим про веб). Данная ситуация также поправима при наличии головы на плечах, однако куски кода с неоптимальной выборкой придется переписать практически полностью, т.е. придется переделывать работу предыдущего человека, а это долго и вообще очень неприятно. 3-ий уровень:\\nпрактически самый серьезный. Отсутствие документации. Очень часто встречаю ситуацию, когда на одной странице вызывается множество различных методов, констант и прочего, но абсолютно непонятно, где все это хранится и что там лежит. Проблема для вас будет заключаться в выявлении возможных ошибок и поиске методов, если код достаточно объемный. Здесь вам на помощь придет поиск и составление собственной документации. Конечно, чем дольше работаешь на одном проекте, тем проще ориентироваться, однако для упрощения жизни себе и другим разработчикам неплохо бы все же составлять краткую выжимку, что где и откуда. 4-ый уровень:\\nсамый серьезный. «Грех» четвертого уровня обычно совершают новички, не особенно знакомые с продуктом, с которым работают. Правка файлов, которые не следует править. Поясню на примере CMS для веб-разработки, т.к. я занимаюсь именно веб-разработкой. Каждая CMS основана на неком ядре, на основе которого разработчик создает нечто свое, но использует те методы, модули и классы, которые изначально заложены ядром. Так вот самый большой грех это правка файлов этого ядра. Этот «грех» очень сложно выявить, практически невозможно устранить и в процессе может доставить очень много неприятных моментов. Я здесь не учла, конечно, ситуации, когда все эти грехи встречаются одновременно, а такое тоже очень часто бывает, однако я постаралась структурировать те ситуации, которые возникали в моей практике и способы борьбы с ними (в идеальном мире). Всем хорошего дня и побольше качественного кода! Если вам «прилетел» плохой код, начните с прямого вопроса заказчику, насколько ему важно сохранить код в текущем состоянии. Если очень важно, можно попытаться доступно и без лишних эмоций объяснить минусы этого решения. В частности, донести до клиента, что низкое качество кода неизбежно увеличивает расходы на дальнейшее развитие продукта. Какие аргументы можно привести? 1. На то, чтобы разобраться с кодом, уйдет неоправданно много времени. Простой пример: команде достался по наследству функционал, работающий на четырех переменных и особой математической магии. Нужно было поменять в нем одно условие. При хорошем коде эта работа заняла бы 30 минут, но программист убил 10 часов, потому что ему пришлось практически побуквенно разбираться, как все это работает. 2. При сохранении некачественного кода в продукте возникнет так называемый «фактор автобуса»: разобравшийся в проекте специалист станет единственным носителем знания по нему. Если он уволится (или попадет под автобус, как следует из названия этого явления), владелец кода опять понесет все убытки, связанные с вводом новых программистов в проект. Также не помешает дать клиенту точную оценку затрат на рефакторинг. Если даже после этих аргументов заказчик не согласится с тем, что старый код безопаснее облить бензином и сжечь, придется работать с тем, что есть. Дописывать чужой код лучше в своем стиле, максимально лаконично и правильно. Как говорится, пиши код так, как будто поддерживать его будет психопат, который знает твой домашний адрес. Есть время и возможность переделать? Выпиши на листочек всю логику плохого куска кода, перепиши его в более структурированном виде; добавь удобное, убери повторяющееся; перепиши еще раз — уже в том виде, в котором тебе самому приятно это читать. В общем, если при изучении legacy-кода у вас появляется стойкое ощущение, что его писала бешеная обезьяна под бутиратом, не молчите — обсуждайте возможность рефакторинга. Не получилось этого добиться — улучшайте код по мере сил в процессе. Оставлять как есть — самый неразумный вариант. Напоминаем, что вы можете задать свой вопрос экспертам, а мы соберём на него ответы, если он окажется интересным. Вопросы, которые уже задавались, можно найти в списке выпусков рубрики. Если вы хотите присоединиться к числу экспертов и прислать ответ от вашей компании или лично от вас, то пишите на experts@tproger.ru, мы расскажем, как это сделать. Анастасия Витвицкая']), (['Разбираемся в типах NoSQL СУБД'], ['В этой статье мы познакомимся с разными типами NoSQL СУБД. Всего есть 4 основных типа: Отсутствие схемы в базах данных «ключ-значение», например, Riak, — это как раз то, что вам нужно для хранения данных. Ключ может быть синтетическим или автосгенерированным, а значение может быть представлено строкой, JSON, блобом (BLOB, Binary Large Object, большой двоичный объект) и т.д. Такие базы данных как правило используют хеш-таблицу, в которой находится уникальный ключ и указатель на конкретный объект данных. Существует понятие блока (bucket) — логической группы ключей, которые не группируют данные физически. В разных блоках могут быть идентичные ключи. Производительность сильно вырастает за счёт кеширующих механизмов, которые работают на основе маппингов. Чтобы прочитать значение, вам нужно знать как ключ, так и блок, поскольку на самом деле ключ является хешем (блок + ключ). В модели «ключ-значение» нет ничего сложного, так как реализовать её проще простого. Не лучший способ, если вам нужно только обновить часть значения или сделать запрос к базе данных. Если поразмыслить о теореме CAP, то становится довольно очевидно, что такие хранилища хороши в плане доступности (Availability) и устойчивости к разделению (Partition tolerance), но явно проигрывают в согласованности данных (Consistency). Пример: посмотрим на набор данных, представленных таблицей ниже. Здесь ключ — это название страны, а значение — список адресов в этой стране:  База данных такого типа позволяет читать и записывать значения с помощью ключа следующим образом: И хотя базы данных типа «ключ-значение» могут пригодиться в определённых ситуациях, они не лишены недостатков. Первый заключается в том, что модель не предоставляет стандартные возможности баз данных вроде атомарности транзакций или согласованности данных при одновременном выполнении нескольких транзакций. Такие возможности должны предоставляться самим приложением. Второй недостаток в том, что при увеличении объёмов данных, поддержание уникальных ключей может стать проблемой. Для её решения необходимо как-то усложнять процесс генерации строк, чтобы они оставались уникальными среди очень большого набора ключей. Riak и Dynamo от Amazon — самые популярные СУБД данных такого типа. Данные, представленные парами ключ-значение, сжимаются как хранилище документов схожим с хранилищем «ключ-значение» образом, с той лишь разницей, что хранимые значения (документы) имеют определённую структуру и кодировку данных. XML, JSON и BSON — некоторые из стандартных распространённых кодировок. В следующем примере можно увидеть данные в виде «документа» который отображает названия определённых магазинов. Обратите внимание, что, хотя все три примера содержат местоположение, они отображают его по-разному: Одним из ключевых различий между хранилищем «ключ-значение» и документоориентированным является то, что последний включает метаданные, связанные с хранимым содержимым, что даёт возможность делать запросы на основе содержимого. Например, в указанном примере можно попробовать найти все документы, в которых «City» равно «Noida», что вернёт все документы, связанные с магазинами в этом городе. Apache CouchDB — пример документоориентированной СУБД. CouchDB использует JSON для хранения данных, JavaScript в качестве языка запросов с использованием MapReduce и HTTP для API. Данные и отношения не хранятся в таблицах так, как в традиционных реляционных базах данных, а по сути являются набором независимых документов. Тот факт, что такие базы данных работают без схемы, делает простой задачей добавление полей в JSON-документы без необходимости сначала заявлять об изменениях. Couchbase и MongoDB — самые популярные документоориентированные СУБД. В колоночных NoSQL базах данных данные хранятся в ячейках, сгруппированных в колонки, а не в строки данных. Колонки логически группируются в колоночные семейства. Колоночные семейства могут состоять из практически неограниченного количества колонок, которые могут создаваться во время работы программы или во время определения схемы. Чтение и запись происходит с использованием колонок, а не строк. В сравнении с хранением данных в строках, как в большинстве реляционных баз данных, преимущества хранения в колонках заключаются в быстром поиске/доступе и агрегации данных. Реляционные базы данных хранят каждую строку как непрерывную запись на диске. Разные строки хранятся в разных местах на диске, в то время как колоночные базы данных хранят все ячейки, относящиеся к колонке, как непрерывную запись, что делает операции поиска/доступа быстрее. Пример: получение списка заголовков нескольких миллионов статей будет трудоёмкой задачей при использовании реляционных баз данных, так как для извлечения заголовков придётся проходить по каждой записи. А можно получить все заголовки с помощью только одной операции доступа к диску. Модель данных Самыми известными примерами являются Google BigTable и HBase с Cassandra, вдохновлённые BigTable. BigTable представляет собой высокопроизводительное, сжатое и проприетарное хранилище данных от Google.  У него есть следующие атрибуты: Двумерная таблица, состоящая из строк и колонок, является частью реляционной системы баз данных.  Эту таблицу можно представить в виде BigTable-сопоставления следующим образом: На колонки можно ссылаться с помощью колоночного семейства. В графовой базе данных вы не найдёте строгого формата SQL или представления таблиц и колонок, вместо этого используется гибкое графическое представление, которое идеально подходит для решения проблем масштабируемости. Графовые структуры используются вместе с рёбрами, узлами и свойствами, что обеспечивает безиндексную смежность. При использовании графового хранилища данные могут быть легко преобразованы из одной модели в другую.  Контейнерная иерархия документоориентированной базы данных содержит данные без схемы, которые можно представить в виде дерева, которое является графом. Если обращаться к документам или их элементам в этом дереве, можно получить более выразительное представление данных, в котором можно легко ориентироваться с помощью Neo4j. Далее описаны некоторые особенности графовой базы данных на основе примера ниже:  Помеченный, направленный, атрибутированный мультиграф: граф содержит узлы, которые помечены определёнными свойствами и которые имеют связи друг с другом, что представлено направленными рёбрами. Например, связь «Элис знает Боба» выражена ребром с некоторыми свойствами. Хотя реляционные базы данных могут скопировать поведение графовых, рёбрам потребуется соединение (JOIN), что дорого обойдётся. Пример использования Любой рейтинг «Рекомендовано вам», который можно увидеть на разных сайтах, зачастую составляется исходя из того, как другие пользователи оценили продукт. Графовые базы данных отлично подходят для такого случая. InfoGrid и Infinite Graph — самые популярные графовые базы данных. InfoGrid позволяет соединять множество рёбер (Relationships) и узлов (MeshObjects), что упрощает представление набора информации со сложными взаимными ссылками. InfoGrid предлагает два типа баз данных: Смотрите также: SQL против NoSQL на примере MySQL и MongoDB Перевод статьи «EXPLORING THE DIFFERENT TYPES OF NOSQL DATABASES» Никита Прияцелюк, последний центурион']), (['Основы функционального программирования с примерами на Scala — часть 2'], ['В прошлой статье из нашей серии мы поговорили о том, зачем нужно функциональное программирование, и о недостатках императивного программирования, из-за которых функциональное набирает популярность. Кроме того, мы привели базовое введение в Scala, мультипарадигменный язык программирования с широкой поддержкой функционального подхода. В предыдущей статье вы можете ознакомиться с базовыми понятиями языка и узнать, как настроить окружение. В этой части мы расскажем об ООП-модели Scala и о некотором синтаксическом сахаре, специфичном для языка Scala. Язык Scala имеет достаточно гибкую ООП-модель, состоящую из обжектов, трейтов, классов и кейс-классов, чем-то похожую на таковую в Java. Класс в Scala объявляется следующим образом: и инстанцируется при помощи конструкции new ClassName(field1, field2, … , fieldN). Параметры, введенные скобках после имени класса, являются private полями класса, однако можно изменить их область видимости: если нужно иметь доступ на чтение извне, перед именем поля нужно поставить модификатор val, а если нужно чтение и запись — модификатор var. Классы являются ссылочными типами и неявно наследуют базовый класс AnyRef. Поэтому присваивание объекта с мутирующими (var) полями другой переменной только копирует ссылку. В статье будет идти речь о функциональном программировании, поэтому особенности языка, связанные с мутирующими переменными, выходят за рамки обсуждения, как и достаточно подробное описание взаимодействия всех возможных сущностей. Внутри класса можно вводить многие вложенные структуры для внутреннего использования. По умолчанию все переменные, постоянные и методы, объявленные в теле класса, — публичные. Их можно защитить при помощи модификаторов private и protected. С другой стороны, вложенные классы недоступны извне. Другой ООП-сущностью является trait. В отличие от класса, trait не может иметь конструктора и не может быть инстанцирован. Кроме того, он может содержать абстрактные методы, в то время как класс — только если помечен модификатором abstract. Синтаксис трейта таков: Ещё одна ООП-сущность, с которой вы столкнулись в предыдущей статье, — object. Его отличительной чертой является то, что он имеет единственный экземпляр, создаваемый автоматически, иначе говоря, является синглтоном. Только обжект может быть точкой входа в программу. Кроме того, вы можете импортировать его вложенные типы, методы, значения в область видимости другого файла. Это позволяет использовать обжект как средство структурной организации программы, давая возможность группировать константы и статические методы. Наследование в Scala имеет некоторые особенности. Во-первых, практически всё может наследовать от нескольких trait при помощи синтаксиса extends trait1 with trait2 with trait3 ... with traitN. Именно это явление вы видели в начале статьи: Здесь App — предопределённый трейт, оборачивающий содержимое внутрь метода main. Зачем нужен синтаксис extends … with … with …? Разве нельзя обойтись одним ключевым словом? Увы, нет. Если же разрешить множественное наследование (с наследованием реализации), то появляется проблема ромбовидного наследования. В Scala эта проблема решена определением главного трейта, реализация которого и наследуется. Этот главный трейт идёт первым после слова extends (к слову, это может быть вовсе не трейт, а класс или абстрактный класс). Кроме того, вы не можете наследовать типы от объекта. На самом деле, вы можете создать анонимный класс-наследник трейта, реализовав все его абстрактные методы используя функционал анонимных классов, и далее создать его экземпляр: Но писать такой код бывает достаточно громоздко. Для этого были введены обжекты-компаньоны (companion object). Для них нет специального ключевого слова, поэтому, чтобы их определить, вам нужно создать в файле с классом (или трейтом) обжект с таким же именем, как у класса. У обжекта-комапньона вы можете определить метод apply: Тогда вы можете создавать «экземпляры» трейта при помощи синтаксиса Foo.apply(1). Этот код громоздкий, поэтому специально для метода apply сделали возможность не писать его название. Эта «магия» работает для всего что называется apply в любой сущности. Выглядит это так: Foo(1) Подобное можно делать и с классами. Отметим, что семантика метода apply может значительно отличаться от приведённого примера, ведь никто вас не ограничивает в том, что будет делать этот метод. Эта особенность активно используется как в стандартных библиотеках, так и в сторонних. В Scala есть возможность параметризовать трейт, класс или его метод некоторым количеством типов. Делается это при помощи такого синтаксиса: Вы можете использовать параметры типа внутри тела метода/класса/трейта разными способами. Например, вы можете написать функцию, которая будет вычислять композицию 2 функций: def compose[U, V, W](f: U => V, g: V => W): W = (x :U) => g(f(x)) На этом примере мы можем продемонстрировать ещё одну особенность системы типов Scala — она может выводить параметры-типы шаблонов. Например, если мы введём две функции то мы можем вычислить их композицию, не указывая цепочку типов: val f = compose(fromIntToDouble, fromIntToString) Что на самом деле эквивалентно: val f: Int => String = compose[Int, Double, String](fromIntToDouble, fromIntToString) Параметрические типы — мощное средство доказательства свойств программы — в том числе типобезопасности. Например, у вас в базе данных хранятся объекты двух типов A и B. Вы не хотите, чтобы кто-то мог добавить объект типа А в коллекцию элементов B (допустим, ваша БД нереляционная, например, MongoDB, и позволяет такое) или же не пытался записать элемент в несуществующую коллекцию. Вы можете написать метод добавления в коллекцию вот таким образом: В этом примере вы сможете добавить элемент в коллекцию соответствующего типа. Попытка добавить элемент типа А в коллекцию элементов типа B вызовет ошибку компиляции, и, скорее всего, IDE предупредит вас об этом ещё на этапе написания кода. Вы можете создавать псевдонимы для параметрических типов, если вы задали в них все или несколько параметров. К примеру, вы можете ввести список строк: type String = List[String] Или же, например, кортеж, один из параметров которого — целое: type TypInt[T] = (T, Int) Хотя на этом возможности системы типов Scala и не завершаются, дальнейшее повествование выходит за рамки нашего введения. Оператор switch во многих языках воплощает задумку классификации данных на несколько категорий. Но большинство реализаций не могут поддерживать классификацию объектов, и поэтому редко используются в программах. В Scala есть очень мощная система сопоставления объекта с образцом. Однако экземпляр далеко не каждого класса можно сопоставлять с образцом без лишних телодвижений. Для упрощения этого процесса были введены два специальных вида сущностей: кейс-классы (case classes) и кейс-обжэкты (case objects). Кейс-классы подобны именованным кортежам, используемым для хранения данных. Часто они могут не иметь своих методов, выступая пассивными контейнерами, ведь Scala предоставляет великое множество других методов взаимодействия. Итак, синтаксис кейс-класса таков: case class CaseClasName(parameterList) { /*опциональное тело класса*/ } Все поля кейс-класса немутирующие и публичные. Это не является нарушением принципа инкапсуляции, поскольку он неприменим к публичным параметрам кейс-класса. Инкапсуляция полей имеет место, когда поля класса должны меняться некоторым сложным взаимосвязанным образом, и поэтому изменение полей должно производиться через методы класса. Здесь же нет никакого изменяемого состояния (мы крайне не рекомендуем его вводить в теле класса, чревато неожиданными последствиями), и инкапсулировать нечего. Инстанцировать экземпляр кейс-класса можно удобным синтаксисом: CaseClasName(parameter1, parameter2, … , parameterN) Примечание Здесь «под капотом» находится автоматически сгенерированный метод apply для этого класса, позволяющий пользоваться таким синтаксисом. Пример: Второй тип сущностей, case object, похож на case class, но не имеет полей. Зачем же он тогда нужен? Для создания строго типизированных перечислений. Заметим, что кейс-класс не может наследовать от другого кейс-класса, поэтому рекомендуем держать иерархию данных как можно более плоской и как можно менее древоподобной. Всегда можно задать кейс-классам общий надтип и, соответственно, нужный набор полей при помощи трейтов. Например, можно сделать так: Каким образом работает сопоставление с шаблоном (pattern matching)? При помощи следующего синтаксиса: Шаблоны могут быть очень разнообразными в зависимости от используемого типа данных. Кейс-класс в сопоставлении с шаблоном можно разложить на его параметры при помощи «вызова конструктора» этого кейс-класса. Например, приведенному выше классу User будет соответствовать шаблон User(name, email). Здесь переменные name и user извлекаются из объекта класса User и после этого становятся доступны блоку value, следующему за =>. Кроме того, вы можете ввести переменную для обозначения объекта, сопоставляемого с образцом, при помощи символа @. Например, такой шаблон введёт переменную user для блока value: user @ User(name, email) Ещё одной полезной особенностью сопоставления с образцом является возможность зафиксировать некоторые поля класса или же, наоборот, игнорировать некоторые из них. Например, мы можем выбирать пользователя только с именем «dave» шаблоном User(\"dave\", email). Если же нам безразличен email пользователя, мы можем не вводить переменную для этого поля шаблоном User(name, _). Стоит отметить, что сопоставление с образцами происходит в порядке их расположения. Если шаблон User(name, email) находится выше шаблона User(\"dave\", email), то сопоставления с последним никогда не произойдет, потому что первому из шаблонов соответствует любой экземпляр класса User. Если же вам не нужны поля класса, а нужно просто соответствие по типу, используйте шаблон вида valName: TypeName. Кроме того, существует шаблон «_», которому удовлетворяет любой объект. Результат сопоставления с образцом — значение, возвращённое из value, приведенное к общему надтипу всех блоков value1, … valueN. Если ни одному из шаблонов значение не соответствует, выбрасывается исключение PatternMatchingException. Приведем пример, объединяющий написанное выше: На самом деле, сопоставление с образцом можно применять не только на кейс-классах. На всех типах-примитивах (числа, строки, символы, …), и типов, для которых определён метод-экстрактор. Например, вы можете сопоставлять строку с регулярным выражением. Просто создайте несколько регулярных выражений и применяйте сопоставление с шаблоном: Такие действия можно осуществлять, определяя метод unapply и объект-экстрактор. Подробнее об этом можно прочитать в документации. Экстрактор имеет ещё одно полезное применение: с помощью него можно распаковывать кейс-классы и другие классы, имеющие экстрактор, используя точно такой же синтаксис, как вид шаблона в сопоставлении с образцом: Хотя сопоставление с шаблоном имеет больше возможностей, о которых вы можете почитать в документации, здесь мы ограничимся только этими. В этой части мы опишем дополнительный синтаксический сахар, относящийся к функциям. Многие функции стандартной библиотеки Scala принимают в качестве параметра другие функции. Например, у списка (List) есть функция map, которая совершает преобразование над каждым его элементом, сигнатура которой упрощенно выглядит так: def map[B](f: A=>B): List[B] Допустим, у нас есть список целых чисел и нам нужно прибавить к каждому из элементов единицу. Мы могли бы сделать это при помощи лямбда-выражения: И получили бы список: List(2, 3, 4, 5, 6) Но Scala для подобных случаев имеет более лаконичный синтаксис: list.map {_ + 1} Здесь на место подчёркивания будет подставлен элемент списка. Сокращение в виде подчёркивания может использоваться в самых неожиданных местах, но общая идея его использования — исключение имени переменной там, где можно восстановить однозначным образом манипуляцию над данными, которую вы хотите описать. Например, там, где требуется функция типа (Int, Int) => Int, вы можете передать суммирование при помощи синтаксиса _ + _. Однако, не всюду такой сахар будет работать, и тогда нужно будет пользоваться обычными лямбда-выражениями. Лямбда-выражения хорошо работают, когда у функции аргументы не запакованы ни в какие обёртки. Если же мы рассмотрим функцию, принимающую функцию из кортежа целых чисел в целое: def f(g: Tuple[Int, Int] => Int) = ??? То мы не сможем сделать вызов при помощи удобного синтаксиса: f { (i1, i2) => i1 + i2 } // ошибка компиляции Функция должна переводить из одного параметра кортежа в целое: f { t => t._1 + t._2 } // верно Но такой синтаксис недостаточно выразителен, поэтому для этих целей существует некоторого вида сопоставление с шаблоном. Вообще, синтаксис case <шаблон> if => value задает частично определённую функцию (partial function). Частично определённая функция, действующая из некоторого набора данных A в набор данных B, отличается от обычной функции тем, что она может быть определена не для всех возможных значений из A. Например, функция извлечения арифметического квадратного корня является частично определённой для всех вещественных чисел от 0 до +∞. Компилятор Scala умеет определять, является ли частичная функция обычной. Если же он обнаружит, что в сопоставлении с шаблоном вы описали не все случаи, он выдаст вам предупреждение с примером входных данных, которые могут выбросить PatternMatchingException. Следующая порция синтаксического сахара Scala — ассоциативность операторов. Оператором в Scala называется нестатический метод, который принимает один параметр. Существует два типа операторов: правоассоциативные и левоассоциативные, если вызывать их через точку, то различия нет, но зато, при использовании сокращенного синтаксиса без точки, семантика записи a op b меняется. Правоассоциативные операторы можно вызывать подобным образом: a1 op1 a2 op2 a3 … opN-1 aN Что эквивалентно записи: ( ... ((a1.op1(a2)).op2(a3)) …).opN-1(aN) Что вполне неплохо сокращает количество скобок. К примеру, мы можем последовательно вызывать метод преобразования элементов списка: List(1, 2, 3) map { _ + 1 } map { _ * 2 } map { _ - 1 } И это будет эквивалентно записи: ((List(1, 2, 3).map { _ + 1 }).map { _ * 2 }).map { _ - 1 } Левоассоциативные операторы можно вызывать похожим способом: a1 op1 a2 op2 a3 … opN-1 aN Но это будет эквивалентно совсем другой записи: (...((aN.opN-1(aN-1)).opN-2(aN-2))...).op1(a1) Левоассоциативные операторы отличаются от правоассоциативных наличием символа «:» на конце. Например, вы можете создавать список, вызывая метод пустого списка (Nil) ::: 1 :: 2 :: 3 :: 4 :: 5 :: Nil, что будет эквивалентно: ((((Nil.::(5)).::(4)).::(3)).::(2)).::(1) И создаст список: List(1, 2, 3, 4, 5) Для унарных и некоторых бинарных операторов существует постфиксная нотация, например, когда у вас нет никаких параметров к методу, вы можете написать value methodName вместо value.methodName. Так, например, в стандартной библиотеке в пакете scala.concurrent.duration устроены конструкторы промежутков времени. Вы можете написать: На самом деле, вы будете вызывать методы: Другое дело, что у обычного Int нет методов second, minutes и milis. Эти методы добавлены к нему при помощи специального механизма методов расширения, о которых мы поговорим в следующей части. Некоторые разработчики критикуют возможность создания DSL (Domain Specific Language) прямо внутри языка. Один из основных пунктов критики — код становится непонятным и нечитаемым, разобраться в стрелочках и прочих закорючках становится довольно сложно. На самом деле эта критика происходит из неправильного использования DSL: разработчики пытаются создавать предметно-специфический язык, чётко не выделив предметную область. Для того чтобы строить полезные и понятные DSL, нужно строго определить область, для которой создаётся язык. Например, стандартная библиотека предоставляет DSL для работы с последовательностями. Далее нужно использовать отличимые и интуитивно понятные символы, и если таких нет, не стесняться использовать слова. Например, использовать -~> и ~-> в одном языке — не самая хорошая идея. На самом деле, правильно написанные DSL позволяют сильно сокращать количество написанного кода и делать его более идиоматическим. В этой части мы довольно подробно рассмотрели, то как устроена ООП система языка Scala. В следующей части мы рассмотрим неявные преобразования, неявные параметры и неявные значения, а также важный конструкт функционального программирования — тайп класс. Кроме того, мы заглянем в стандартную библиотеку и посмотрим, как можно делать хорошо известные вещи, но в функциональном стиле. Иван Камышан Типичный программист']), (['Подборка эзотерических языков программирования'], ['Многие из вас и так знают, что из себя представляют PHP с JavaScript. Есть и классики, предпочитающие C++, и приверженцы нью-скула, пишущие на C# или даже Go. А как насчёт экзотики? В этой подборке мы рассмотрим наиболее интересные и необычные с эстетической точки зрения языки программирования. И начнём мы… с нежити! Да-да, именно так. Ведь первым у нас на очереди язык настоящих некромантов и чернокнижников. Название представляет собой аббревиатуру, расшифровывающуюся как Zombie-Oriented Machine-being Interface Engine. Программисты на нём повелевают мёртвыми телами, призывают души и заставляют их подчиняться своей воле. То есть решать задачи. Структурно язык полагается на Сущности. Есть несколько типов: Нежить: души умерших, неспособные покинуть бренный физический мир по различным причинам. В нежить входят: Демоны: вольные и зловещие сущности, обычно населяющие различные параллельные реальности. Их можно призвать и назначить им задания, но необходимо иметь в виду две особенности: Джинны: капризные вольнодумные создания. Становятся подконтрольны заклинателю, если тот владеет некой вещью, привязанной к сущности создания неразрывным соединением. Горе вам, если потеряете контроль над ним, ибо Джинны таят внутри ненависть к тем, кто ими управляет. Ритуал «Hello world!»   Ритуал «Числа Фибоначчи» Этот язык программирования совершенно не понимает, к чему вся эта спешка, и выполняет команды тогда, когда ему это вздумается, в удобной для себя последовательности. Переменные? О чём речь, здесь нет даже контроля потока данных! Структура данных? Да бросьте. Так как код на Whenever обязательно исполняется последовательно, он выглядит как типичный to-do список дел, который интерпретируется при исполнении так, как придётся. Из-за этого довольно сложно убедиться в том, что определённая задача была выполнена до того, как выполнится следующая, использующая её выходные данные. Поэтому в языке присутствуют конструкции, позволяющие переназначить команды в списке дел, если необходимые для текущей задачи действия ещё не были совершены. Hello world! Постыдно тривиальная задача в списке дел на Whenever: Числа Фибоначчи Этот код выведет первые 100 чисел Фибоначчи. Когда-нибудь… Язык, чей создатель вдохновился уже легендарным Brainfuck. Предназначен для написания программ одной рукой, преимущественно левой. А код выглядит так, словно программист заснул на левой части клавиатуры. Правой можно держать кружку с кофе, например. Или подыскивать более стоящее для программиста занятие. Язык работает, опять же, подобным Brainfuck образом. Указатель перемещается в массив ячеек, значение которго по умолчанию равно нулю. Каждая ячейка может быть увеличена или уменьшена на единицу, после чего программа выведет ASCII-символ. Hello world! Что мы говорили про сон на клавиатуре? Следующий язык программирования создан в качестве некоего пазла для программеров — разработчику предстоит практически методом тыка разобраться, как и что работает, и что нужно для создания хотя бы простейшей программы. Сам язык подчиняется практически бессмысленным правилам и ограничениям. Лексический анализатор Incident работает посредством анализа исходного кода, а не исполнения предопределённых инструкций. Основная суть заключается в том, что токены для каждой отдельно взятой программы — это подстроки, повторяющиеся в ней строго три раза. При этом существуют ограничения: Традиционный «Hello world!». По крайней мере, должен быть он… А это «кошка». Все любят кошек. Надоели баги? От одной мысли о том, что придётся прочёсывать полторы тысячи строк кода в поисках пропущенной точки с запятой, волосы начинают седеть целыми прядями? Хватит это терпеть! Даёшь свободу от дебажного рабства! Libertas! А если серьёзно, то особенностью этого языка является то, что он выполнит ваш код несмотря ни на что. И даже совсем не будет засорять лог ненужной информацией. Да кому вообще дело есть до такой мелочи, как отладка. Вот код, выводящий традиционный «Hello world!»: А вот похожий код: Смутил метод toScren? А вот Libertas всё равно. Он просто продолжит выполнять следующий код. А toScren  просто будет хранить в качестве значения два самых главных слова. Пропустили закрывающие кавычки в первой строке? Тоже мне проблема, просто строка в переменной превратится в «Hello World if Date:Day:Set =». Пришло время вспомнить далёкое детство… Нет, речь не про BASIC, хотя сегодня он и считается практически реликвией. У всех или почти всех ведь была «Денди»? И принцессу искали, и монетки собирали, и по трубам лазили… Встречайте новое амплуа итальянского водопроводчика в мире программирования. Исходный код на этом языке действительно напоминает мир игр серии Super Mario. В качестве условных операторов используются символы, выступающие строительным материалом для локаций, а также указания для усатого сантехника. Всё, что не распознаётся как команда, является комментарием. В результате выполнения кода выводится последовательность чисел: 4 6 0 5 6 7 8 9 10 11 12 12 12 12 12 11. После этого пользователь должен ввести букву, и программа выведет её и следующую в алфавите. Эта программа выводит цифру 4. Двоеточие в конце означает, что Марио останавливается и выполняет вывод. Этот язык не похож ни на один из вышеперечисленных. Ведь в нём вам придётся не писать код, а… рисовать. Нет, правда. В качестве условных операторов в нём используются цвета — а точнее, их коды. Сами по себе программы работают на ленте памяти, бесконечной в обоих направлениях. Каждая ячейка памяти по умолчанию равна нулю и при изменениях принимает значение единицы. Сам исходный код представляет собой точечный рисунок, где пиксели играют роль операторов: Код программы начинает выполняться с левого верхнего угла. При достижении синего пикселя направление указателя меняется влево или вправо в зависимости от того, содержится ли в текущей ячейке начальное либо любое другое значение. При этом указатель не переходит на сам синий пиксель. Если при этом достигается край изображения, работа программы останавливается. Данная программа вводит четырёхзначное число, прибавляет к нему один, а затем выводит уже пятизначное (чтобы обработать возможное переполнение). Для работы с более простыми числами просто увеличьте части с вводом и выводом.  Ну и куда без «Hello world!»?  Вам когда-нибудь хотелось создать шедевр? Написать симфонию? Ведь программирование — это искусство! Ах, этот оркестр потоков, увертюра из инициализаций, крещендо растущих логов!.. Начните с простенького этюда, добавьте аккомпанемент в функции бэкенда, затем аранжировки UI… Безусловно, это самый прекрасный язык программирования для истинных ценителей прекрасного. Velato использует в качестве исходного кода MIDI-файлы. В качестве условных операторов язык использует ноты, чьё назначение зависит от тональности. Первой идёт «корневая» нота. Все интервалы отсчитываются от неё до следующей «корневой» ноты, которую можно менять между выражениями. Также можно играть аккорды, в которых команды выполняются в порядке представления внутри MIDI-файла, несмотря на отсутствие интервала между ними. Сами-знаете-какая программа. Ах, эта музыка!..  На самом деле, существует огромное количество экзотических, странных и просто нелепых языков программирования, мы же отобрали самые интересные на наш вкус. И вне зависимости от того, какую цель преследуют их создатели, они доказывают главное: программирование — одна из самых увлекательных и творческих профессий. Corewood']), (['Создаём простую игру на Vanilla JS'], [\"В этой статье мы создадим простую игру с помощью HTML5, CSS3 и чистого JavaScript. Вам не понадобятся глубокие знания программирования. Если вы знаете, для чего нужны HTML, CSS и JS, то этого более чем достаточно. На работу игры вы можете посмотреть здесь. Начнём с создания нужных папок и файлов: Начальный шаблон, соединяющий CSS- и JS-файлы: В игре будет 12 карточек. Каждая карта состоит из контейнера div с классом .memory-card, внутри которого находится два элемента img. Первая отвечает за лицо (front-face) карточки, а вторая — за рубашку (back-face).  Необходимые изображения можно скачать из репозитория проекта. Обернём набор карточек в контейнер section. В итоге получаем: Мы используем простой, но очень полезный сброс стилей, который будет применён ко всем элементам: Свойство box-sizing: border-box учитывает значения внутренних отступов и границ элемента при подсчёте общей высоты и ширины, поэтому нам не нужно заниматься математикой. Если применить к body свойство display: flex и margin: auto к контейнеру .memory-game, то он будет выровнен вертикально и горизонтально. .memory-game также будет flex-контейнером. По умолчанию ширина элементов уменьшается, чтобы они помещались в контейнер. Если присвоить свойству flex-wrap значение wrap, элементы будут располагаться на нескольких строках в соответствии с их размерами: Ширина и высота каждой карточки подсчитывается с помощью CSS-функции calc(). Создадим три ряда по четыре карточки, установив значения ширины и высоты равными 25% и 33.333% соответственно минус 10px от внешнего отступа. Чтобы разместить наследников .memory-card, добавим position: relative. Так мы сможем абсолютно расположить наследников относительно родительского элемента. Смотрите также: Вредные советы по CSS Свойство position: absolute, установленное для .front-face и .back-face, уберёт элементы с их исходных позиций и разместит поверх друг друга: Поле из карточек должно выглядеть примерно так:  Добавим ещё эффект при клике. Псевдокласс :active будет срабатывать при каждом нажатии на элемент. Он устанавливает длительность анимации равной 0.2 с:  Чтобы перевернуть карточку после нажатия, добавим класс flip. Для этого давайте выберем все элементы memory-card с помощью document.querySelectorAll(). Затем пройдёмся по ним в forEach-цикле и добавим обработчики событий. При каждом нажатии на карточку будет вызываться функция flipCard(). this отвечает за нажатую карточку. Функция получает доступ к списку классов элемента и активирует класс flip: CSS класс flip переворачивает карточку на 180 градусов: Смотрите также: Детальный список инструментов для JavaScript Чтобы создать 3D-эффект переворота, добавим свойство perspective в .memory-game. Это свойство отвечает за расстояние между объектом и пользователем в z-плоскости. Чем ниже значение, тем сильнее эффект. Установим значение 1000px для едва уловимого эффекта: Добавим к элементам .memory-card свойство transform-style: preserve-3d, чтобы поместить их в 3D-пространство, созданное в родителе, вместо того, чтобы ограничивать их плоскостью z = 0 (transform-style): Теперь мы можем применить transition к свойству transform, чтобы создать эффект движения: Отлично, теперь карточки переворачиваются в 3D! Но почему мы не видим лицо карточки? На данный момент .front-face и .back-face наложены друг на друга из-за абсолютного позиционирования. Рубашкой каждого элемента является зеркальное отражение его лица. По умолчанию значение свойства backface-visibility равно visible, поэтому вот что мы видим при перевороте карточки:  Чтобы исправить это, применим свойство backface-visibility: hidden для .front-face и .back-face: Если перезагрузить страницу и снова перевернуть карточку, она пропадёт!  Так как мы скрыли заднюю сторону обеих картинок, на обратной стороне ничего нет. Поэтому сейчас нам нужно перевернуть .front-face на 180 градусов: Наконец, мы получили желаемый эффект переворота!  Мы научились переворачивать карточки, теперь нужно разобраться с проверкой на совпадение. После нажатия на первую карточку она ожидает переворота другой. Переменные hasFlippedCard и flippedCard будут отвечать за состояние переворота. Если ни одна карточка не перевёрнута, значение hasFlippedCard устанавливается равным true, а нажатой карточке присваивается flippedCard. Ещё давайте сменим метод toggle() на add(): Теперь при нажатии на вторую карточку мы попадаем в else-блок нашего условия. Чтобы проверить, совпадают ли карточки, нужно их всех идентифицировать. Всякий раз, когда нам нужно добавить дополнительную информацию к HTML-элементам, мы можем использовать data-* атрибуты, где вместо «*» может быть любое слово. Добавим каждой карточке атрибут data-framework: Теперь мы можем проверить, совпадают ли карточки, с помощью свойства dataset. Поместим логику сравнения в метод checkForMatch() и снова присвоим переменной hasFlippedCard значение false. В случае совпадения будет вызван метод disableCards() и обработчики событий будут откреплены от обеих карточек, чтобы предотвратить их переворот. В противном случае метод unflipCards() перевернёт обе карточки с помощью 1500 мс тайм-аута, который удалит класс .flip: Складываем всё воедино: Более элегантный способ написать условие совпадения — тернарный оператор. Он состоит из трёх частей. Первая часть — это условие, вторая часть выполняется, если условие возвращает true, в противном случае выполняется третья часть: Мы научились проверять, совпадают ли карточки, а теперь нужно заблокировать поле. Это нужно для того, чтобы два набора карточек не могли быть перевёрнуты одновременно, в противном карточки не будут переворачиваться обратно.  Объявим переменную lockBoard. Когда игрок нажмёт на вторую карточку, lockBoard будет присвоено значение true, а условие if (lockBoard) return; предотвратит переворот других карточек до того, как эти две будут спрятаны или совпадут: У нас всё ещё есть сценарий, при котором после нажатия на одну карточку дважды условие совпадения будет выполнено и обработчик событий будет удалён.  Чтобы избежать этого, добавим проверку на то, равняется ли нажатая карточка переменной firstCard, и вёрнемся из функции, если это так: Переменные firstCard и secondCard нужно обнулять после каждого раунда. Реализуем эту логику в новом методе resetBoard(). Поместим в него hasFlippedCard = false и lockBoard = false. Деструктурирующее присваивание [var1, var2] = ['value1', 'value2'] из ES6 позволяет писать код меньших размеров: Новый метод будет вызываться из disableCards() и unflipCards(): Наша игра выглядит довольно неплохо, но играть в неё не очень весело, если карточки всегда на одном месте. Пора это исправить. Когда у контейнера есть свойство display: flex, его элементы упорядочиваются сначала по номеру группы, а потом по порядку в исходном коде. Каждая группа определяется свойством order, которое содержит положительное или отрицательное целое число. По умолчанию свойство order каждого flex-элемента имеет значение 0. Если групп больше одной, элементы сначала упорядочиваются по возрастанию порядка группы. В игре есть 12 карточек, поэтому мы пройдёмся по ним в цикле, сгенерируем случайное число от 0 до 12 и присвоим его свойству order: Чтобы вызвать функцию shuffle(), сделаем её IIFE (Immediately Invoked Function Expression). Это значит, что она будет выполнена сразу после объявления. Скрипт должен иметь примерно такой вид: Вот и всё! Перевод статьи «Memory Game in Vanilla JavaScript» Никита Прияцелюк, последний центурион\"])], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "test_period = getNewsPaper()\n",
    "a = test_period.getPeriod(3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(['Что делать, если в наследство достался некачественный код — отвечают эксперты'], ['Программирование — это не только написание кода, но и его исправление. Наш подписчик явно столкнулся с ситуацией, где ему потребовался совет опытного человека: «Что делать, если в наследство достался некачественный код?» За разъяснениями мы обратились к нашим экспертам, а полученные ответы предоставляем вашему вниманию. Если вам в голову пришел такой вопрос, то главное не бежать все переписывать. Обычно это стандартная реакция одного программиста на код другого программиста — в коде ничего непонятно, сложно разбираться. Кажется проще переписать все с нуля. Переписать вы всегда успеете, сначала попробуйте разобраться в требованиях и способах использования приложения. Обычно кодовая база растет эволюционно и накапливает бизнес-логику, которая может быть неочевидной для внешнего человека. Попытка переписать эту логику с нуля породит много багов и проблем. Если после недели чтения кода и документации все ещё ничего непонятно, то, кажется, код действительно «не очень». В этот момент нужно понять, как вы будете взаимодействовать с этим кодом. Вам его дали на поддержку? Или нужно его развивать? Если нужно поддерживать систему в работоспособном состоянии без серьезных доработок, то переписывать точно НЕ нужно. Напишите тесты на критические функции, чтобы быть уверенным в корректной работе. Если возникает вопрос о доработке системы, которую сложно расширять или архитектура которой устарела, то это вопрос, требующий обсуждения с заказчиком системы. Обязательно поговорите с заказчиком и донесите до него информацию о том, что расширение текущей системы становится дорогим (или долгим). Нужно подготовить обоснование тому, почему мы хотим причинить боль всем, переписывая систему, которая уже работает. Когда все заинтересованные лица поняли проблему и согласились ее исправлять, нужно подготовить план действий. Я участвовал в нескольких таких проектах — хуже всего при переработке системы уйти на полгода писать новый код. Такой подход проваливается в подавляющем большинстве случаев. Самый удачный вариант — переписывать частями. Выделять модули из существующей системы, покрывать тестами, а затем менять реализацию внутри модуля. Обязательно показывать промежуточные результаты всем заинтересованным и слушать обратную связь. Так вы победите! Для начала нужно убедиться, что код действительно плох: не исключено, что он сложен для понимания из-за специфики предметной области. Например, код DSP-контроллера чрезвычайно сложно читать без релевантного опыта. Для верности стоит посоветоваться с коллегами, более осведомленными в данном вопросе. Если код действительно плохого качества, важно понять, можно ли закрыть на него глаза. Толерантность к чужому коду — важный скилл. Если нельзя, стоит определить:\\n— Какова трудоемкость переписывания кода (оценка трудоемкости — это сложно, есть риск просчитаться на целые месяцы);\\n— Хорошо ли ты понимаешь код, который планируешь переписать;\\n— А сможешь ли ты написать более качественный код;\\n— Если проблемный код не покрыт тестами (а так, скорее всего, и будет), то есть риск, что новый код создаст регрессию в проекте. Готов ли ты взять на себя ответственность за результат? На заметку: переписанный код в лучшем случае будет оценен по достоинству вашими коллегами, но не начальством. Другими словами, премию и прочие плюшки за это не жди. Если все же остается желание переписать код, могу дать несколько советов:\\n— Максимально декомпозируй код — чем меньше кусок переписываемого кода, тем лучше.\\n— Используй методы рефакторинга, покрой код тестами, если их нет.\\n— Не трать много времени на переписывание, оно не должно мешать основной работе. В первую очередь нужно разделить понятия наследия и некачественного кода. Legacy code — устойчивое выражение, обозначающее, что код, попавший в руки, был написан достаточно давно. И тут нужно учесть, что, возможно то, что было написано 10 лет назад очень актуально для своего времени. Назвать такой код некачественным — вряд ли, устаревшим — вполне. Это может быть вполне поддерживаемая, масштабируемая система, просто сейчас никто таких не делает. С таким кодом можно продолжить работу в определенной парадигме. А некачественный код мог быть написан буквально вчера. И это уже совсем другой — более тяжелый случай. Если в случае Legacy code возможна дальнейшая поддержка и надстройка, то когда тебе достается откровенно плохая работа, есть смысл задуматься о рентабельности проекта в целом. В этом случае придется, скорее всего, переписывать с нуля. И тут очень важно понять, на каком этапе находится проект: если это старт, то переписать код достаточно легко, если уже заключительный этап — то проще доделать как есть, хотя риск, что в итоге система не будет работать велик. Важно объяснить это заказчику. Наверно, каждый разработчик на своем программистском поприще рано или поздно сталкивался с ситуацией, когда смотришь на чужой код и думаешь: «Это что, шутка?» Что же делать, когда открываешь код и на глаза, кроме слез, ничего не попадает? Как не впасть в панику и не положить заявление на стол? Понятие «некачественный код» у каждого свое. Более того, в каждом языке программирования свои парадигмы и представления об идеальном коде. Для себя я вывела несколько уровней «греха» под названием «некачественный код». 0-ой уровень:\\nпроблема с переменными. Это очень абстрактное понятие, но если обобщить, то сюда попадает интуитивно непонятное наименование переменной (ar1, ar2, item), лишние переменные, которые нигде не используются, подтягивание одних и тех же данных несколько раз и т.д. Решить данную проблему достаточно просто, хоть и не доставит особого удовольствия. Переименовать переменные в интуитивно понятный вид (array_for_organization, obuv_items, users_telephones), удалить лишние вызовы, присваивания и переменные. 1-ый уровень:\\nодин из самых безобидных, как по мне, отсутствие комментариев. Должна признаться, сама я этим иногда тоже грешу, особенно в ситуациях, когда «надо сделать вчера», тогда ты делаешь максимально быстро, но потом это аукается и приходится потратить очень много времени на разбор кода. Однако, если все достаточно четко расписано, структурировано и нет лишнего, этот грех является самым меньшим и с ним проще всего справиться. В этом случае ваша проблема заключается в том, что вам придется писать комментарии вместо создателя кода. Комментарии в данной ситуации помогут вам разобраться что к чему и почему и ускорят разработку в дальнейшем. 2-ой уровень:\\nэтот «грех» уже серьезнее, чем первые два и может доставить очень много неприятностей. Неоптимизированная выборка, зацикливание и прочее. Что-то, что может повесить систему или страницу (если говорим про веб). Данная ситуация также поправима при наличии головы на плечах, однако куски кода с неоптимальной выборкой придется переписать практически полностью, т.е. придется переделывать работу предыдущего человека, а это долго и вообще очень неприятно. 3-ий уровень:\\nпрактически самый серьезный. Отсутствие документации. Очень часто встречаю ситуацию, когда на одной странице вызывается множество различных методов, констант и прочего, но абсолютно непонятно, где все это хранится и что там лежит. Проблема для вас будет заключаться в выявлении возможных ошибок и поиске методов, если код достаточно объемный. Здесь вам на помощь придет поиск и составление собственной документации. Конечно, чем дольше работаешь на одном проекте, тем проще ориентироваться, однако для упрощения жизни себе и другим разработчикам неплохо бы все же составлять краткую выжимку, что где и откуда. 4-ый уровень:\\nсамый серьезный. «Грех» четвертого уровня обычно совершают новички, не особенно знакомые с продуктом, с которым работают. Правка файлов, которые не следует править. Поясню на примере CMS для веб-разработки, т.к. я занимаюсь именно веб-разработкой. Каждая CMS основана на неком ядре, на основе которого разработчик создает нечто свое, но использует те методы, модули и классы, которые изначально заложены ядром. Так вот самый большой грех это правка файлов этого ядра. Этот «грех» очень сложно выявить, практически невозможно устранить и в процессе может доставить очень много неприятных моментов. Я здесь не учла, конечно, ситуации, когда все эти грехи встречаются одновременно, а такое тоже очень часто бывает, однако я постаралась структурировать те ситуации, которые возникали в моей практике и способы борьбы с ними (в идеальном мире). Всем хорошего дня и побольше качественного кода! Если вам «прилетел» плохой код, начните с прямого вопроса заказчику, насколько ему важно сохранить код в текущем состоянии. Если очень важно, можно попытаться доступно и без лишних эмоций объяснить минусы этого решения. В частности, донести до клиента, что низкое качество кода неизбежно увеличивает расходы на дальнейшее развитие продукта. Какие аргументы можно привести? 1. На то, чтобы разобраться с кодом, уйдет неоправданно много времени. Простой пример: команде достался по наследству функционал, работающий на четырех переменных и особой математической магии. Нужно было поменять в нем одно условие. При хорошем коде эта работа заняла бы 30 минут, но программист убил 10 часов, потому что ему пришлось практически побуквенно разбираться, как все это работает. 2. При сохранении некачественного кода в продукте возникнет так называемый «фактор автобуса»: разобравшийся в проекте специалист станет единственным носителем знания по нему. Если он уволится (или попадет под автобус, как следует из названия этого явления), владелец кода опять понесет все убытки, связанные с вводом новых программистов в проект. Также не помешает дать клиенту точную оценку затрат на рефакторинг. Если даже после этих аргументов заказчик не согласится с тем, что старый код безопаснее облить бензином и сжечь, придется работать с тем, что есть. Дописывать чужой код лучше в своем стиле, максимально лаконично и правильно. Как говорится, пиши код так, как будто поддерживать его будет психопат, который знает твой домашний адрес. Есть время и возможность переделать? Выпиши на листочек всю логику плохого куска кода, перепиши его в более структурированном виде; добавь удобное, убери повторяющееся; перепиши еще раз — уже в том виде, в котором тебе самому приятно это читать. В общем, если при изучении legacy-кода у вас появляется стойкое ощущение, что его писала бешеная обезьяна под бутиратом, не молчите — обсуждайте возможность рефакторинга. Не получилось этого добиться — улучшайте код по мере сил в процессе. Оставлять как есть — самый неразумный вариант. Напоминаем, что вы можете задать свой вопрос экспертам, а мы соберём на него ответы, если он окажется интересным. Вопросы, которые уже задавались, можно найти в списке выпусков рубрики. Если вы хотите присоединиться к числу экспертов и прислать ответ от вашей компании или лично от вас, то пишите на experts@tproger.ru, мы расскажем, как это сделать. Анастасия Витвицкая']), (['Разбираемся в типах NoSQL СУБД'], ['В этой статье мы познакомимся с разными типами NoSQL СУБД. Всего есть 4 основных типа: Отсутствие схемы в базах данных «ключ-значение», например, Riak, — это как раз то, что вам нужно для хранения данных. Ключ может быть синтетическим или автосгенерированным, а значение может быть представлено строкой, JSON, блобом (BLOB, Binary Large Object, большой двоичный объект) и т.д. Такие базы данных как правило используют хеш-таблицу, в которой находится уникальный ключ и указатель на конкретный объект данных. Существует понятие блока (bucket) — логической группы ключей, которые не группируют данные физически. В разных блоках могут быть идентичные ключи. Производительность сильно вырастает за счёт кеширующих механизмов, которые работают на основе маппингов. Чтобы прочитать значение, вам нужно знать как ключ, так и блок, поскольку на самом деле ключ является хешем (блок + ключ). В модели «ключ-значение» нет ничего сложного, так как реализовать её проще простого. Не лучший способ, если вам нужно только обновить часть значения или сделать запрос к базе данных. Если поразмыслить о теореме CAP, то становится довольно очевидно, что такие хранилища хороши в плане доступности (Availability) и устойчивости к разделению (Partition tolerance), но явно проигрывают в согласованности данных (Consistency). Пример: посмотрим на набор данных, представленных таблицей ниже. Здесь ключ — это название страны, а значение — список адресов в этой стране:  База данных такого типа позволяет читать и записывать значения с помощью ключа следующим образом: И хотя базы данных типа «ключ-значение» могут пригодиться в определённых ситуациях, они не лишены недостатков. Первый заключается в том, что модель не предоставляет стандартные возможности баз данных вроде атомарности транзакций или согласованности данных при одновременном выполнении нескольких транзакций. Такие возможности должны предоставляться самим приложением. Второй недостаток в том, что при увеличении объёмов данных, поддержание уникальных ключей может стать проблемой. Для её решения необходимо как-то усложнять процесс генерации строк, чтобы они оставались уникальными среди очень большого набора ключей. Riak и Dynamo от Amazon — самые популярные СУБД данных такого типа. Данные, представленные парами ключ-значение, сжимаются как хранилище документов схожим с хранилищем «ключ-значение» образом, с той лишь разницей, что хранимые значения (документы) имеют определённую структуру и кодировку данных. XML, JSON и BSON — некоторые из стандартных распространённых кодировок. В следующем примере можно увидеть данные в виде «документа» который отображает названия определённых магазинов. Обратите внимание, что, хотя все три примера содержат местоположение, они отображают его по-разному: Одним из ключевых различий между хранилищем «ключ-значение» и документоориентированным является то, что последний включает метаданные, связанные с хранимым содержимым, что даёт возможность делать запросы на основе содержимого. Например, в указанном примере можно попробовать найти все документы, в которых «City» равно «Noida», что вернёт все документы, связанные с магазинами в этом городе. Apache CouchDB — пример документоориентированной СУБД. CouchDB использует JSON для хранения данных, JavaScript в качестве языка запросов с использованием MapReduce и HTTP для API. Данные и отношения не хранятся в таблицах так, как в традиционных реляционных базах данных, а по сути являются набором независимых документов. Тот факт, что такие базы данных работают без схемы, делает простой задачей добавление полей в JSON-документы без необходимости сначала заявлять об изменениях. Couchbase и MongoDB — самые популярные документоориентированные СУБД. В колоночных NoSQL базах данных данные хранятся в ячейках, сгруппированных в колонки, а не в строки данных. Колонки логически группируются в колоночные семейства. Колоночные семейства могут состоять из практически неограниченного количества колонок, которые могут создаваться во время работы программы или во время определения схемы. Чтение и запись происходит с использованием колонок, а не строк. В сравнении с хранением данных в строках, как в большинстве реляционных баз данных, преимущества хранения в колонках заключаются в быстром поиске/доступе и агрегации данных. Реляционные базы данных хранят каждую строку как непрерывную запись на диске. Разные строки хранятся в разных местах на диске, в то время как колоночные базы данных хранят все ячейки, относящиеся к колонке, как непрерывную запись, что делает операции поиска/доступа быстрее. Пример: получение списка заголовков нескольких миллионов статей будет трудоёмкой задачей при использовании реляционных баз данных, так как для извлечения заголовков придётся проходить по каждой записи. А можно получить все заголовки с помощью только одной операции доступа к диску. Модель данных Самыми известными примерами являются Google BigTable и HBase с Cassandra, вдохновлённые BigTable. BigTable представляет собой высокопроизводительное, сжатое и проприетарное хранилище данных от Google.  У него есть следующие атрибуты: Двумерная таблица, состоящая из строк и колонок, является частью реляционной системы баз данных.  Эту таблицу можно представить в виде BigTable-сопоставления следующим образом: На колонки можно ссылаться с помощью колоночного семейства. В графовой базе данных вы не найдёте строгого формата SQL или представления таблиц и колонок, вместо этого используется гибкое графическое представление, которое идеально подходит для решения проблем масштабируемости. Графовые структуры используются вместе с рёбрами, узлами и свойствами, что обеспечивает безиндексную смежность. При использовании графового хранилища данные могут быть легко преобразованы из одной модели в другую.  Контейнерная иерархия документоориентированной базы данных содержит данные без схемы, которые можно представить в виде дерева, которое является графом. Если обращаться к документам или их элементам в этом дереве, можно получить более выразительное представление данных, в котором можно легко ориентироваться с помощью Neo4j. Далее описаны некоторые особенности графовой базы данных на основе примера ниже:  Помеченный, направленный, атрибутированный мультиграф: граф содержит узлы, которые помечены определёнными свойствами и которые имеют связи друг с другом, что представлено направленными рёбрами. Например, связь «Элис знает Боба» выражена ребром с некоторыми свойствами. Хотя реляционные базы данных могут скопировать поведение графовых, рёбрам потребуется соединение (JOIN), что дорого обойдётся. Пример использования Любой рейтинг «Рекомендовано вам», который можно увидеть на разных сайтах, зачастую составляется исходя из того, как другие пользователи оценили продукт. Графовые базы данных отлично подходят для такого случая. InfoGrid и Infinite Graph — самые популярные графовые базы данных. InfoGrid позволяет соединять множество рёбер (Relationships) и узлов (MeshObjects), что упрощает представление набора информации со сложными взаимными ссылками. InfoGrid предлагает два типа баз данных: Смотрите также: SQL против NoSQL на примере MySQL и MongoDB Перевод статьи «EXPLORING THE DIFFERENT TYPES OF NOSQL DATABASES» Никита Прияцелюк, последний центурион']), (['Как на фронтенд-собеседовании превратить сложный вопрос в лёгкий'], ['Рассказывает Алекс Паттисон За последние пару месяцев я активно проводил собеседования фронтенд-разработчиков. Каждый, кто когда-либо бывал на подобных собеседованиях, знает, что вопросы могут быть совершенно разные, а вот уровень знаний необходимо демонстрировать неизменно высокий. Вас могут спросить о чём угодно, начиная со структур данных/алгоритмов и заканчивая нюансами CSS. При подготовке к очередному собеседованию возникает чувство, будто нас просят вызубрить всю информацию с MDN. Тем более, что большинство тренировочных материалов для собеседований сконцентрированы на CS, поэтому фронтенд-разработчику приходится особенно туго. В этой статье я собираюсь объяснить мой подход к подготовке к интервью. Суть в том, чтобы научиться сводить каждый вопрос по структурам данных/алгоритму к DOM. DOM — это дерево узлов. Оно очень органично соединяет друг с другом как определённые структуры данных: графы (родительский класс), связные списки (подкласс дерева и, в частности, унарное дерево), — так и сами деревья. Немного креатива, и любой вопрос можно подстроить под фронтенд-тематику. Все примеры будут рассматриваться для Vanilla JS. На собеседованиях довольно часто спрашивают о DOM-манипуляциях, а времени пробежаться по любимому фреймворку (и всему встроенному инструментарию) не остаётся. Перед соискателями ставится трудная задача, ведь специалисты, как правило, мало пишут на Vanilla JS. Они предпочитают React, Angular, Vue и т. д. Но попрактиковаться в работе с чистым JS заранее всё-таки нужно: так вы гарантированно облегчите свою жизнь на следующем собеседовании. А учитывая тот факт, что почти все проблемы с кроссбраузерностью остались в прошлом, лишний козырь в виде умения писать на чистом JS явно не помешает. Задача: Верните значение k-тому с конца элементу односвязного списка. Первый шаг при преобразовании вопроса о структурах данных/алгоритмах к формату фронтенда — поиск оптимального способа представления данных в DOM. Поскольку связный список — не что иное, как унарное дерево, можно представить задачу в виде иерархической структуры узлов, где у одного предка будет по одному потомку. Несмотря на удобство данного варианта, лучше рассмотреть задачу под другим углом — так будет легче добавить простые стили. Фронтенд-вопрос: Измените цвет фона k-того с конца среди сестринских элементов. Вы стараетесь смоделировать связный список, поэтому необходимо ограничение: в каждом узле может быть доступ только к nextElementSibling (и встроенным стилям). Теперь задачу можно решить по-разному. Для начала проанализируйте возможные варианты. Не садитесь сразу за написание кода, ведь первое пришедшее на ум решение не всегда оказывается самым правильным. Варианты действий: Давайте разберём каждый вариант по порядку: Как оказалось, можно немножко схитрить во втором варианте, взяв из него самое лучшее и избежав лишних действий в виде повторной итерации по длине списка. Мы просто добавим к нему сразу два указателя. Пока главный указатель движется к концу списка, а разница между указателями равна k\\u200a-1, следующий указатель остаётся в узле, который мы выделим цветом. На картинке всё смотрится более понятно:  Представим, что у нас есть HTML, который выглядит как-то так: Как именно использовать последний указатель для закрашивания k из последнего элемента красным? Посмотрим внимательнее на код: Сначала мы инициализируем наши указатели leader и follower (2–3 строки) — оба они указывают на начало списка. Задаём выполнение цикла до тех пор, пока первый указатель не достигнет следующего сестринского элемента. Это будет индикатором окончания списка. Строка 12 перенаправляет наш главный указатель в сторону родственного элемента. Теперь посмотрим на строки 6–10. Здесь указано, нужно ли нам также перенаправлять и follower. Нам нужно, чтобы при k = 1 оба указателя двигались вперёд. Логика тут немного хромает, но мы вынуждены действовать так в рамках стандартного использования языка. Если я говорю «сделать вторую с конца ссылку красной», то это значит поменять цвет ссылки под номером 8. При наших текущих условиях входное значение k = 2 как раз и будет «второй ссылкой с конца».\\nВыставим дополнительное условие в строке 15 — так мы случайно не покрасим первую ссылку, если значение k превысит количество узлов в списке. Наконец, вызываем наш метод: Проверяем рабочий пример — вот здесь:  Данный код выполняется с O(n) временной сложностью и O(1) пространственной сложностью. Даже несмотря на то, что нам придётся перебирать весь список для определения длины, этот вариант всё равно окажется лучшим из всех существующих. Если вас заинтересовал третий вариант, то, как я и обещал, даю подсказку: Нам нужно просто поменять цвет узла, а возвращаемое значение функции здесь не важно. countAhead — это количество узлов перед текущим значением (включительно). То есть при вызове верхнего уровня метод recTurnKthToLastRed() вернёт длину нашего списка. Примечание: После обсуждения статьи с друзьями я бы хотел подчеркнуть, что самое важное здесь — найти работающее решение. А мозговой штурм с обдумыванием вариантов как раз и нужен для того, чтобы самим для себя понять, что именно мы хотим получить. Не пытайтесь сразу найти лучшую реализацию чего-либо. Ведь всегда можно провести оптимизацию и сделать небольшой рефакторинг готового кода. Написанный код, который ещё не оптимизировали, — это ВСЕГДА лучше, чем оптимизированный код, который ещё не написали. Смотрите также: Хочу стать frontend разработчиком: базовые знания и план обучения Перевод статьи «Cracking the (Frontend) Coding Interview» Ольга Сайфудинова ']), (['Выход Google Chrome 70 может привести к блокировке сайтов'], ['Релиз браузера Google Chrome 70 ожидается уже 16 октября. И выход новой версии может спровоцировать волну недовольства пользователей, поскольку программа начнёт блокировать сайты, содержащие старые сертификаты Symantec. Многие сайты до сих пор используют сертификаты, выпущенные Symantec, Thawte, VeriSign, Equifax, GeoTrust и RapidSSL до июня 2016 года. По словам специалиста по безопасности Скотта Хельма (Scott Helme), он нашёл в топе крупнейших ресурсов 1139 сайтов со старыми сертификатами. В их числе есть Федеральный банк Индии, сайт правительства Тель-Авива, Social Science Research Network и другие. Как ожидается, Google Chrome 70 начнёт блокировать такие ресурсы, и зайти на них можно будет, разве что, добавив сайт в исключение. Однако это потенциально может угрожать безопасности, ведь сертификаты HTTPS не только подтверждают легальность сайта, но и гарантируют, что никто не сможет получить доступ к вашей информации, даже при использовании публичных Wi-Fi сетей. В большинстве случаев их получают от центров сертификации. Однако для этого нужно соблюсти определённые правила. А сертификаты Symantec ещё в прошлом году были признаны Google неподходящими. Сама же компания, как оказалось, выдаёт их без строгого надзора, что вынудило тысячи сайтов срочно заменить ключи. С другой стороны, в начале года поставщик бесплатных сертификатов HTTPS под названием Let’s Encrypt получил «зелёный свет» со стороны всех основных разработчиков браузеров — Apple, Google, Microsoft и Mozilla. На сегодняшний день некоммерческая организация выдала более 380 миллионов сертификатов. Это уже не первая проблема с Google Chrome. Ранее версия 69 не удаляла «куки» сервисов Google, а также скрывала поддомены даже в тех случаях, когда этого не требовалось. Разработчики пообещали, что в Google Chrome 70 уберут эту функциональность. Source: TechCrunch Андрей Галадей']), (['HeadHunter: У разработчиков ПО самые высокие зарплаты в России'], ['Пресс-служба HeadHunter сообщила «ТАСС», что разработчики программного обеспечения возглавляют рейтинги самых высокооплачиваемых профессий как в Москве, так и в остальных регионах России. Средняя зарплата столичного специалиста достигает 130 тыс. рублей, их коллеги в разных уголках страны получают до 85 тыс. рублей. По данным HeadHunter за три квартала 2018 года, в число топовых профессий Москвы также входят специалисты в сфере консалтинговых услуг, которые зарабатывают в среднем 127 тыс. рублей, и администраторы БД с зарплатой 123 тыс. рублей. Чуть ниже в списке значатся: В регионах за разработчиками ПО следуют: Также анализ HH показал, что темп роста зарплат в регионах превышает столичный. В июле 2018 года свою аналитику по зарплатам в IT-сфере опубликовала школа HackerU. По данным организации, первое место по размеру оплаты труда занимают специалисты, проводящие тесты на проникновение. Чуть позже, в сентябре, Stack Overflow обновил свой зарплатный калькулятор, добавив в число поддерживаемых стран Россию. Source: ТАСС Екатерина Никитина']), (['Эволюция криптографии: от математики до физики'], ['Нужно ли быть математическим гением, чтобы понять криптографию? Возможно, низкоуровневое понимание нужно разве что криптографу, чья работа состоит в изобретении сложного для взлома алгоритма. Шифрование каждый день используется для защиты банковских карт, удалённого подключения к рабочему месту по сети или интеллектуальной собственности от цифрового пиратства. Цель этой статьи — объяснить изумительную науку криптографию настолько просто, чтобы все могли понять, как она используется для шифрования данных. Слова «криптология» и «криптография» в современной литературе часто используются как взаимозаменяемые термины, но между ними есть семантическая разница. У слов различные значения, которые проще всего объяснить так: Большая часть статьи посвящена «криптографии», как это принято сегодня, и хотелось бы, чтобы вы знали различия и значения обоих слов. Само по себе изучение криптологии как науки существует уже много лет. Первый известный случай использования криптографии был обнаружен в надписи, высеченной в 1900 году до нашей эры, в главной гробнице дворянина Хнумхотепа II, в Египте. Писец (да) то тут, то там использовал некоторые странные иероглифы вместо обычных. Целью было скорее не спрятать сообщение, а изменить его форму, чтобы оно выглядело более величавым.  Во время расцвета Римской Империи (100-й год до нашей эры) Юлий Цезарь, как известно, использовал форму шифрования для передачи секретных сообщений своим армейским генералам во время войны. Этот шифр подстановки (шифр Цезаря), вероятно, самый упоминаемый в литературе исторический шифр (шифр — алгоритм, используемый для шифровки или дешифровки сообщений). В шифрах подстановки каждая буква исходного сообщения (исходное сообщение — текст, который будет зашифрован) заменяется на другую букву, находящуюся правее или левее исходной по алфавиту, для получения шифротекста (шифротекст — зашифрованное сообщение). В шифре Цезаря используется сдвиг вправо на 3 позиции, так что «A» становится «D», «B» — «E» и т. д. Алфавит в данном случае будет замкнут и после «X» следует «A». Во время Второй мировой войны морские пехотинцы США завербовали и обучили людей из племени индейцев навахо, свободно владеющих языком навахо. Это был привлекательный способ использования кодирования, мало людей не из племени знали этот язык, а также не было никаких опубликованных книг на языке навахо. Речевой язык навахо был не очень сложен по криптографическим стандартам и, вероятно, был бы быстро взломан при взаимном сотрудничестве носителя языка и обученных криптографов. У японцев была возможность сделать это, когда в 1942 году на Филлипинах они пленили Джо Кийомию во время Марша смерти в Батаане. После попадания в плен Кийомия, навахо-сержант в армии США, но не носитель кода, получил приказ расшифровать перехваченное радиосообщение. Однако, так как Кийомия не был обучен коду, используемому военными службами США, то слова ему показались ничего не значащими. Когда он доложил, что не может понять перехваченные сообщения, его начали пытать. Японская Имперская Армия и ВМС так и не взломали устный код. В начале 1970-х компания IBM осознала, что их пользователи требуют какой-нибудь вид шифрования, и сформировала «криптогруппу», возглавляемую Хорстом Фейстелем. Они спроектировали шифр под названием Lucifer. В 1973 году Национальное бюро стандартов США (сейчас NIST — Национальный институт стандартов и технологий США) объявило конкурс на блочный шифр, который должен был стать национальным стандартом. Видимо, они наконец-то осознали, что покупают множество коммерческого ПО без хорошей криптографической поддержки. В конце концов, Lucifer приняли и переименовали в DES (Data Encryption Standard). DES был взломан методом полного перебора в 1997 году. Главной уязвимостью DES была маленькая длина ключа. Когда вычислительная мощность компьютеров увеличилась, то метод полного перебора стал возможен для получения исходного сообщения. И если в 1980-х годах DES был единственным вариантом, то сейчас это изменилось. Сегодня у нас есть широкий выбор более сильных, быстрых и улучшенных алгоритмов. Сейчас проблематично выбрать среди них нужный. В 1997 году NIST объявил конкурс на новый блочный шифр и получил 50 заявок. В 2000 году был принят Rijndael и назван AES (Advanced Encryption Standard). Шифрование — это процесс изменения данных таким образом, чтобы они стали неузнаваемыми и бесполезными для несанкционированного лица. А дешифрование — процесс превращения данных в их первоначальный вид. Наиболее безопасные виды шифрования используют математические алгоритмы и переменную — «ключ». Выбранный ключ (зачастую любая случайная последовательность) вводится при шифровании и является неотъемлемой частью изменения данных. Тот же самый ключ необходим для дешифровки сообщения. Это основа защиты, если ключ известен только уполномоченному лицу (лицам), то данные не смогут раскрыть другие стороны. Только те, кому известен ключ, могут расшифровать сообщение. Такой способ называется симметричным шифрованием, и он наиболее популярен. Основные причины, по которым требуется криптография: Криптография — это искусство и наука сокрытия (через шифрование) чувствительной информации. Она включает в себя шифрование (когда шифр первоначально применяется к необработанному «открытому/исходному тексту» и дешифрование, когда шифр используется для возвращения сообщению читабельной формы). Чтобы лучше понять эти шифры, рассмотрим простенький пример. Квадрат Полибия — шифр простой замены. В данном примере будет использоваться двумерная матрица 6х6, содержащая заглавные буквы алфавита и цифры от 0 до 9. Матрица будет выглядеть следующим образом:  С матрицей 6х6 (36 буквенно-цифровых знаков) мы можем начать замену. Например, буква «А» имеет адрес 1х1 или x=1, y=1. Эту запись можно упростить до 11. Другой пример: адрес буквы «N» будет 2х3 или x=2, y=3 или 23. Давайте зашифруем простое сообщение: Сообщение: ENCRYPT ME 2 DAY Шифротекст: 51–23–31–63–15–43–24 13–51 55 41–11–15 Шифр может сделать достаточно длинным и сложным, используя прописные буквы и специальные символы. Также повторение символов и написание алфавита вразброс может дать непредсказуемый результат, устойчивый для метода полного перебора. Это аналогично полиморфизму, используемому сегодня в современных компьютерных системах шифрования.  Шифр Цезаря, названный в честь его создателя — Юлия Цезаря, считается самым первым шифром, изобрётенным человечеством. Цезарь использовал его для кодирования сообщений своим генералам, чтобы враги из Римской Империи не смогли прочитать приказы при перехвате. Шифр Цезаря имеет элементарную форму шифрования, и его очень легко взломать. По этой причине сейчас он не используется для шифрования каких-либо серьёзных данных. Шифр Цезаря просто сдвигает алфавит вправо или влево. Разные значения сдвига приводят к разным результатам шифровки. Число сдвига — это число букв, на которое происходит смещение в одну из сторон, для создания шифротекста. Пример использования шифра со сдвигом влево на 3: Сообщение: ENCRYPT ME Шифротекст: HQFUBSW PH Шифротекст выше может быть легко взломан методом полного перебора, который заключается в сдвиге в одну из сторон на одну позицию, пока не получится какое-то смысловое сообщение. Прим. пер. Существует более простой способ взлома шифра Цезаря — частотный анализ, о котором автор не упоминает. Он заключается в подсчёте частоты встреч каждого символа в любом обычном тексте и в шифротексте. Потом символы с похожими частотами заменяются. Например, если в шифротексте чаще всего встречается буква «T», то она заменяется на букву «Е» для английского алфавита. Этот способ действует только для текстов свыше 300 символов. С помощью такого принципа шифрования можно создать более сложные шифры, такие как шифры Виженера или Гронсфельда. В них применяется метод замены. В расшифровке можно легко запутаться, так как каждый символ сдвигается по своим правилам.  Прежде чем продолжить изучение, важно понять, как работают простые шифры, так как они являются базой, на которой строится всё шифрование. Стеганография — процесс написания скрытых сообщений, больше подходит под определение классической криптографии, так как современная криптография стала синонимом «компьютерной безопасности». Полиморфизм является относительно продвинутой практикой в криптографии и часто используется в техниках компьютерного шифрования. Полиморфное преобразование — это техника, которая самостоятельно модифицирует криптоалгоритм после каждого выполнения, таким образом на каждой итерации получаются различные результаты. Такое часто встречается в шифровальных алгоритмах, используемых злоумышленниками. Это означает, что если понадобится зашифровать одну и ту же информацию два раза, то алгоритм выдаст разные шифротексты. Например, представьте ключ к машине. Сейчас у нас всех есть маленькое электронное устройство, которое может по одному лишь нажатию кнопки дистанционно разблокировать машину. Вы наверняка не задумываетесь о том, что при каждом нажатии на кнопку вы отсылаете небольшое сообщение с помощью своего устройства, и это сообщение индивидуально для каждой машины. Если оно совпадёт, то машина откроется. Было бы проще назначить каждой машине свою частоту работы, но это сложно в реализации, ведь диапазона частот может не хватить. Вместо этого используется одна частота, но разные алгоритмы шифрования информации, посылаемой машине. Эти алгоритмы полиморфны. Такие алгоритмы сложнее взломать, так как они каждый раз меняются. Даже если хакер разузнает алгоритм (что уже сложно с полиморфными шифрами), то ему ещё потребуется найти пару из машины и ключа, что довольно сложно. Сегодня шифры используют алгоритмы либо с секретным, либо с публичным ключом. В шифрах с закрытым ключом используется единственный ключ, которым обмениваются стороны. Такой ключ или шифр также называют симметричным.  В 1949 году Клод Шеннон из Bell Laboratories опубликовал фундаментальную теорию, положившую начало симметричному шифрованию, а десятилетия эволюции принесли примеры высокого качества. Однако только в 1975 году мощный алгоритм с закрытым ключом DES стал доступен для общего пользования. Шифрование с помощью открытого ключа или асимметричное шифрование также возникло в середине 1970-х. Асимметричные шифры используют пару ключей — открытый, им делятся с другими людьми, и соответствующий ему закрытый, пользователь должен хранить его в секрете от других. Например, получатель может сгенерировать пару ключей и поделиться открытым ключом со всеми, кто хочет отослать ему секретное сообщение. Отправитель может зашифровать сообщение для получателя с помощью открытого ключа, а получатель расшифровать его с помощью закрытого.  Стойкость шифровального алгоритма зависит от трёх важных факторов: Пример из жизни: популярный файловый архиватор PKZIP обычно применял встроенный алгоритм шифрования, который использовал 64-битный ключ. В теории для проверки всех ключей понадобилось бы 2^64 попытки. Но на практике оказалось, что в шифровании есть недостаток, который позволил сократить количество попыток взлома шифротекста до 2^27.\\nЕдинственный способ найти такие недостатки — попытаться взломать алгоритм шифрования на практике, используя трюки, сработавшие на других алгоритмах. О качестве алгоритма можно судить после того, как алгоритм подвергли тщательному анализу и попытались взломать. Но даже это не спасает от возможности того, что позже кто-то другой найдёт недостатки в алгоритме шифрования. DES выдержал испытание временем, потому что качество шифрования доказывалось на протяжении многих лет опубликованных исследований. После четверти века исследований учёным удалось найти несколько спекулятивных атак, которые в конечном итоге не были столь эффективными, как метод полного перебора. Единственная реальная слабость DES-шифра — его критически маленькая длина ключа в 56 бит.  Triple DES (3DES) \\u200aмодификация DES, позволяющая увеличить длину ключа до 112 или 168 бит. Получившийся шифр намного медленнее других шифров, использующих такую же длину ключа, но вышел из употребления, когда мощные компьютерные атаки взломали алгоритм. AES \\u200a(Advanced Encryption Standard или Rijndael) поддерживает три длины ключа 128, 192 и 256 бит и использует 128-битный размер блоков. В настоящее время он считается достаточно стойким и используется по всему миру.  Так как DES был специально разработан для аппаратного обеспечения, то не было предусмотрено, чтобы он эффективно работал в ПО. NIST протестировал работу алгоритма AES в программной среде и разработал требования к хранению криптоматериала, чтобы гарантировать, что AES будет эффективно работать на C и Java, которые используются на рабочих станциях, а также в более ограниченных средах встроенный процессоров ARM и смарт-карт. Архитектура AES основана на принципе, известном как замена и перестановка, и быстро работает как в программном, так и на аппаратном уровнях. В отличие от своего предшественника — DES, AES не использует сеть Фейстеля (прим. пер. — один из методов построения блочных шифров). AES — это вариант Rijndael с фиксированной длиной блоков в 128 бит и критическим размером 128, 192 и 256 бит. Напротив, спецификации Rijndael задаются длиной блоков и ключей, которые могут быть кратными 32 в диапазоне от 128 до 256 бит. AES работает с матрицей порядка 4х4 столбцов с байтами, называемой состоянием, хотя некоторые версии Rijndael имеют больший размер блоков и дополнительные столбцы. Большинство расчётов AES выполняются в определённом конечном поле. Длина ключа, используемого для шифрования AES, указывает на количество повторений раундов преобразования, которые преобразуют входной сигнал, называемый исходным текстом, а конечный вывод — шифротекстом. Число циклов повторения выглядит следующим образом:  Каждый раунд состоит из нескольких этапов обработки, каждый из которых содержит четыре одинаковых по алгоритму, но разных по входным данным этапа, включая тот, который зависит от самого ключа шифрования. Набор обратных повторений применяется для преобразования шифротекста в исходный текст с использованием того же самого ключа шифрования. Смотрите также: 10 популярных кодов и шифров  На приведённой выше диаграмме нарисовано квантовое распределение ключей (протокол BB84), являющееся безопасным способом связи, который реализует криптографический протокол с участием компонентов квантовой механики. Он позволяет двум сторонам создавать общий закрытый ключ (симметричные ключи), известный только им, который может быть использован для шифровки и дешифровки сообщений. Квантовая механика — это совокупность научных законов, описывающих поведение фотонов, электронов и других частиц, составляющих вселенную. Индустрии ищут большую безопасность от хакеров, поэтому новое поколение криптографии уже будет основываться не на математике, а на физике. Учёные в области физики атомов и частиц уже вошли в мир криптографии. Эти учёные хотят использовать законы квантовой механики для отправки сообщений, которые невозможно взломать. Они основоположники новой науки — квантовой криптографии, которая достигла совершеннолетия только в последние несколько десятилетий. Квантовая криптография опирается на физику частиц. Частицы, составляющие нашу вселенную, по своей сути являются неопределёнными явлениями, способными одновременно существовать в более чем одном месте или состоянии. Они самостоятельно выбирают, как себя вести, только когда они сталкиваются с объектом или кто-то измеряет их свойства. Криптография — это захватывающая область информационной безопасности и является одной из самых сложных дисциплин. Как только мы полностью понимаем такие простые шифры, как Цезаря и Полибия, то при переходе к более сложным шифрам с повторяющимися итерациями шифрования становится легче понимать алгоритмы вроде DES и AES. Криптология — это самостоятельная наука, и мы исследовали её историю, фундаментальные основы шифров от наименее до самых сложных типов, используемых сегодня. Перевод статьи «Understanding Cryptography From Math to Physics» Варвара Николаева ']), (['Исследователи обнаружили, что нейронные сети не могут распознавать оптические иллюзии'], ['В последнее время машинное обучение зашло далеко вперёд: нейронные сети научились понимать содержание картинок, определять человеческие лица, даже создавать подобные изображения. Поэтому, казалось бы, заставить машины идентифицировать и создавать оптические иллюзии может быть так же просто. С этим вопросом столкнулись учёные из Луисвиллского университета Роберт Уильямс и Роман Ямпольский. Они попытались научить нейронные сети распознавать и генерировать иллюзии, но это оказалось гораздо сложнее, чем предполагалось. Глубокое обучение нейронных сетей базируется на двух основных факторах: мощные машины с надлежащим программным обеспечением и огромные базы данных, на которых учатся нейронные сети. Чтобы научить компьютер распознавать на картинке лицо, потребуется база данных, содержащая более десяти тысяч изображений с чётко обозначенными лицами. По ним нейронная сеть определит наиболее характерные черты, к примеру, два глаза, нос и рот. Картинок же со статическими иллюзиями несколько тысяч, а уникальных иллюзий, вероятно, всего лишь несколько десятков. Уильямс и Ямпольский попробовали научить нейронную сеть распознавать оптические иллюзии по тому же принципу. Первостепенной проблемой стало отсутствие баз данных с оптическими иллюзиями для обучения. Тем не менее, учёным удалось собрать базу данных из более чем 6000 таких изображений, а затем создать генеративно-состязательную сеть для формирования иллюзий. Но эксперимент провалился: после 7 часов обучения на видеокарте NVIDIA Tesla K80 учёные так и не получили желаемого результата. Они считают, что основная проблема кроется в коренном различии человеческого и компьютерного зрения. Нейронные сети пока не способны понять принципы, по которым составляются оптические иллюзии. В сентябре 2018 года команда учёных из Брауновского университета (США) тоже провела свой эксперимент с оптическими иллюзиями и нейросетью. Исследователи представили нейронную сеть, чьё восприятие изображения ограничено физиологией зрительной коры головного мозга приматов.   via MIT Technology\\nSource: научные труды Р. Уильямса и Р. Ямпольского Никита Нельсон']), (['Курс «Компьютерная криминалистика в Windows»'], ['19 ноября израильская высшая школа информационных технологий HackerU запускает курс «Компьютерная криминалистика в Windows». Да, если вы хотите получить более глубокие знания о киберрасследованиях и кибер-криминалистических процессах. Приглашают сетевых/IT-администраторов, компьютерных криминалистов, сотрудников ИБ-служб и всех заинтересованных. Два уровня сложности — основной (стартует 19 ноября) и продвинутый (середина декабря). Каждый рассчитан на 40 часов (или 3 недели) обучения. Изучите основы криминалистической экспертизы в OC Windows. Узнаете, как: Познакомитесь с продвинутыми инструментами и техниками. Вас ждут лабораторные сессии и практические занятия для тренировки проведения экспертно-криминалистических операций. С каждым годом всё больше компаний становятся жертвами киберпреступлений, при этом страдает крупный и средний бизнес. Поэтому сейчас таким компаниям требуются специалисты в области компьютерной криминалистики. Чтобы освоиться в этой профессии, необходимо сочетать навыки следователя, программиста и аналитика данных — разобраться с этим и поможет курс. Оставить заявку и пройти тестирование на сайте мероприятия. Занятия пройдут по адресу: Москва, проспект 60-летия Октября, д. 9 стр. 2, к .120, учебный класс офиса Сбербанка.']), (['Разработан язык программирования CRN++ для биохимических реакций'], ['Исследователи из Техасского университета в Остине разработали язык программирования CRN++ для биохимиков. Он должен упростить процесс разработки биохимических веществ до начала их фактического создания. Главной проблемой в химии является построение структуры нужного вещества. Для этого необходим метод, учитывающий множество факторов. CRN++ должен решить эту проблему. Как заявил один из разработчиков нового языка Марко Вазик, язык программирования CRN++ создан для выражения хода химических реакций. Он провёл аналогию с классическими ЯП, а также отметил, что CRN++ представляет собой язык с открытым кодом, что позволит учёным вносить дополнения по мере необходимости. Его исходные коды уже доступны на GitHub. Авторы позиционируют CRN++ как язык программирования для синтетической и молекулярной биологии. Он предназначен для строгого описания химических реакций и позволяет переводить программный код в записи химических уравнений. Также он манипулирует реальными значениями концентраций химических веществ. И хотя пока отклонений от идеального выходного значения нельзя избежать, учёные разрабатывают методы минимизации ошибок и применяют инструменты их анализа. По словам исследователей, молекулы взаимодействуют между собой посредством химических реакций, что позволяет запрограммировать биохимическую систему на определённые действия. Для этого надо лишь направить реакции по нужному пути. Технически это аналог химического контроллера. При этом язык программирования CRN++ поддерживает все типы химических реакций. В мире сейчас очень много языков программирования. Однако самым популярным, по данным рейтинга PYPL на октябрь 2018 года, является Python. В рейтинге TIOBE за этот же период он не попал даже в тройку лидеров. Source: arXiv.org\\n Андрей Галадей']), (['Amazon отказалась от нейросети, которая отдавала предпочтение резюме мужчин'], ['Amazon отказалась от использования искусственного интеллекта для оценки резюме, поскольку высокие баллы получали в основном соискатели-мужчины. Компания пыталась решить проблему с 2015 года, однако сейчас прекратила работу над системой. Об этом пишет Reuters со ссылкой на анонимный источник в компании. Команда Эдинбургского инженерного центра Amazon разрабатывала ИИ, который мог бы быстро оценить опубликованные в Интернете резюме и найти подходящих сотрудников. Разработчики создали 500 компьютерных моделей для разных навыков и локаций соискателей, каждая из которых распознавала около 50 тысяч слов-сигналов в резюме. В 2015 году в Amazon заметили, что алгоритм обращал мало внимания на опыт и навыки потенциальных сотрудников. Вместо этого он отдавал предпочтение тем, кто употреблял свойственные «мужским» резюме глаголы, такие как «выполнил» (executed) или «захватил» (captured). Разработчики объяснили это тем, что алгоритм обучался на базе резюме всех сотрудников компании за последние десять лет. Мужчин в ней оказалось больше, поэтому ИИ решил, что кандидаты мужского пола предпочтительнее. Также ИИ часто рекомендовал неквалифицированный персонал. Компания изменила алгоритм, однако существенных улучшений это не дало, и к началу 2017 года Amazon отказалась от использования системы. Хотя корпорация остановила разработку алгоритма, она использует часть наработок для решения базовых задач рекрутинга, например, для исключения дубликатов профилей кандидатов. В июне 2018 года стало известно, что Amazon заменяет менеджеров по работе с брендами на программы. Это помогает корпорации экономить на сотрудниках с шестизначными зарплатами. Source: Reuters Наташа Маркова'])]]\n"
     ]
    }
   ],
   "source": [
    "test_top = getNewsPaper()\n",
    "t = test_top.getTop(period=\"week\")\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ОБОЖЕЭТОРАБОТАЕТ!!!!!**\n",
    "\n",
    "<s>в кои-то веки!</s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Домашнее задание выполнено.\n",
    "\n",
    "В процессе было очень многое повторено, изучено и отработано, за что большое спасибо!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
